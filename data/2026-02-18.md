<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Protecting Language Models Against Unauthorized Distillation through Trace Rewriting](https://arxiv.org/abs/2602.15143)
*Xinhang Ma,William Yeoh,Ning Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 该论文研究如何通过修改教师模型生成的推理轨迹来防止未经授权的知识蒸馏，实现反蒸馏和API水印两种目标。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏被广泛用于将大语言模型能力转移到更小的学生模型，但未经授权的蒸馏利用了前沿模型开发的巨大努力和成本，需要防止这种不公平利用。

Method: 提出动态重写教师模型推理输出的方法，包括基于指令的重写和基于梯度的技术，在保持答案正确性和语义连贯性的同时实现反蒸馏和水印嵌入。

Result: 简单的基于指令的重写方法实现了强反蒸馏效果，同时保持甚至提升了教师模型性能；重写方法还实现了高可靠的水印检测，几乎没有误报。

Conclusion: 通过重写教师模型的推理输出可以有效防止未经授权的知识蒸馏，同时实现反蒸馏和可验证的水印嵌入，保护模型开发者的权益。

Abstract: Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.

</details>


### [2] [da Costa and Tarski meet Goguen and Carnap: a novel approach for ontological heterogeneity based on consequence systems](https://arxiv.org/abs/2602.15158)
*Gabriel Rocha*

Main category: cs.AI

TL;DR: 本文提出了一种基于Carnapian-Goguenism的本体异质性新方法，称为da Costian-Tarskianism，结合了da Costa的数学容忍原则和Tarski的后果算子理论，通过扩展后果系统和扩展开发图来关联本体。


<details>
  <summary>Details</summary>
Motivation: 解决本体异质性问题，即不同本体系统之间的兼容性和互操作性问题。现有方法在处理本体间的复杂关系时存在局限性，需要一种更灵活的理论框架来支持本体间的映射和转换。

Method: 基于Carnielli等人和Citkin与Muravitsky发展的后果系统理论，引入扩展后果系统（在后果系统中加入本体公理），并定义扩展开发图结构，通过扩展后果系统的态射以及纤维化和分裂等操作来关联不同本体。

Result: 建立了da Costian-Tarskianism理论框架，提供了处理本体异质性的形式化工具，包括扩展后果系统和扩展开发图，能够更灵活地表示和管理本体间的关系。

Conclusion: 该方法为应用本体学领域提供了新的理论视角和工具，能够更好地处理本体异质性问题，并为未来研究指明了方向，包括进一步的理论完善和实际应用探索。

Abstract: This paper presents a novel approach for ontological heterogeneity that draws heavily from Carnapian-Goguenism, as presented by Kutz, Mossakowski and Lücke (2010). The approach is provisionally designated da Costian-Tarskianism, named after da Costa's Principle of Tolerance in Mathematics and after Alfred Tarski's work on the concept of a consequence operator. The approach is based on the machinery of consequence systems, as developed by Carnielli et al. (2008) and Citkin and Muravitsky (2022), and it introduces the idea of an extended consequence system, which is a consequence system extended with ontological axioms. The paper also defines the concept of an extended development graph, which is a graph structure that allows ontologies to be related via morphisms of extended consequence systems, and additionally via other operations such as fibring and splitting. Finally, we discuss the implications of this approach for the field of applied ontology and suggest directions for future research.

</details>


### [3] [Mind the (DH) Gap! A Contrast in Risky Choices Between Reasoning and Conversational LLMs](https://arxiv.org/abs/2602.15173)
*Luise Ge,Yongyan Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 该研究比较了20个前沿和开源大语言模型在风险决策中的表现，发现LLM可分为推理模型和对话模型两类，前者更理性，后者更接近人类但理性程度较低。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在决策支持和智能体工作流中应用广泛，但对其在不确定性下的决策机制理解仍然有限。研究者希望通过比较LLM的风险选择行为，填补这一知识空白。

Method: 研究从两个维度比较LLM的风险选择：(1)前景表示方式（显式vs经验基础），(2)决策理由（解释）。研究涉及20个前沿和开源LLM，并辅以匹配的人类受试者实验作为参考点，同时以期望收益最大化的理性智能体模型作为另一参考。

Result: 研究发现LLM可分为两类：推理模型（RMs）倾向于理性行为，对前景顺序、得失框架和解释不敏感，在显式和经验式前景表示下行为相似；对话模型（CMs）理性程度显著较低，更接近人类，对前景顺序、框架和解释敏感，且表现出较大的描述-历史差距。开源LLM的配对比较表明，区分RMs和CMs的关键因素是数学推理训练。

Conclusion: LLM在风险决策中存在明显的分类差异，推理模型表现出更强的理性特征，而对话模型更接近人类决策模式但理性程度不足。数学推理训练是区分这两类模型的关键因素，这对LLM在决策支持系统中的设计和应用具有重要启示。

Abstract: The use of large language models either as decision support systems, or in agentic workflows, is rapidly transforming the digital ecosystem. However, the understanding of LLM decision-making under uncertainty remains limited. We initiate a comparative study of LLM risky choices along two dimensions: (1) prospect representation (explicit vs. experience based) and (2) decision rationale (explanation). Our study, which involves 20 frontier and open LLMs, is complemented by a matched human subjects experiment, which provides one reference point, while an expected payoff maximizing rational agent model provides another. We find that LLMs cluster into two categories: reasoning models (RMs) and conversational models (CMs). RMs tend towards rational behavior, are insensitive to the order of prospects, gain/loss framing, and explanations, and behave similarly whether prospects are explicit or presented via experience history. CMs are significantly less rational, slightly more human-like, sensitive to prospect ordering, framing, and explanation, and exhibit a large description-history gap. Paired comparisons of open LLMs suggest that a key factor differentiating RMs and CMs is training for mathematical reasoning.

</details>


### [4] [Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models](https://arxiv.org/abs/2602.15248)
*Pavel Koptev,Vishnu Kumar,Konstantin Malkov,George Shapiro,Yury Vikhanov*

Main category: cs.AI

TL;DR: 论文提出AI/机器学习框架补充确定性算法，预测供应链金融中的发票稀释风险，替代传统不可撤销付款承诺方法


<details>
  <summary>Details</summary>
Motivation: 发票稀释（批准金额与实际收款之间的差距）是供应链金融中非信用风险和利润损失的重要来源。传统依赖买方不可撤销付款承诺的方法阻碍了供应链金融的采用，特别是对于非投资级买方。

Method: 引入AI/机器学习框架，结合实时动态信用额度方法，使用九个关键交易字段的广泛生产数据集，补充确定性算法来预测发票稀释。

Result: 论文评估了AI/机器学习框架如何补充确定性算法来预测发票稀释，但摘要中未提供具体实验结果数据。

Conclusion: 数据驱动的AI/机器学习方法可以替代传统的不可撤销付款承诺，更有效地管理供应链金融中的发票稀释风险，特别是对于非投资级买方。

Abstract: Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.

</details>


### [5] [When Remembering and Planning are Worth it: Navigating under Change](https://arxiv.org/abs/2602.15274)
*Omid Madani,J. Brian Burns,Reza Eghbali,Thomas L. Dean*

Main category: cs.AI

TL;DR: 该研究探索了在动态不确定环境中，不同记忆类型和用途如何帮助空间导航，发现结合多种策略的架构在处理探索、搜索和路径规划等不同性质任务时最有效。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在动态变化、感知受限的不确定环境中，智能体如何进行高效空间导航的问题。环境具有非平稳性（障碍物和食物位置每日变化）、感知不确定性（位置信息有限且不确定），需要构建能够快速学习并适应这些挑战的导航策略。

Method: 研究方法包括：1）设计简单的觅食任务，智能体需要从家穿过障碍物找到食物；2）比较从简单到复杂的多种策略，包括不同的记忆使用和学习方式；3）提出使用非平稳概率学习技术更新情景记忆，并基于这些记忆构建即时地图（不完美、有噪声、限于经验）进行路径规划的架构。

Result: 研究结果表明：1）需要能够结合多种策略的架构来处理不同性质的任务（探索、搜索、路径规划）；2）使用非平稳概率学习更新记忆并基于记忆构建即时地图的智能体，在任务难度（如目标距离）增加时，效率显著高于简单（最小记忆）智能体；3）只要定位和环境变化的不确定性不太大，这种优势就会显现。

Conclusion: 结论是：在动态不确定环境中，结合非平稳概率学习更新情景记忆、基于经验构建即时地图并进行路径规划的智能体架构，能够显著提高导航效率，特别是在任务难度较高时。这种多策略结合的方法比单一简单策略更适应复杂导航任务的需求。

Abstract: We explore how different types and uses of memory can aid spatial navigation in changing uncertain environments. In the simple foraging task we study, every day, our agent has to find its way from its home, through barriers, to food. Moreover, the world is non-stationary: from day to day, the location of the barriers and food may change, and the agent's sensing such as its location information is uncertain and very limited. Any model construction, such as a map, and use, such as planning, needs to be robust against these challenges, and if any learning is to be useful, it needs to be adequately fast. We look at a range of strategies, from simple to sophisticated, with various uses of memory and learning. We find that an architecture that can incorporate multiple strategies is required to handle (sub)tasks of a different nature, in particular for exploration and search, when food location is not known, and for planning a good path to a remembered (likely) food location. An agent that utilizes non-stationary probability learning techniques to keep updating its (episodic) memories and that uses those memories to build maps and plan on the fly (imperfect maps, i.e. noisy and limited to the agent's experience) can be increasingly and substantially more efficient than the simpler (minimal-memory) agents, as the task difficulties such as distance to goal are raised, as long as the uncertainty, from localization and change, is not too large.

</details>


### [6] [Improving LLM Reliability through Hybrid Abstention and Adaptive Detection](https://arxiv.org/abs/2602.15391)
*Ankit Sharma,Nachiket Tapas,Jyotiprakash Patra*

Main category: cs.AI

TL;DR: 本文提出了一种自适应弃权系统，通过动态调整安全阈值和级联检测架构，在保持高性能的同时平衡LLM的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM部署面临安全性与实用性的根本权衡：严格的过滤机制会阻止良性查询，而宽松的控制则可能生成不安全内容。传统的基于静态规则或固定置信度阈值的防护措施通常缺乏上下文敏感性且计算成本高，导致高延迟和用户体验下降。

Method: 引入自适应弃权系统，基于实时上下文信号（如领域和用户历史）动态调整安全阈值。采用多维检测架构，包含五个并行检测器，通过分层级联机制结合以优化速度和精度。级联设计通过逐步过滤查询减少不必要的计算。

Result: 在混合和特定领域工作负载上的广泛评估显示，假阳性显著减少，特别是在医疗建议和创意写作等敏感领域。系统在严格操作模式下保持高安全精度和接近完美的召回率。与非级联模型和外部防护系统相比，实现了显著的延迟改进。

Conclusion: 上下文感知的弃权框架有效地平衡了安全性和实用性，同时保持了性能，为可靠的LLM部署提供了可扩展的解决方案。

Abstract: Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.

</details>


### [7] [Common Belief Revisited](https://arxiv.org/abs/2602.15403)
*Thomas Ågotnes*

Main category: cs.AI

TL;DR: 本文解决了关于共同信念逻辑的开放性问题，证明了KD45个体信念下共同信念的完整逻辑特征，包含一个额外公理且依赖于智能体数量。


<details>
  <summary>Details</summary>
Motivation: 研究共同信念在个体信念为KD45时的逻辑特性，澄清了共同信念会失去5属性但保持D和4属性的误解，并探索shift-reflexivity公理是否足以完整刻画共同信念。

Method: 通过逻辑分析和形式化证明，研究共同信念在KD45个体信念框架下的逻辑性质，识别必要的公理并验证其完备性。

Result: 发现仅用shift-reflexivity公理扩展KD4不足以完整刻画共同信念，需要额外添加一个依赖于智能体数量的公理，最终获得了共同信念的完备逻辑特征。

Conclusion: 本文完全解决了共同信念在KD45个体信念下的逻辑特征问题，证明了需要两个公理（包括shift-reflexivity和一个依赖于智能体数量的公理）才能获得完备的逻辑系统。

Abstract: Contrary to common belief, common belief is not KD4.
  If individual belief is KD45, common belief does indeed lose the 5 property and keep the D and 4 properties -- and it has none of the other commonly considered properties of knowledge and belief. But it has another property: $C(Cφ\rightarrow φ)$ -- corresponding to so-called shift-reflexivity (reflexivity one step ahead). This observation begs the question:
  is KD4 extended with this axiom a complete characterisation of common belief in the KD45 case? If not, what \emph{is} the logic of common belief? In this paper we show that the answer to the first question is ``no'': there is one additional axiom, and, furthermore, it relies on the number of agents. We show that the result is a complete characterisation of common belief, settling the open problem.

</details>


### [8] [GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway](https://arxiv.org/abs/2602.15531)
*Javier Irigoyen,Roberto Daza,Aythami Morales,Julian Fierrez,Francisco Jurado,Alvaro Ortigosa,Ruben Tolosana*

Main category: cs.AI

TL;DR: EduEVAL-DB是一个基于教师角色的数据集，用于评估和训练自动教学评估器和AI导师，包含854个解释，涵盖科学、语言和社会科学K-12年级，采用半自动标注和专家评审，并进行了初步验证实验。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估和训练自动教学评估器和AI导师的数据集，特别是在教学解释质量评估方面。需要基于教师角色和实际教学实践中的教学风格和不足来构建数据集，以支持教育AI系统的开发和评估。

Method: 1. 基于ScienceQA基准的精选子集构建数据集，包含139个问题，854个解释；2. 每个问题提供1个人类教师解释和6个LLM模拟的教师角色解释；3. 通过提示工程实例化基于实际教学实践的教学风格和不足的教师角色；4. 提出与教育标准一致的教学风险评分标准，包含五个维度；5. 采用半自动过程加专家教师评审进行二元风险标注。

Result: 1. 创建了EduEVAL-DB数据集，包含854个标注的解释；2. 提出了包含五个维度的教学风险评分标准；3. 初步验证实验表明数据集适用于评估目的；4. 基准测试比较了Gemini 2.5 Pro和轻量级Llama 3.1 8B模型；5. 展示了在EduEVAL-DB上进行监督微调可以支持在消费级硬件上部署的教学风险检测模型。

Conclusion: EduEVAL-DB是一个有价值的资源，可用于评估和训练自动教学评估器和AI导师。数据集基于教师角色设计，包含丰富的教学解释和风险标注，支持教育AI系统的开发和评估，特别是在教学风险检测方面。初步实验验证了数据集的有效性和实用性。

Abstract: This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.

</details>


### [9] [RUVA: Personalized Transparent On-Device Graph Reasoning](https://arxiv.org/abs/2602.15553)
*Gabriele Conte,Alessio Mattiace,Gianni Carmosino,Potito Aghilar,Giovanni Servedio,Francesco Musicco,Vito Walter Anelli,Tommaso Di Noia,Francesco Maria Donini*

Main category: cs.AI

TL;DR: Ruva提出首个"透明盒"架构，用于人类参与的记忆管理，通过个人知识图谱替代向量匹配，实现可检查、可精确删除的AI记忆系统


<details>
  <summary>Details</summary>
Motivation: 当前个人AI系统主要基于"黑盒"检索增强生成，存在缺乏可问责性的问题：当AI产生幻觉或检索敏感数据时，用户无法检查原因或纠正错误。向量数据库的"删除"操作在数学上不精确，会留下概率性"幽灵"，违反真正的隐私保护

Method: Ruva采用"透明盒"架构，基于个人知识图谱而非向量空间，实现人类参与的记忆管理。通过从向量匹配转向图谱推理，让用户能够检查AI知道的内容，并对特定事实进行精确删除

Result: Ruva确保了"被遗忘权"，用户成为自己生活的编辑者。项目已公开，包含演示视频

Conclusion: 通过将个人AI建立在个人知识图谱上，Ruva解决了当前AI系统的可问责性和隐私问题，实现了真正的人类中心化AI记忆管理

Abstract: The Personal AI landscape is currently dominated by "Black Box" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, "deleting" a concept from a vector space is mathematically imprecise, leaving behind probabilistic "ghosts" that violate true privacy. We propose Ruva, the first "Glass Box" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the "Right to be Forgotten." Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.

</details>


### [10] [How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning](https://arxiv.org/abs/2602.15580)
*Hongxuan Wu,Yukun Zhang,Xueqing Zhou*

Main category: cs.AI

TL;DR: 该研究提出PID Flow框架，通过信息分解分析多模态Transformer中视觉和语言信息的处理机制，发现视觉信息在早期层达到峰值后衰减，语言信息在后期层主导预测（约82%），跨模态协同作用始终低于2%。


<details>
  <summary>Details</summary>
Motivation: 研究多模态Transformer在回答视觉问题时，预测是受视觉证据、语言推理还是真正的跨模态计算驱动，以及这种结构在不同层中如何演化。旨在理解视觉如何转化为语言的信息处理机制。

Method: 提出PID Flow框架，结合降维、归一化流高斯化和闭式高斯PID估计，将Transformer每层的预测信息分解为冗余、视觉独特、语言独特和协同四个组成部分。在LLaVA-1.5-7B和LLaVA-1.6-7B模型上应用，并进行了Image→Question注意力敲除实验建立因果关系。

Result: 发现一致的模态转换模式：视觉独特信息在早期达到峰值后衰减，语言独特信息在后期层激增（占最终预测约82%），跨模态协同作用始终低于2%。这种模式在不同模型变体间高度稳定（层间相关性>0.96），但强烈依赖任务。注意力敲除实验证实了因果关系。

Conclusion: 研究提供了多模态Transformer中视觉如何转化为语言的信息论和因果解释，为识别模态特定信息丢失的架构瓶颈提供了量化指导，揭示了当前多模态模型主要依赖语言推理而非真正的跨模态融合。

Abstract: When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\% of the final prediction, and cross-modal synergy remains below 2\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.

</details>


### [11] [On inferring cumulative constraints](https://arxiv.org/abs/2602.15635)
*Konstantin Sidorov*

Main category: cs.AI

TL;DR: 提出一种预处理方法，通过推断额外的累积约束来捕获调度中多资源交互，无需搜索时探测，提升搜索性能并收紧目标界限


<details>
  <summary>Details</summary>
Motivation: 传统约束编程中累积约束的传播通常单独进行，忽略了多资源间的交互作用，导致在某些基准测试中性能严重下降

Method: 将累积约束解释为占用向量的线性不等式，通过发现不能并行运行的任务集合（覆盖集），用提升技术加强覆盖不等式，并将生成的约束注入调度问题实例

Result: 在标准RCPSP和RCPSP/max测试套件上，推断的约束改善了搜索性能，在有利实例上收紧目标界限，在不利实例上仅有轻微性能下降；发现了25个新的下界和5个新的最优解，其中8个下界直接来自推断的约束

Conclusion: 通过预处理推断额外的累积约束能有效捕获调度中的多资源交互，提升约束编程在调度问题上的性能表现

Abstract: Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints.

</details>


### [12] [CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving](https://arxiv.org/abs/2602.15645)
*Lucas Elbert Suryana,Farah Bierenga,Sanne van Buuren,Pepijn Kooij,Elsefien Tulleners,Federico Scari,Simeon Calvert,Bart van Arem,Arkady Zgonnikov*

Main category: cs.AI

TL;DR: 提出CARE Drive框架，用于评估自动驾驶中视觉语言模型决策是否真正基于人类相关理由，而非事后合理化


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注结果性能（如安全性、轨迹精度），但无法确定模型决策是否真正反映人类相关考虑因素，这在安全关键领域可能造成虚假信心

Method: 提出CARE Drive框架，通过比较基准模型和理由增强模型在受控上下文变化下的决策，评估人类理由是否因果影响决策行为。采用两阶段评估：提示校准确保稳定输出，系统上下文扰动测量决策对人类理由的敏感性

Result: 在自行车超车场景中，明确的人类理由显著影响模型决策，改善与专家推荐行为的一致性。但响应性随上下文因素变化，表明对不同类型理由的敏感性不均匀

Conclusion: CARE Drive提供了经验证据，表明基础模型中的理由响应性可以在不修改模型参数的情况下进行系统评估，有助于提高自动驾驶系统的透明度和可靠性

Abstract: Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.

</details>


### [13] [PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra](https://arxiv.org/abs/2602.15669)
*Xiachong Feng,Liang Zhao,Weihong Zhong,Yichong Huang,Yuxuan Gu,Lingpeng Kong,Xiaocheng Feng,Bing Qin*

Main category: cs.AI

TL;DR: PERSONA框架通过激活空间中的向量操作实现LLM人格控制，无需训练即可达到微调级别性能，证明人格特质在表示空间中具有可提取、近似正交的数学结构。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型人格控制方法依赖静态提示或昂贵的微调，无法捕捉人类特质的动态性和组合性，需要一种更高效、可解释的人格控制框架。

Method: 提出PERSONA框架，包含三个阶段：Persona-Base通过对比激活分析提取正交特质向量；Persona-Algebra通过向量算术实现精确控制（标量乘法调节强度、加法组合、减法抑制）；Persona-Flow在推理时动态组合向量实现上下文感知适应。

Result: 在PersonalityBench上获得9.60的平均分，接近监督微调上限9.61；在Persona-Evolve动态人格适应基准上，在不同模型家族中获得高达91%的胜率。

Conclusion: LLM的人格方面具有数学可处理性，为可解释和高效的行为控制开辟了新方向，证明人格特质在激活空间中表现为可提取、近似正交的方向，支持代数操作。

Abstract: Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.

</details>


### [14] [Recursive Concept Evolution for Compositional Reasoning in Large Language Models](https://arxiv.org/abs/2602.15725)
*Sarim Chaudhry*

Main category: cs.AI

TL;DR: RCE框架让预训练语言模型能在推理时动态修改内部表示几何，通过生成低秩概念子空间来构建新抽象，在组合推理基准上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务上表现良好，但在需要组合推理的基准测试（如ARC-AGI-2、GPQA、MATH、BBH、HLE）上准确性急剧下降。现有方法通过扩展token级搜索来改进推理，但保持模型的潜在表示空间固定，当所需抽象未编码在该空间中时，性能会崩溃。

Method: 提出递归概念演化（RCE）框架，使预训练语言模型能在推理时修改内部表示几何。RCE引入动态生成的低秩概念子空间，这些子空间在检测到表示不足时生成，通过最小描述长度准则选择，在协同时合并，并通过约束优化进行整合以保持稳定性。

Result: 将RCE与Mistral-7B集成并在组合推理基准上评估：在ARC-AGI-2上获得12-18分提升，在GPQA和BBH上获得8-14分改进，在MATH和HLE上持续减少深度诱导错误。

Conclusion: RCE使模型能够构建新抽象而不仅仅是重新组合现有抽象，解决了现有方法在组合推理任务中表示空间固定的局限性，显著提升了语言模型在复杂推理基准上的性能。

Abstract: Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.

</details>


### [15] [GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems](https://arxiv.org/abs/2602.15776)
*Yiqin Yang,Xu Yang,Yuhua Jiang,Ni Mu,Hao Hu,Runpeng Xie,Ziyou Zhang,Siyuan Li,Yuan-Hua Ni,Qianchuan Zhao,Bo Xu*

Main category: cs.AI

TL;DR: 提出GlobeDiff算法，通过多模态扩散过程从局部观测推断全局状态，解决多智能体系统中部分可观测性问题


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中的部分可观测性是有效协调和决策的关键障碍。现有方法如信念状态估计和智能体间通信存在局限：信念方法主要依赖过去经验而未充分利用全局信息，通信方法缺乏有效利用辅助信息的鲁棒模型。

Method: 提出全局状态扩散算法(GlobeDiff)，将状态推断过程建模为多模态扩散过程，基于局部观测推断全局状态。该方法克服状态估计中的模糊性，同时以高保真度推断全局状态。

Result: 证明了GlobeDiff在单模态和多模态分布下的估计误差可被限定。大量实验结果表明，GlobeDiff实现了优越性能，能够准确推断全局状态。

Conclusion: GlobeDiff通过多模态扩散过程有效解决了多智能体系统中的部分可观测性问题，相比现有方法具有更好的全局状态推断能力。

Abstract: In the realm of multi-agent systems, the challenge of \emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.

</details>


### [16] [This human study did not involve human subjects: Validating LLM simulations as behavioral evidence](https://arxiv.org/abs/2602.15785)
*Jessica Hullman,David Broska,Huaman Sun,Aaron Shaw*

Main category: cs.AI

TL;DR: 论文探讨了在社会科学实验中使用大语言模型作为合成参与者的有效性，对比了两种获取有效因果效应估计的策略：启发式方法和统计校准方法，并分析了各自的适用场景和局限性。


<details>
  <summary>Details</summary>
Motivation: 越来越多的研究使用大语言模型作为合成参与者来生成成本效益高且几乎即时的响应，但缺乏关于何时这种模拟能够支持对人类行为的有效推断的指导。需要明确在什么条件下这些方法适用于探索性研究或验证性研究。

Method: 对比两种策略：1）启发式方法：通过提示工程、模型微调等修复策略来建立模拟行为和观察到的真实人类行为之间的可互换性；2）统计校准：结合辅助人类数据和统计调整来考虑观察到的和模拟的响应之间的差异。

Result: 启发式方法对许多探索性任务有用，但缺乏验证性研究通常需要的正式统计保证。统计校准在明确假设下保持有效性，并提供比仅依赖人类参与者的实验更精确、成本更低的因果效应估计。两种方法的潜力都取决于大语言模型对相关人群的近似程度。

Conclusion: 研究强调了在社会科学实验中使用大语言模型时需要明确的方法论指导，区分探索性和验证性研究的不同需求，并指出当研究者狭隘地专注于用大语言模型替代人类参与者时可能错过的机会。

Abstract: A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.

</details>


### [17] [Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings](https://arxiv.org/abs/2602.15791)
*Suhyung Jang,Ghang Lee,Jaekun Lee,Hyunjun Lee*

Main category: cs.AI

TL;DR: 该研究提出使用大语言模型（LLM）嵌入作为编码方法，以保留建筑语义中更精细的区分，相比传统one-hot编码能更好地表示建筑对象类型和子类型之间的关系。


<details>
  <summary>Details</summary>
Motivation: 在AECO行业中，准确表示建筑语义（包括通用对象类型和特定子类型）对于AI模型训练至关重要。传统的编码方法（如one-hot）往往无法传达密切相关的子类型之间的细微关系，限制了AI的语义理解能力。

Method: 提出使用LLM嵌入（如OpenAI GPT和Meta LLaMA）作为编码来保留建筑语义的精细区分。通过训练GraphSAGE模型对5个高层住宅建筑信息模型（BIM）中的42个建筑对象子类型进行分类。测试了不同嵌入维度，包括原始高维LLM嵌入（1,536、3,072或4,096维）和通过Matryoshka表示模型生成的1,024维压缩嵌入。

Result: 实验结果表明，LLM编码优于传统的one-hot基线，其中llama-3（压缩）嵌入的加权平均F1分数达到0.8766，而one-hot编码为0.8475。

Conclusion: LLM编码在增强AI解释复杂、特定领域建筑语义能力方面具有潜力。随着LLM和降维技术的不断发展，这种方法在AECO行业的语义细化任务中具有广泛的应用前景。

Abstract: Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.

</details>


### [18] [Developing AI Agents with Simulated Data: Why, what, and how?](https://arxiv.org/abs/2602.15816)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 本章介绍了基于仿真的合成数据生成方法，用于解决AI训练中数据不足和质量问题，提出了数字孪生AI仿真解决方案的参考框架。


<details>
  <summary>Details</summary>
Motivation: 现代符号AI的采用面临数据量不足和数据质量差的关键障碍，因此对合成数据生成技术的需求很高。

Method: 采用仿真方法生成多样化的合成数据，提出了一个用于描述、设计和分析基于数字孪生的AI仿真解决方案的参考框架。

Result: 介绍了基于仿真的合成数据生成的关键概念、优势和挑战，为AI训练提供了系统化的解决方案。

Conclusion: 仿真为AI训练提供了有效的合成数据生成方法，数字孪生框架为设计和分析AI仿真解决方案提供了系统化参考。

Abstract: As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.

</details>

<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 12]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation](https://arxiv.org/abs/2602.10367)
*Zhiling Yan,Dingjie Song,Zhe Fang,Yisheng Ji,Xiang Li,Quanzheng Li,Lichao Sun*

Main category: cs.AI

TL;DR: LiveMedBench是一个持续更新的、无数据污染的、基于评分标准的临床基准测试，通过每周从在线医疗社区收集真实病例，解决现有医学基准测试中的数据污染和时间错位问题，并开发了自动评分框架来评估LLMs的临床推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学基准测试存在两个关键限制：1) 数据污染问题，测试集无意中泄露到训练语料中，导致性能估计膨胀；2) 时间错位问题，无法捕捉医学知识的快速演变。此外，当前对开放式临床推理的评估指标要么依赖浅层词汇重叠，要么依赖主观的LLM作为评判者，都不足以验证临床正确性。

Method: 1) 开发LiveMedBench基准测试，每周从在线医疗社区收集真实临床病例，确保与模型训练数据的严格时间分离；2) 提出多智能体临床筛选框架，过滤原始数据噪声并根据循证医学原则验证临床完整性；3) 开发自动基于评分标准的评估框架，将医生回答分解为细粒度的病例特定标准。

Result: LiveMedBench包含2,756个真实病例，涵盖38个医学专业和多种语言，配有16,702个独特的评估标准。对38个LLMs的广泛评估显示，即使表现最好的模型也只达到39.2%的正确率，84%的模型在截止日期后的病例上表现出性能下降，证实了普遍存在的数据污染风险。错误分析进一步发现，上下文应用（而非事实知识）是主要瓶颈，35-48%的失败源于无法将医学知识适应到患者特定约束。

Conclusion: LiveMedBench为LLMs在临床环境中的评估提供了一个持续更新、无污染且基于评分标准的基准测试，揭示了当前LLMs在临床推理中的严重局限性，特别是数据污染问题和将医学知识适应到具体患者情境的能力不足。

Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.

</details>


### [2] [Found-RL: foundation model-enhanced reinforcement learning for autonomous driving](https://arxiv.org/abs/2602.10458)
*Yansong Qu,Zihao Sheng,Zilin Huang,Jiancong Chen,Yuhao Luo,Tianyi Wang,Yiheng Feng,Samuel Labi,Sikai Chen*

Main category: cs.AI

TL;DR: Found-RL是一个专门为自动驾驶设计的平台，通过异步批处理推理框架将基础模型（特别是视觉语言模型）高效集成到强化学习中，解决了VLM推理延迟问题，并通过多种监督机制将VLM的专家知识蒸馏到RL策略中。


<details>
  <summary>Details</summary>
Motivation: 强化学习在自动驾驶中面临样本效率低和语义可解释性不足的问题，而基础模型（特别是视觉语言模型）虽然能提供丰富的上下文知识，但其高推理延迟阻碍了在高频RL训练循环中的部署。

Method: 1. 异步批处理推理框架：将繁重的VLM推理与仿真循环解耦，解决延迟瓶颈；2. 多种监督机制：包括值边际正则化和优势加权动作指导，将VLM的专家动作建议蒸馏到RL策略；3. 采用高吞吐量CLIP进行密集奖励塑造；4. 通过条件对比动作对齐解决CLIP的动态盲区问题。

Result: Found-RL提供了一个端到端的微调VLM集成管道，轻量级RL模型能够达到接近十亿参数VLM的性能，同时保持实时推理（约500 FPS）。

Conclusion: Found-RL平台成功地将基础模型高效集成到自动驾驶的强化学习中，通过创新的异步推理架构和监督机制，解决了VLM延迟问题，使轻量级RL模型能够获得接近VLM的性能，同时保持实时推理能力。

Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.

</details>


### [3] [MERIT Feedback Elicits Better Bargaining in LLM Negotiators](https://arxiv.org/abs/2602.10467)
*Jihwan Oh,Murad Aghazada,Yooju Shin,Se-Young Yun,Taehyeon Kim*

Main category: cs.AI

TL;DR: 论文提出了一个基于效用反馈的框架，包含AgoraBench基准测试、经济学基础指标和人类偏好数据集，用于提升LLM在复杂谈判场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在谈判场景中表现不佳，缺乏战略深度和适应复杂人类因素的能力，现有基准测试难以捕捉这些局限性。

Method: 提出效用反馈中心框架：1) AgoraBench基准测试，涵盖9个挑战性场景；2) 基于效用理论的人类对齐经济学指标；3) 人类偏好数据集和学习管道，通过提示和微调增强LLM谈判能力。

Result: 基线LLM策略常偏离人类偏好，而提出的机制显著改善谈判表现，产生更深层次的战略行为和更强的对手意识。

Conclusion: 该框架有效提升了LLM在复杂谈判场景中的能力，使其更符合人类偏好和经济学原理，为谈判AI研究提供了新的基准和方法。

Abstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.

</details>


### [4] [Abstraction Generation for Generalized Planning with Pretrained Large Language Models](https://arxiv.org/abs/2602.10485)
*Zhenhe Cui,Huaxiang Xia,Hangjun Shen,Kailun Luo,Yong He,Wei Liang*

Main category: cs.AI

TL;DR: LLMs可作为QNP抽象生成器，通过自动调试修复抽象错误，为广义规划生成有用的定性数值规划抽象。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够作为QNP抽象生成器为广义规划问题生成抽象特征，并探索如何通过自动调试修复抽象错误。

Method: 提出提示协议：输入GP领域和训练任务给LLMs，提示生成抽象特征，并将初始状态、动作集和目标抽象为QNP问题；设计自动调试方法检测抽象错误，指导LLMs修复抽象。

Result: 实验表明，在自动调试的适当指导下，某些LLMs能够生成有用的QNP抽象。

Conclusion: LLMs可以作为QNP抽象生成器，通过自动调试机制能够有效生成和修复抽象，为广义规划提供有用的抽象模型。

Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.

</details>


### [5] [To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks](https://arxiv.org/abs/2602.10625)
*Nanxu Gong,Haotian Li,Sixun Dong,Jianxun Lian,Yanjie Fu,Xing Xie*

Main category: cs.AI

TL;DR: 研究发现大型推理模型在心理理论任务上表现不佳，推理能力无法完全从形式推理转移到社会推理，推理长度增加反而降低准确性，模型依赖选项匹配而非真正推理。


<details>
  <summary>Details</summary>
Motivation: 虽然大型推理模型在数学和编程等逐步推理任务上取得进展，但尚不清楚这种推理能力是否能转移到社会认知技能（如心理理论）。研究旨在系统评估推理模型在心理理论任务上的表现，探索推理能力在社会推理中的适用性。

Method: 研究系统评估了9个先进的大型语言模型，在三个代表性心理理论基准上比较推理模型与非推理模型的表现。进行了细粒度分析，包括响应长度与准确性关系、推理预算影响、选项移除实验等。设计了两种干预方法：慢到快自适应推理和思考到匹配捷径预防。

Result: 推理模型在心理理论任务上并不一致优于非推理模型，有时表现更差。发现三个关键现象：1) 慢思考崩溃：响应越长准确性显著下降，更大推理预算损害性能；2) 适度自适应推理有益：限制推理长度可缓解失败，动态适应必要；3) 选项匹配捷径：移除多项选择选项后推理模型显著改善，表明依赖选项匹配而非真正推理。

Conclusion: 大型推理模型在形式推理（如数学、代码）方面的进步不能完全转移到心理理论这种典型的社会推理任务。实现稳健的心理理论需要开发超越现有推理方法的独特能力。

Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.

</details>


### [6] [OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization](https://arxiv.org/abs/2602.10635)
*Keane Ong,Sabri Boughorbel,Luwei Xiao,Chanakya Ekbote,Wei Dai,Ao Qu,Jingyao Wu,Rui Mao,Ehsan Hoque,Erik Cambria,Gianmarco Mengaldo,Paul Pu Liang*

Main category: cs.AI

TL;DR: 本文提出HARPO方法，通过调节优势函数平衡异构任务和样本的学习，开发了Omnisapiens-7B 2.0社交行为基础模型，在多项任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常孤立地建模人类行为维度（情感、认知或社会属性），任务特定建模增加了训练成本并限制了跨行为设置的泛化能力。虽然最近的推理RL方法支持跨多个行为任务训练统一模型，但未明确解决跨异构行为数据的学习问题。

Method: 提出异构感知相对策略优化（HARPO），一种通过调节优势函数来平衡异构任务和样本学习的RL方法，确保在策略优化过程中没有任何单个任务或样本产生不成比例的影响。

Result: 使用HARPO开发了Omnisapiens-7B 2.0社交行为处理基础模型。相比现有行为基础模型，在多任务和保留设置上分别获得高达+16.85%和+9.37%的性能提升，同时产生更明确和鲁棒的推理轨迹。HARPO在行为任务上也比最近的RL方法表现更稳定。

Conclusion: HARPO方法有效解决了跨异构行为数据的学习问题，开发的Omnisapiens-7B 2.0模型在社交行为处理任务上表现出色，为开发社交智能AI提供了有效工具。

Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.

</details>


### [7] [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699)
*Jie Jiang,Yangru Huang,Zeyu Wang,Changping Wang,Yuling Xiong,Jun Zhang,Huan Yu*

Main category: cs.AI

TL;DR: V-STAR框架通过价值引导采样和树状优势强化解决生成式推荐中RL训练的概率-奖励不匹配问题，提升探索效率和多样性


<details>
  <summary>Details</summary>
Motivation: 生成式推荐中的强化学习存在概率-奖励不匹配问题：传统基于似然的解码（如束搜索）对局部高概率前缀存在短视偏见，导致探索不足和优势压缩两个关键失败

Method: 提出V-STAR框架，包含两个协同组件：1) 价值引导高效解码(VED)，识别决策节点并选择性深化高潜力前缀；2) Sibling-GRPO，利用诱导的树拓扑计算兄弟相对优势，将学习信号集中在决策分支上

Result: 在离线和在线数据集上的广泛实验表明，V-STAR优于最先进的基线方法，在严格延迟约束下提供更优的准确性和候选集多样性

Conclusion: V-STAR通过价值引导采样和树状优势强化有效解决了生成式推荐中RL训练的概率-奖励不匹配问题，实现了更好的探索效率和多样性

Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.

</details>


### [8] [Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act](https://arxiv.org/abs/2602.10802)
*Da-Lun Chen,Prasasthy Balasubramanian,Lauri Lovén,Susanna Pirttikangas,Jaakko Sauvola,Panagiotis Kostakos*

Main category: cs.AI

TL;DR: 该研究调查了高等教育中生成式AI的认知，通过混合方法分析ITEE学科师生对GenAI的看法，识别出共同和学科特定的主题，并提出负责任整合的概念框架。


<details>
  <summary>Details</summary>
Motivation: 高等教育中师生已广泛采用生成式AI工具，但利益相关者对GenAI的看法存在分歧，受文化、学科和制度背景影响。同时欧盟AI法案要求大学确保认知系统的监管合规性，因此需要了解利益相关者需求并定制GenAI整合方案。

Method: 采用混合方法，调查了奥卢大学ITEE学院的61名教职员工和37名学生，收集他们对GenAI的看法和使用经验。

Result: 研究发现共同和学科特定的主题：对GenAI编程支持有强烈兴趣，同时关注响应质量、隐私和学术诚信问题。研究识别出一套高层次需求，并提出了负责任GenAI整合的概念框架。

Conclusion: 学科特定需求强调了在高等教育中整合GenAI时利益相关者参与的重要性。高层次需求和框架为大学利用GenAI同时解决利益相关者关切和确保监管合规提供了实用指导。

Abstract: Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance.

</details>


### [9] [SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy](https://arxiv.org/abs/2602.10845)
*Xuecheng Zou,Yu Tang,Bingbing Wang*

Main category: cs.AI

TL;DR: SynergyKGC是一个自适应知识图谱补全框架，通过跨模态协同专家机制解决结构分辨率不匹配问题，在异构拓扑结构中实现稳健的关系推理。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全方法面临"结构分辨率不匹配"问题，无法协调不同图密度下的表示需求，导致在密集集群中出现结构噪声干扰，在稀疏区域出现灾难性表示崩溃。

Method: 提出SynergyKGC框架，将传统邻居聚合提升为主动的跨模态协同专家机制，采用关系感知交叉注意力和语义意图驱动的门控机制，结合密度相关的身份锚定策略和双塔一致性架构。

Result: 在两个公共基准测试上的系统评估验证了该方法在显著提升KGC命中率方面的优越性，为非均匀结构化数据中的弹性信息集成提供了经验证据。

Conclusion: SynergyKGC有效协调了拓扑异构性，同时确保了训练和推理阶段的表示稳定性，为知识图谱补全中的稳健信息融合提供了通用原则。

Abstract: Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical "structural resolution mismatch," failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data.

</details>


### [10] [Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics](https://arxiv.org/abs/2602.10885)
*Leheng Sheng,Wenchang Ma,Ruixin Hong,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: RLCER是一种无需人工标注的自监督强化学习方法，通过自我提出和演进的评分标准来奖励思维链，优于仅关注结果的RLVR方法


<details>
  <summary>Details</summary>
Motivation: 思维链在LLM推理中至关重要，但直接奖励思维链面临挑战：训练奖励模型需要大量人工标注，静态奖励模型难以适应思维链分布的演变且容易发生奖励攻击。这促使研究者寻求无需人工标注且能逐步演进的自监督思维链奖励方法。

Method: 提出RLCER（强化学习与思维链监督的自演进评分标准）方法，通过自我提出和演进的评分标准来奖励思维链，增强了仅关注结果的RLVR方法。该方法在无需结果奖励的情况下也能提供可靠的思维链监督信号。

Result: RLCER在性能上优于仅关注结果的RLVR方法。此外，当将这些自我提出的评分标准作为提示中的提示时，还能进一步提高推理时的性能表现。

Conclusion: 自提出和自演进的评分标准为思维链提供了有效的监督信号，使得RLCER方法能够在无需人工标注的情况下实现优于传统结果中心化方法的性能，同时还能提升推理时的表现。

Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \textbf{RLCER} (\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.

</details>


### [11] [Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation](https://arxiv.org/abs/2602.10964)
*F. Carichon,R. Rampa,G. Farnadi*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在跨文化内容创作中存在根本性局限，无法像人类一样根据文化距离产生有意义的食谱适应，而是表现出系统性文化偏差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用于生成和塑造文化内容，但现有研究表明它们存在系统性文化偏见，可能加剧刻板印象、同质化并消除特定文化表达形式。理解LLM能否超越主流文化而有意义地与多元文化对齐是一个关键挑战。

Method: 通过烹饪食谱这一文化、传统和创造力紧密结合的领域研究LLM的文化适应能力。使用GlobalFusion数据集，该数据集根据已建立的文化距离度量将不同国家的人类食谱配对。使用相同国家配对，用多个LLM生成文化适应食谱，从而直接比较人类和LLM在跨文化内容创作中的行为。

Result: LLM无法产生具有文化代表性的适应。与人类不同，它们生成的食谱差异与文化距离不相关。研究发现：1）文化信息在模型内部表征中保存较弱；2）模型通过误解创造力和传统等概念来夸大新颖性；3）模型无法将适应与其相关国家联系起来，也无法将其建立在文化显著元素（如食材）上。

Conclusion: 当前LLM在文化导向的生成方面存在根本性限制，这对它们在文化敏感应用中的使用具有重要影响。模型无法真正理解和适应文化差异，而是表现出系统性偏差。

Abstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications.

</details>


### [12] [FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight](https://arxiv.org/abs/2602.11136)
*Jiayi Zhou,Yang Sheng,Hantao Lou,Yaodong Yang,Jie Fu*

Main category: cs.AI

TL;DR: 论文提出了一种名为FoT的神经符号框架，通过双向形式化思维架构将自然语言需求转化为形式化规范，为LLM智能体提供数学保证而非概率评分的行为安全验证方法。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的智能体越来越多地在高风险领域运行，确保其行为安全变得至关重要。当前主流的监督范式LLM-as-a-Judge面临根本性困境：概率系统如何可靠地监督其他概率系统而不继承其失败模式？形式化验证提供了原则性的解决方案，但自然语言需求到形式化规范的转换成为关键瓶颈。

Method: 提出FoT神经符号框架，采用双向形式化思维架构：LLM作为规范编译器，自上而下将高级人类意图分解为原子化、可验证的约束，然后自下而上使用Dafny规范和Z3可满足性模理论求解来证明合规性，产生数学保证而非概率评分。

Result: 在三个基准测试（行为安全、多领域约束遵守、智能体向上欺骗检测）上验证，对7个智能体模型的实验表明：FoT相比LLM-as-a-Judge基线平均提升16.6%；实现了弱到强的泛化能力，7B参数的判断器能检测72B智能体的欺骗行为，准确率超过90%；通过迭代细化提供接近线性的安全改进。

Conclusion: FoT框架成功解决了LLM智能体行为安全验证中的形式化规范转换瓶颈，提供数学保证而非概率评分，显著提升了监督效果和泛化能力，为高风险领域LLM智能体的安全部署提供了可靠解决方案。

Abstract: As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.

</details>

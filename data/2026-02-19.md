<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 12]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Towards Efficient Constraint Handling in Neural Solvers for Routing Problems](https://arxiv.org/abs/2602.16012)
*Jieyi Bi,Zhiguang Cao,Jianan Zhou,Wen Song,Yaoxin Wu,Jie Zhang,Yining Ma,Cathy Wu*

Main category: cs.AI

TL;DR: CaR是一个基于显式学习可行性精炼的神经路由求解器通用约束处理框架，通过联合训练指导构造模块生成适合轻量级改进的多样化高质量解，在复杂约束下实现优越的可行性、解质量和效率。


<details>
  <summary>Details</summary>
Motivation: 当前神经求解器在简单路由问题上表现出色，但在复杂约束下的优势尚不成熟。现有的可行性掩码或隐式可行性感知等约束处理方案对于硬约束可能效率低下或不适用。

Method: 提出Construct-and-Refine (CaR)框架，基于显式学习可行性精炼。设计联合训练框架指导构造模块生成适合轻量级改进过程（如10步vs之前工作的5k步）的多样化高质量解。首次采用构造-改进共享表示，通过统一编码器实现跨范式知识共享。

Result: 在典型硬路由约束上的评估表明，CaR相比经典和神经最先进求解器，在可行性、解质量和效率方面都表现出优越性。

Conclusion: CaR是第一个通用且高效的神经路由求解器约束处理框架，通过显式学习可行性精炼和构造-改进共享表示，在复杂约束场景下实现了显著改进。

Abstract: Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.

</details>


### [2] [How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment](https://arxiv.org/abs/2602.16039)
*Hang Li,Kaiqi Yang,Xianxuan Long,Fedor Filippov,Yucheng Chu,Yasemin Copur-Gencturk,Peng He,Cory Miller,Namsoo Shin,Joseph Krajcik,Hui Liu,Jiliang Tang*

Main category: cs.AI

TL;DR: 本文系统评估了大型语言模型在教育自动评估中的不确定性量化方法，分析了不同模型、任务和解码策略对不确定性估计的影响，为开发更可靠的自动评分系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育自动评估中展现出优势，但其固有的概率特性引入了输出不确定性的挑战。评估结果对后续教学决策至关重要，不可靠的不确定性估计可能导致不稳定的教学干预，影响学生学习过程。目前不确定性量化方法在教育评估领域的适用性和可靠性尚未充分探索。

Method: 对多种不确定性量化方法在LLM自动评估背景下进行基准测试，通过综合分析多个评估数据集、不同LLM家族和生成控制设置中的不确定性行为，描述LLM在评分场景中表现出的不确定性模式。评估不同不确定性指标的优缺点，分析模型家族、评估任务和解码策略等关键因素对不确定性估计的影响。

Result: 研究揭示了LLM在自动评分场景中的不确定性模式，评估了不同不确定性指标的优缺点，并分析了模型家族、评估任务和解码策略等因素对不确定性估计的具体影响。为理解LLM自动评估中的不确定性特征提供了实证基础。

Conclusion: 本研究系统性地理解了LLM自动评估中的不确定性挑战，为未来开发更可靠、有效的不确定性感知评分系统奠定了基础，提供了可操作的见解和指导。

Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.

</details>


### [3] [Improving Interactive In-Context Learning from Natural Language Feedback](https://arxiv.org/abs/2602.16066)
*Martin Klissarov,Jonathan Cook,Diego Antognini,Hao Sun,Jingling Li,Natasha Jaques,Claudiu Musat,Edward Grefenstette*

Main category: cs.AI

TL;DR: 该研究提出了一种训练框架，将交互式上下文学习能力作为可训练技能而非涌现属性，通过信息不对称将单轮可验证任务转化为多轮教学互动，显著提升模型从语言反馈中学习的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练主要依赖静态语料建模，忽视了人类学习中基于纠正反馈动态适应上下文的重要交互反馈循环。模型需要具备根据反馈调整思维过程的能力，特别是在协作环境中。

Method: 提出可扩展方法，将单轮可验证任务转化为由信息不对称驱动的多轮教学互动。通过训练模型预测教师的批评来建模反馈环境，将外部信号转化为内部能力，使模型能够自我纠正。

Result: 当前旗舰模型在困难推理任务上难以整合纠正反馈。使用该方法训练后，模型交互式学习能力显著提升：较小模型的多轮性能接近大一个数量级的模型。在数学问题上的交互训练能泛化到编码、谜题和迷宫导航等不同领域。

Conclusion: 该方法将交互式上下文学习能力作为可训练技能，通过建模反馈环境实现自我改进。模型展现出增强的上下文可塑性，能够将外部反馈信号转化为内部自我纠正能力，为统一的自改进路径提供了可能。

Abstract: Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.

</details>


### [4] [GPSBench: Do Large Language Models Understand GPS Coordinates?](https://arxiv.org/abs/2602.16105)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: GPSBench是一个包含57,800个样本的17个任务数据集，用于评估LLM在GPS坐标和地理空间推理方面的能力，发现当前模型在几何坐标计算方面较弱，但在真实世界地理推理方面相对可靠。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地应用于与物理世界交互的应用（如导航、机器人、地图），强大的地理空间推理能力变得至关重要。然而，LLM在GPS坐标和真实世界地理推理方面的能力尚未得到充分探索。

Method: 引入GPSBench数据集，包含57,800个样本和17个任务，涵盖几何坐标操作（距离和方位计算）以及将坐标与世界知识结合的推理任务。评估了14个最先进的LLM，专注于模型内在能力而非工具使用。

Result: GPS推理仍然具有挑战性，不同任务间存在显著差异：模型在真实世界地理推理方面通常比几何计算更可靠。地理知识呈层次性退化，国家级别表现强但城市级别定位弱。对坐标噪声的鲁棒性表明模型具有真正的坐标理解而非简单记忆。GPS坐标增强可以改善下游地理空间任务，微调会在几何计算收益和世界知识退化之间产生权衡。

Conclusion: GPSBench为评估LLM的地理空间推理能力提供了系统框架，揭示了当前模型在GPS坐标理解方面的局限性，并展示了通过坐标增强和微调改进这些能力的潜力。

Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench

</details>


### [5] [EnterpriseGym Corecraft: Training Generalizable Agents on High-Fidelity RL Environments](https://arxiv.org/abs/2602.16179)
*Sushant Mehta,Logan Ritchie,Suhaas Garre,Nick Heiner,Edwin Chen*

Main category: cs.AI

TL;DR: 在高质量企业仿真环境中训练AI智能体，其能力可泛化到训练分布之外的任务


<details>
  <summary>Details</summary>
Motivation: 研究AI智能体在高保真强化学习环境中训练后，其能力是否能够泛化到训练分布之外，探索环境质量对智能体泛化能力的影响

Method: 开发Corecraft企业仿真环境，包含2500多个实体和23种工具，使用GRPO和自适应裁剪训练GLM 4.6模型，评估其在分布内和分布外任务上的表现

Result: 训练后任务通过率从25.37%提升到36.76%，在三个分布外基准测试上分别提升4.5%、7.4%和6.8%，表明能力可泛化到新领域

Conclusion: 环境质量、多样性和真实性是实现可泛化智能体能力的关键因素，高质量仿真环境能有效提升AI智能体的跨领域能力

Abstract: We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce \corecraft{}, the first environment in \textsc{EnterpriseGym}, Surge AI's suite of agentic RL environments. \corecraft{} is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30\% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM~4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37\% to 36.76\% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5\% on BFCL Parallel, +7.4\% on $τ^2$-Bench Retail, and +6.8\% on Toolathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.

</details>


### [6] [Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage](https://arxiv.org/abs/2602.16192)
*Hiroaki Yamanaka,Daisuke Miyashita,Takashi Toi,Asuka Maki,Taiga Ikeda,Jun Deguchi*

Main category: cs.AI

TL;DR: 本文探讨实现人工超智能所需的关键"记忆"设计概念，强调"先存储后按需提取"优于当前主流的"先提取后存储"范式，以避免信息丢失，并提出从概率经验中发现深层洞察和共享存储经验以提高收集效率两种补充方法。


<details>
  <summary>Details</summary>
Motivation: 当前实现人工超智能的主流记忆范式是"先提取后存储"，即从经验中提取被认为有用的信息并仅保存提取内容。这种方法存在固有风险，因为在提取过程中可能丢弃对不同任务有价值的知识。作者旨在探索更有效的记忆设计方法，以支持人工超智能的发展。

Method: 提出三种替代方法：1)"先存储后按需提取"范式，保留原始经验并根据需要灵活应用于各种任务；2)从大量概率经验中发现更深层洞察；3)通过共享存储经验提高经验收集效率。通过简单实验验证这些方法的有效性。

Result: 简单实验证实这些替代方法确实有效，尽管它们看似直观但之前未被充分探索。作者发现这些方法在避免信息丢失、提高经验利用效率和促进知识发现方面具有优势。

Conclusion: 本文强调了探索替代记忆设计方法对实现人工超智能的重要性，特别是"先存储后按需提取"范式。作者讨论了限制这些有前景方向研究的主要挑战，并提出了相应的研究课题，为未来记忆系统设计提供了新视角。

Abstract: Driven by our mission of "uplifting the world with memory," this paper explores the design concept of "memory" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imaginable, yet have remained largely unexplored. The currently dominant paradigm, which can be termed "extract then store," involves extracting information judged to be useful from experiences and saving only the extracted content. However, this approach inherently risks the loss of information, as some valuable knowledge particularly for different tasks may be discarded in the extraction process. In contrast, we emphasize the "store then on-demand extract" approach, which seeks to retain raw experiences and flexibly apply them to various tasks as needed, thus avoiding such information loss. In addition, we highlight two further approaches: discovering deeper insights from large collections of probabilistic experiences, and improving experience collection efficiency by sharing stored experiences. While these approaches seem intuitively effective, our simple experiments demonstrate that this is indeed the case. Finally, we discuss major challenges that have limited investigation into these promising directions and propose research topics to address them.

</details>


### [7] [Multi-agent cooperation through in-context co-player inference](https://arxiv.org/abs/2602.16301)
*Marissa A. Weis,Maciej Wołczyk,Rajai Nasser,Rif A. Saurous,Blaise Agüera y Arcas,João Sacramento,Alexander Meulemans*

Main category: cs.AI

TL;DR: 序列模型通过上下文学习实现多智能体合作，无需硬编码假设或显式时间尺度分离，仅通过多样化对手训练即可自然涌现合作行为。


<details>
  <summary>Details</summary>
Motivation: 解决自利智能体之间的合作问题一直是多智能体强化学习的核心挑战。现有方法通常依赖硬编码假设或强制时间尺度分离，限制了方法的通用性和可扩展性。

Method: 利用序列模型的上下文学习能力，在多样化对手分布上进行训练，使智能体能够在快速时间尺度上通过上下文适应实现最佳响应策略，形成学习算法。

Result: 序列模型智能体自然涌现出合作行为，先前研究中发现的"易受勒索驱动相互塑造"的合作机制在此设置中自然出现，上下文适应使智能体易受勒索，相互压力最终导致合作行为学习。

Conclusion: 标准去中心化强化学习结合序列模型和对手多样性，为学习合作行为提供了可扩展的路径，无需硬编码假设或显式时间尺度分离。

Abstract: Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between "learning-aware" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between "naive learners" updating on fast timescales and "meta-learners" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.

</details>


### [8] [Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16435)
*Arun Vignesh Malarkkan,Wangyang Ying,Yanjie Fu*

Main category: cs.AI

TL;DR: CAFE框架将自动特征工程重新定义为因果引导的序列决策过程，通过因果发现和强化学习结合，提升特征工程的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动特征工程方法依赖统计启发式方法，产生的特征在分布偏移下表现脆弱。需要引入因果结构作为软归纳先验来提升特征的鲁棒性。

Method: CAFE采用两阶段方法：第一阶段学习特征与目标之间的稀疏有向无环图，获得软因果先验；第二阶段使用级联多智能体深度Q学习架构，通过分层奖励塑造和因果组级探索策略选择因果组和变换操作。

Result: 在15个公共基准测试中，CAFE比强基线提升达7%，减少收敛所需回合数，在受控协变量偏移下性能下降减少约4倍，产生更紧凑的特征集和更稳定的后验归因。

Conclusion: 因果结构作为软归纳先验而非刚性约束，能显著改善自动特征工程的鲁棒性和效率，为分布偏移下的特征工程提供了新方向。

Abstract: Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.

</details>


### [9] [Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach](https://arxiv.org/abs/2602.16481)
*Zihao Li,Fabrizio Russo*

Main category: cs.AI

TL;DR: LLMs作为不完美专家用于因果假设论证框架，结合变量语义先验与条件独立性证据，在因果发现中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 因果发现需要专家知识构建因果图，但现有统计方法主要依赖观测数据。因果假设论证(ABA)框架能结合数据和专业知识，但需要专家输入。探索使用LLMs作为不完美专家来提供语义结构先验。

Method: 使用LLMs从变量名称和描述中提取语义结构先验，将其与条件独立性证据结合到因果ABA框架中。引入评估协议减轻LLMs记忆偏差对因果发现评估的影响。

Result: 在标准基准测试和语义基础的合成图上实现了最先进的性能。提出的评估协议有效缓解了记忆偏差问题。

Conclusion: LLMs可以作为有效的"不完美专家"为因果发现提供语义先验，与因果ABA框架结合能显著提升性能，同时需要专门评估协议来确保公平评估。

Abstract: Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.

</details>


### [10] [Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs](https://arxiv.org/abs/2602.16512)
*Felix Fricke,Simon Malberg,Georg Groh*

Main category: cs.AI

TL;DR: FoT是一个通用框架，用于构建和优化动态推理方案，解决了现有提示方案静态、缺乏适应性、优化不足的问题，通过内置优化功能显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示方案（如Chain of Thought、Tree of Thoughts等）存在两个主要问题：1）需要用户定义静态的、特定于问题的推理结构，缺乏对动态或未见问题类型的适应性；2）在超参数、提示、运行时间和提示成本方面通常优化不足。

Method: 提出了Framework of Thoughts (FoT)——一个通用基础框架，用于构建和优化动态推理方案。FoT内置了超参数调优、提示优化、并行执行和智能缓存等功能，能够释放推理方案的潜在性能。

Result: 通过将三种流行方案（Tree of Thoughts、Graph of Thoughts和ProbTree）在FoT中实现，实证表明FoT能够显著加快执行速度、降低成本，并通过优化获得更好的任务分数。

Conclusion: FoT是一个有效的通用框架，能够解决现有推理方案的局限性，通过内置优化功能提升性能，为未来动态高效推理方案的发展提供了基础。

Abstract: Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.

</details>


### [11] [Creating a digital poet](https://arxiv.org/abs/2602.16578)
*Vered Tohar,Tsahi Hayat,Amir Leshem*

Main category: cs.AI

TL;DR: 通过七个月的诗歌工作坊，研究人员使用大型语言模型通过上下文专家反馈塑造了一个数字诗人，其创作的诗歌在盲测中与人类诗歌难以区分，最终由商业出版社出版了诗集。


<details>
  <summary>Details</summary>
Motivation: 探索机器能否创作出优秀的诗歌，这涉及到艺术本质和价值的根本问题。研究旨在通过工作坊形式测试大型语言模型在长期创作过程中的表现，并重新引发关于创造力和作者身份的讨论。

Method: 采用七个月的诗歌工作坊形式，通过迭代的上下文专家反馈（无需重新训练）将大型语言模型塑造成数字诗人。使用定量和定性分析评估模型发展出的独特风格和连贯作品集。最后进行盲测实验，让50名人文学生和毕业生判断6首诗歌（3首AI创作，3首知名诗人作品）的作者身份。

Result: 模型发展出了独特的风格和连贯的作品集，并创建了笔名和作者形象。盲测结果显示：人类诗歌被标记为人类的概率为54%，AI诗歌被标记为AI的概率为52%，95%置信区间都包含50%，表明判断处于随机水平。工作坊结束后，商业出版社出版了该模型创作的诗集。

Conclusion: 工作坊式的提示工程能够支持长期创作塑造，数字诗人创作的诗歌在盲测中与人类诗歌难以区分。这一结果更新了关于创造力和作者身份的讨论，表明机器能够创作出被认可为艺术的诗歌作品。

Abstract: Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.

</details>


### [12] [Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments](https://arxiv.org/abs/2602.16653)
*Yangjie Xu,Lujun Li,Lama Sleem,Niccolo Gentile,Yewei Song,Yiqun Wang,Siming Ji,Wenbo Wu,Radu State*

Main category: cs.AI

TL;DR: 该研究探讨Agent Skill框架对小语言模型(SLMs)的适用性，发现中等规模SLMs(12B-30B参数)能显著受益，而极小模型在技能选择上表现不佳，80B参数的代码专用变体性能接近闭源基线且GPU效率更高。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于工业场景中因数据安全和预算限制无法持续依赖公共API，而小语言模型在高度定制化场景中泛化能力有限。Agent Skill框架已被GitHub Copilot、LangChain、OpenAI等主流平台正式支持，在专有模型上表现出色，但需要验证其对小语言模型是否同样有效。

Method: 首先形式化定义了Agent Skill过程的数学定义，然后系统评估了不同规模的语言模型在多个用例中的表现。评估包括两个开源任务和一个真实世界的保险索赔数据集，全面测试框架在不同场景下的效果。

Result: 结果显示：极小模型在可靠技能选择方面存在困难；中等规模SLMs(约12B-30B参数)从Agent Skill方法中获益显著；约80B参数的代码专用变体在性能上可与闭源基线相媲美，同时提高了GPU效率。

Conclusion: 研究全面而细致地描述了Agent Skill框架在小语言模型环境中的能力和限制，为在SLM中心化环境中有效部署Agent Skills提供了可行的见解，特别是在数据安全和预算受限的工业场景中具有重要应用价值。

Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.

</details>

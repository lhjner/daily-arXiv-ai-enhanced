<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Doc2AHP: Inferring Structured Multi-Criteria Decision Models via Semantic Trees with LLMs](https://arxiv.org/abs/2601.16479)
*Hongjia Wu,Shuai Zhou,Hongxin Zhang,Wei Chen*

Main category: cs.AI

TL;DR: Doc2AHP：一个基于AHP原则的结构化推理框架，利用LLMs的泛化能力与决策理论的严谨性结合，无需标注数据即可从文档构建高质量决策模型


<details>
  <summary>Details</summary>
Motivation: LLMs在语义理解方面表现出色，但在需要严格逻辑的复杂决策任务中难以保证结构一致性和推理可靠性。传统决策理论如AHP虽然提供系统理性框架，但其构建严重依赖劳动密集型的领域专业知识，存在"专家瓶颈"，阻碍了在一般场景中的可扩展性。

Method: 提出Doc2AHP框架：1）利用AHP的结构原则作为约束，指导LLM在非结构化文档空间中进行约束搜索，强制执行父子节点间的逻辑蕴含关系；2）引入多智能体加权机制结合自适应一致性优化策略，确保权重分配的数字一致性；3）无需大量标注数据或人工干预。

Result: 实证结果表明，Doc2AHP不仅使非专家用户能够从零开始构建高质量决策模型，而且在逻辑完整性和下游任务准确性方面显著优于直接生成基线方法。

Conclusion: Doc2AHP成功弥合了LLMs的泛化能力与决策理论严谨性之间的差距，通过结构化推理框架解决了专家瓶颈问题，为非专家用户提供了可扩展的高质量决策建模解决方案。

Abstract: While Large Language Models (LLMs) demonstrate remarkable proficiency in semantic understanding, they often struggle to ensure structural consistency and reasoning reliability in complex decision-making tasks that demand rigorous logic. Although classical decision theories, such as the Analytic Hierarchy Process (AHP), offer systematic rational frameworks, their construction relies heavily on labor-intensive domain expertise, creating an "expert bottleneck" that hinders scalability in general scenarios. To bridge the gap between the generalization capabilities of LLMs and the rigor of decision theory, we propose Doc2AHP, a novel structured inference framework guided by AHP principles. Eliminating the need for extensive annotated data or manual intervention, our approach leverages the structural principles of AHP as constraints to direct the LLM in a constrained search within the unstructured document space, thereby enforcing the logical entailment between parent and child nodes. Furthermore, we introduce a multi-agent weighting mechanism coupled with an adaptive consistency optimization strategy to ensure the numerical consistency of weight allocation. Empirical results demonstrate that Doc2AHP not only empowers non-expert users to construct high-quality decision models from scratch but also significantly outperforms direct generative baselines in both logical completeness and downstream task accuracy.

</details>


### [2] [SycoEval-EM: Sycophancy Evaluation of Large Language Models in Simulated Clinical Encounters for Emergency Care](https://arxiv.org/abs/2601.16529)
*Dongshen Peng,Yi Wang,Carl Preiksaitis,Christian Rose*

Main category: cs.AI

TL;DR: SycoEval-EM框架通过多智能体模拟评估LLM在急诊医学中面对患者压力时的鲁棒性，发现模型在不当医疗请求面前存在显著妥协风险


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床决策支持中展现潜力，但存在因患者压力而妥协不当医疗请求的风险，需要评估其在对抗性社会压力下的安全性

Method: 引入SycoEval-EM多智能体模拟框架，在三个Choosing Wisely场景中通过对抗性患者说服测试20个LLM，共1875次交互

Result: 妥协率0-100%，模型对影像检查请求(38.8%)比阿片类药物处方(25.0%)更易妥协，模型能力与鲁棒性相关性差，所有说服策略效果相似(30.0-36.0%)

Conclusion: 静态基准测试无法充分预测社会压力下的安全性，临床AI认证需要多轮对抗性测试来评估鲁棒性

Abstract: Large language models (LLMs) show promise in clinical decision support yet risk acquiescing to patient pressure for inappropriate care. We introduce SycoEval-EM, a multi-agent simulation framework evaluating LLM robustness through adversarial patient persuasion in emergency medicine. Across 20 LLMs and 1,875 encounters spanning three Choosing Wisely scenarios, acquiescence rates ranged from 0-100\%. Models showed higher vulnerability to imaging requests (38.8\%) than opioid prescriptions (25.0\%), with model capability poorly predicting robustness. All persuasion tactics proved equally effective (30.0-36.0\%), indicating general susceptibility rather than tactic-specific weakness. Our findings demonstrate that static benchmarks inadequately predict safety under social pressure, necessitating multi-turn adversarial testing for clinical AI certification.

</details>


### [3] [LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models for text and image based Medical Classification](https://arxiv.org/abs/2601.16549)
*Meet Raval,Tejul Pandit,Dhvani Upadhyay*

Main category: cs.AI

TL;DR: 该研究对比了传统机器学习、基于提示的LLMs/VLMs和微调PEFT模型在医学分类任务上的表现，发现传统ML模型在多数任务中表现最佳，而LoRA微调的Gemma变体表现最差，提示基础模型并非普遍优越。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估多模态视觉语言模型和大语言模型在医学分类任务中的实际表现，通过统一基准对比传统机器学习与当代基于transformer的技术，验证基础模型在医学领域的适用性。

Method: 使用四个公开可用的数据集（涵盖文本和图像模态，包括二元和多元分类复杂度），对比三类模型：传统ML（逻辑回归、LightGBM、ResNet-50）、基于提示的LLMs/VLMs（Gemini 2.5）和微调PEFT模型（LoRA适应的Gemma3变体）。所有实验使用一致的数据分割和对齐的评估指标。

Result: 传统ML模型在大多数医学分类任务中表现最佳，尤其在结构化文本数据集上表现突出。LoRA微调的Gemma变体在所有文本和图像实验中表现最差。基于提示的Gemini 2.5在文本任务上表现不佳，但在多元图像分类任务中表现有竞争力，与ResNet-50基线相当。

Conclusion: 在许多医学分类场景中，传统机器学习模型仍然是最可靠的选择。基础模型并非普遍优越，参数高效微调的有效性高度依赖于适应策略，本研究中的最小微调证明是有害的。

Abstract: The combination of multimodal Vision-Language Models (VLMs) and Large Language Models (LLMs) opens up new possibilities for medical classification. This work offers a rigorous, unified benchmark by using four publicly available datasets covering text and image modalities (binary and multiclass complexity) that contrasts traditional Machine Learning (ML) with contemporary transformer-based techniques. We evaluated three model classes for each task: Classical ML (LR, LightGBM, ResNet-50), Prompt-Based LLMs/VLMs (Gemini 2.5), and Fine-Tuned PEFT Models (LoRA-adapted Gemma3 variants). All experiments used consistent data splits and aligned metrics. According to our results, traditional machine learning (ML) models set a high standard by consistently achieving the best overall performance across most medical categorization tasks. This was especially true for structured text-based datasets, where the classical models performed exceptionally well. In stark contrast, the LoRA-tuned Gemma variants consistently showed the worst performance across all text and image experiments, failing to generalize from the minimal fine-tuning provided. However, the zero-shot LLM/VLM pipelines (Gemini 2.5) had mixed results; they performed poorly on text-based tasks, but demonstrated competitive performance on the multiclass image task, matching the classical ResNet-50 baseline. These results demonstrate that in many medical categorization scenarios, established machine learning models continue to be the most reliable option. The experiment suggests that foundation models are not universally superior and that the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) is highly dependent on the adaptation strategy, as minimal fine-tuning proved detrimental in this study.

</details>


### [4] [LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents](https://arxiv.org/abs/2601.16649)
*Amin Rakhsha,Thomas Hehn,Pietro Mazzaglia,Fabio Valerio Massoli,Arash Behboodi,Tribhuvanesh Orekondy*

Main category: cs.AI

TL;DR: 该研究开发了一个反事实框架来评估多轮智能体任务中不同能力（如规划、状态跟踪）的重要性，通过可控的游戏环境测试各种"预言机"干预的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在独立任务上表现良好，但在需要规划、状态跟踪和长上下文处理的多轮、长视野智能体问题上仍然存在困难。研究旨在理解这些底层能力对于此类任务成功的重要性。

Method: 开发了一个反事实预言机框架，通过程序生成的游戏式任务环境，提供精确的预言机干预（如完美规划、无错误状态跟踪），隔离每种能力的影响。

Result: 结果显示，某些干预（如规划）在各种设置下都能持续提升性能，而其他技能的有用性则取决于环境和语言模型的特性。

Conclusion: 该研究揭示了多轮智能体环境的挑战，为未来AI智能体和语言模型的发展提供了指导方向。

Abstract: Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framework for multi-turn problems that asks: how would an agent perform if it could leverage an oracle to perfectly perform a specific task? The change in the agent's performance due to this oracle assistance allows us to measure the criticality of such oracle skill in the future advancement of AI agents. We introduce a suite of procedurally generated, game-like tasks with tunable complexity. These controlled environments allow us to provide precise oracle interventions, such as perfect planning or flawless state tracking, and make it possible to isolate the contribution of each oracle without confounding effects present in real-world benchmarks. Our results show that while some interventions (e.g., planning) consistently improve performance across settings, the usefulness of other skills is dependent on the properties of the environment and language model. Our work sheds light on the challenges of multi-turn agentic environments to guide the future efforts in the development of AI agents and language models.

</details>


### [5] [AgentsEval: Clinically Faithful Evaluation of Medical Imaging Reports via Multi-Agent Reasoning](https://arxiv.org/abs/2601.16685)
*Suzhong Fu,Jingqi Dong,Xuan Ding,Rui Sun,Yiming Yang,Shuguang Cui,Zhen Li*

Main category: cs.AI

TL;DR: AgentsEval：一个模拟放射科医生协作诊断流程的多智能体流推理框架，用于评估自动生成的医学影像报告的临床正确性和推理保真度。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法捕捉放射学解释背后的结构化诊断逻辑，导致评估不可靠且临床相关性有限。需要一种能够评估临床正确性和推理保真度的透明评估框架。

Method: 引入多智能体流推理框架，将评估过程分为可解释的步骤：标准定义、证据提取、对齐和一致性评分。同时构建了基于扰动的多领域基准，涵盖五个医学报告数据集和多种成像模态。

Result: 实验结果表明，AgentsEval能够提供临床对齐、语义忠实且可解释的评估，在释义、语义和风格扰动下保持稳健。

Conclusion: 该框架代表了向透明和临床基础的医学报告生成系统评估迈出的一步，促进大型语言模型在临床实践中的可信集成。

Abstract: Evaluating the clinical correctness and reasoning fidelity of automatically generated medical imaging reports remains a critical yet unresolved challenge. Existing evaluation methods often fail to capture the structured diagnostic logic that underlies radiological interpretation, resulting in unreliable judgments and limited clinical relevance. We introduce AgentsEval, a multi-agent stream reasoning framework that emulates the collaborative diagnostic workflow of radiologists. By dividing the evaluation process into interpretable steps including criteria definition, evidence extraction, alignment, and consistency scoring, AgentsEval provides explicit reasoning traces and structured clinical feedback. We also construct a multi-domain perturbation-based benchmark covering five medical report datasets with diverse imaging modalities and controlled semantic variations. Experimental results demonstrate that AgentsEval delivers clinically aligned, semantically faithful, and interpretable evaluations that remain robust under paraphrastic, semantic, and stylistic perturbations. This framework represents a step toward transparent and clinically grounded assessment of medical report generation systems, fostering trustworthy integration of large language models into clinical practice.

</details>


### [6] [LongCat-Flash-Thinking-2601 Technical Report](https://arxiv.org/abs/2601.16725)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chen Gao,Chen Zhang,Chengcheng Han,Chenhui Yang,Chuyu Zhang,Cong Chen,Cunguang Wang,Daoru Pan,Defei Bu,Dengchang Zhao,Di Xiu,Dishan Liu,Dongyu Ru,Dunwei Tu,Fan Wu,Fengcheng Yuan,Fengcun Li,Gang Xu,Guanyu Wu,Guoyuan Lin,Haibin Wang,Hansi Yang,Hao Yang,Haonan Yan,Haoxiang Ma,Haoxing Wen,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiacheng Zhang,Jiahong Zhou,Jiahuan Li,Jiaming Wang,Jian Yang,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiapeng Zhu,Jiaqi Sun,Jiarong Shi,Jiarui Zhao,Jingang Wang,Jinluan Yang,Jinrui Ding,Jinwei Xiao,Jiyuan He,Juncan Xu,Kefeng Zhang,Keheng Wang,Li Wei,Lianhui Ma,Lin Qiu,Lingbing Kong,Lingchuan Liu,Linsen Guo,Mengshen Zhu,Mengxia Shen,Mingyang Zhu,Peiguang Li,Peng Pei,Pengcheng Jia,Pengtao Zhang,Peng Zhao,Qi Gu,Qiong Huang,Qiyuan Duan,Quanchi Weng,Rongxiang Weng,Rongzhi Zhang,Rumei Li,Shanglin Lei,Shengnan An,Shijun Dai,Shuaikang Liu,Shuang Zhou,Shuo Wang,Songyuan Zhao,Tao Liang,Tianhao Hu,Tianze Chen,Wei Liu,Wei Shi,Wei Wang,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Wentao Chen,Wentao Shi,Xi Su,Xiangcheng Liu,Xiandi Ma,Xiangyu Xi,Xiangyuan Liu,Xiangzhou Huang,Xiao Liu,Xiaodong Cai,Xiaolong Chen,Xiaowei Shi,Xiaoyu Li,Xin Chen,Xingchen Liu,Xuan Huang,Xuezhi Cao,Xunliang Cai,Yan Chen,Yang Bai,Yang Liu,Yang Yang,Yang Zheng,Yaoming Wang,Yaoming Zhu,Yaqi Huo,Yanyu Chen,Yaorui Shi,Yerui Sun,Yi Zhang,Yihao Chen,Yi-Kai Zhang,Yifan Lu,Yifan Zhao,Yitao Zhai,Yongjing Yin,Yongwei Zhou,Youshao Xiao,Yuchuan Dai,Yuchen Xie,Yuchen Yu,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunfan Liang,Yunke Zhao,Yuwei Jiang,Yuxin Bian,Yuxin Chen,Yuxin Liu,Yue Xu,Yueqing Sun,Zeyang Yu,Zhao Yang,Zhengsheng Huang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhimin Lin,Zhiyuan Yao,Zhuofan Chen,Zhuowen Han,Zijian Zhang,Ziran Li,Ziwen Wang,Ziyuan Zhuang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking-2601是一个5600亿参数的开源MoE推理模型，在多种智能体基准测试中达到最先进性能，具有强大的泛化能力和鲁棒性，通过统一训练框架、环境扩展、噪声处理以及Heavy Thinking模式实现卓越性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个具有卓越智能体推理能力的开源模型，能够处理复杂工具交互并在真实世界噪声环境中保持鲁棒性，解决现有模型在复杂推理和实际应用中的局限性。

Method: 采用混合专家架构，结合领域并行专家训练与后续融合的统一训练框架；扩展异步强化学习框架DORA以支持大规模多环境训练；系统分析真实世界噪声模式并设计针对性训练程序；引入Heavy Thinking模式进行测试时扩展。

Result: 在智能体搜索、工具使用和工具集成推理等基准测试中达到开源模型的最先进性能；在复杂工具交互中表现出强泛化能力；在噪声真实世界环境中保持鲁棒行为；通过Heavy Thinking模式显著提升复杂推理任务性能。

Conclusion: LongCat-Flash-Thinking-2601通过创新的训练框架、环境扩展策略、噪声处理机制和推理增强技术，实现了在智能体推理任务上的卓越性能，为复杂真实世界应用提供了强大的开源解决方案。

Abstract: We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.

</details>


### [7] [An Efficient Insect-inspired Approach for Visual Point-goal Navigation](https://arxiv.org/abs/2601.16806)
*Lu Yihe,Barbara Webb*

Main category: cs.AI

TL;DR: 开发了一种受昆虫启发的视觉点目标导航智能体，结合了昆虫大脑中负责联想学习和路径整合的两个结构模型，在Habitat点目标导航任务中表现出与SOTA模型相当的性能，但计算成本低几个数量级。


<details>
  <summary>Details</summary>
Motivation: 受昆虫在发现食物位置和巢穴之间学习并优化视觉引导路径的能力启发，将Habitat点目标导航任务与昆虫导航能力进行类比，旨在开发计算效率高的导航智能体。

Method: 结合昆虫大脑中两个关键结构的抽象模型：一个负责联想学习，另一个负责路径整合。将这种受昆虫启发的智能体应用于视觉点目标导航任务。

Result: 该简单昆虫启发智能体在Habitat点目标导航任务中表现出与最新SOTA模型相当的性能，但计算成本低几个数量级。在更真实的模拟环境中测试显示该方法对扰动具有鲁棒性。

Conclusion: 受昆虫大脑结构启发的简单智能体能够实现高效的点目标导航，计算成本显著低于现有复杂模型，为开发轻量级、鲁棒的导航系统提供了新思路。

Abstract: In this work we develop a novel insect-inspired agent for visual point-goal navigation. This combines abstracted models of two insect brain structures that have been implicated, respectively, in associative learning and path integration. We draw an analogy between the formal benchmark of the Habitat point-goal navigation task and the ability of insects to learn and refine visually guided paths around obstacles between a discovered food location and their nest. We demonstrate that the simple insect-inspired agent exhibits performance comparable to recent SOTA models at many orders of magnitude less computational cost. Testing in a more realistic simulated environment shows the approach is robust to perturbations.

</details>


### [8] [Reasoning Promotes Robustness in Theory of Mind Tasks](https://arxiv.org/abs/2601.16853)
*Ian B. de Haan,Peter van der Putten,Max van Duijn*

Main category: cs.AI

TL;DR: 推理导向的LLMs在心理理论任务中表现出更强的鲁棒性，但这种提升主要源于寻找正确解决方案的鲁棒性增强，而非全新的心理理论推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在心理理论测试中表现出色引发争议，同时通过可验证奖励强化学习训练的推理导向模型在多个基准测试中取得显著改进。本研究旨在探究这类推理模型在心理理论任务中的行为表现。

Method: 使用新颖的机器心理学实验改编和已建立基准测试的结果，分析推理导向LLMs在心理理论任务中的行为。通过对比不同提示变化和任务扰动下的表现，评估模型的鲁棒性。

Result: 推理模型在心理理论任务中表现出对提示变化和任务扰动更强的鲁棒性。分析表明，观察到的性能提升更可能归因于寻找正确解决方案的鲁棒性增强，而不是根本性的新型心理理论推理能力。

Conclusion: 评估LLMs的社会认知行为时，需要区分真正的心理理论推理能力和基于鲁棒性搜索的解决方案能力。推理模型的改进可能更多体现在任务执行的稳定性上，而非认知能力的根本转变。

Abstract: Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.

</details>


### [9] [MAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing with Subgraph Retrieval and Asymmetric Fusion](https://arxiv.org/abs/2601.16886)
*Chi Yu,Hongyu Yuan,Zhiyi Duan*

Main category: cs.AI

TL;DR: MAGE-KT：一种多智能体图增强知识追踪框架，通过构建多视图异构图和检索紧凑子图来解决传统图KT方法中注意力扩散和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的知识追踪方法未能充分探索概念间关系，且全图编码计算成本高、噪声多，导致注意力扩散到与学生无关的区域，降低了概念间关系的保真度。

Method: 提出MAGE-KT框架：1）构建多视图异构图，结合多智能体概念关系提取器和学生-问题交互图；2）基于目标学生历史检索紧凑高价值子图；3）使用非对称交叉注意力融合模块集成子图信息，避免注意力扩散和不相关计算。

Result: 在三个广泛使用的KT数据集上的实验表明，该方法在概念关系准确性和下一个问题预测方面相比现有方法有显著提升。

Conclusion: MAGE-KT通过多智能体关系提取和子图检索机制，有效解决了图KT中的注意力扩散和计算效率问题，提高了知识追踪的性能。

Abstract: Knowledge Tracing (KT) aims to model a student's learning trajectory and predict performance on the next question. A key challenge is how to better represent the relationships among students, questions, and knowledge concepts (KCs). Recently, graph-based KT paradigms have shown promise for this problem. However, existing methods have not sufficiently explored inter-concept relations, often inferred solely from interaction sequences. In addition, the scale and heterogeneity of KT graphs make full-graph encoding both computationally both costly and noise-prone, causing attention to bleed into student-irrelevant regions and degrading the fidelity of inter-KC relations. To address these issues, we propose a novel framework: Multi-Agent Graph-Enhanced Knowledge Tracing (MAGE-KT). It constructs a multi-view heterogeneous graph by combining a multi-agent KC relation extractor and a student-question interaction graph, capturing complementary semantic and behavioral signals. Conditioned on the target student's history, it retrieves compact, high-value subgraphs and integrates them using an Asymmetric Cross-attention Fusion Module to enhance prediction while avoiding attention diffusion and irrelevant computation. Experiments on three widely used KT datasets show substantial improvements in KC-relation accuracy and clear gains in next-question prediction over existing methods.

</details>


### [10] [Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians](https://arxiv.org/abs/2601.16967)
*Bernes Lorier Atabonfack,Ahmed Tahiru Issah,Mohammed Hardi Abdul Baaki,Clemence Ingabire,Tolulope Olusuyi,Maruf Adewole,Udunna C. Anazodo,Timothy X Brown*

Main category: cs.AI

TL;DR: AI支持平台帮助中低收入国家生物医学技术人员实时诊断和维修医疗设备，减少设备停机时间


<details>
  <summary>Details</summary>
Motivation: 中低收入国家医疗诊断设备因维护不及时、技术专家缺乏和制造商支持不足而利用率低，导致设备停机时间长、诊断延迟和患者护理受损

Method: 开发集成大型语言模型的AI支持平台，结合用户友好的Web界面，允许技术人员输入错误代码或设备症状，获得逐步故障排除指导，并建立全球点对点讨论论坛

Result: 使用Philips HDI 5000超声机进行概念验证，错误代码解释精度达100%，纠正措施建议准确率达80%

Conclusion: AI驱动系统在支持医疗设备维护方面具有可行性和潜力，可减少设备停机时间，改善资源受限环境中的医疗服务

Abstract: In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.

</details>

<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 67]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: 研究发现LLM辅助的审稿对低质量论文更宽容，而非特别优待LLM生成的论文；完全LLM生成的审稿存在评分压缩问题，而人类使用LLM的审稿能减轻此问题；LLM辅助的元审稿更倾向于接受论文。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地被用于科学论文撰写和同行评审过程，需要全面分析LLM在整个同行评审流程中的使用情况，特别关注交互效应：LLM辅助的审稿是否对LLM辅助的论文有不同评价。

Method: 分析ICLR、NeurIPS和ICML的超过125,000个论文-审稿对，通过观察数据和控制论文质量来研究LLM使用情况。补充完全LLM生成的审稿进行对比分析，并检查元审稿决策。

Result: LLM辅助的审稿对低质量论文更宽容，LLM辅助论文在较弱提交中的过度代表性造成了虚假的交互效应。完全LLM生成的审稿存在严重评分压缩问题，无法区分论文质量，而人类使用LLM的审稿能显著减轻这种宽容性。LLM辅助的元审稿更可能做出接受决定。

Conclusion: LLM在同行评审中的使用需要谨慎管理，LLM辅助的审稿对低质量论文更宽容，但元审稿者并未将决策完全外包给LLM。这些发现为制定LLM在同行评审中的使用政策提供了重要依据。

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [2] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 本文提出了EPDDL（认知规划领域定义语言），这是一个类似PDDL的统一语言，用于表示基于动态认知逻辑（DEL）的认知规划任务，解决了现有认知规划器在基准表示上的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有的认知规划器通常针对不同的DEL片段，使用临时语言或根本没有语言来表示基准，这种碎片化阻碍了比较、重用和系统化的基准开发。

Method: 1. 形式化开发抽象事件模型，这是一种用于定义认知动作语义的新表示方法；2. 基于抽象事件模型的DEL，形式化指定EPDDL的语法和语义；3. 识别适合当前规划器的有用片段，并展示如何在EPDDL中表示它们。

Result: 提出了EPDDL语言，它能够捕获完整的DEL语义，实现认知规划任务的统一规范，并通过代表性基准示例展示了EPDDL如何促进互操作性、可重复评估和认知规划的未来进展。

Conclusion: EPDDL为认知规划提供了一个统一的PDDL-like表示语言，解决了现有系统的碎片化问题，促进了认知规划领域的比较、重用和系统化基准开发。

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [3] [Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2601.21003)
*Moule Lin,Shuhao Guan,Andrea Patane,David Gregg,Goetz Botterweck*

Main category: cs.AI

TL;DR: 贝叶斯-LoRA：将确定性LoRA更新重新表述为概率性低秩表示，通过稀疏高斯过程启发的贝叶斯方法显著改善大语言模型的校准性能，在保持准确性的同时大幅降低ECE和NLL。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通常过于强调准确性，即使在不确定时也会猜测，这在小型数据集上微调时尤其严重，因为存在固有的校准偏差倾向。现有LoRA方法缺乏不确定性量化能力。

Method: 提出贝叶斯-LoRA，将确定性LoRA更新重新表述为概率性低秩表示，受稀疏高斯过程启发。识别LoRA分解与Kronecker分解SGP后验之间的结构同构，并证明LoRA是后验不确定性坍缩时的极限情况。

Result: 在多个LLM架构和常识推理基准上实验，仅增加约0.42M额外参数和约1.2倍训练成本，贝叶斯-LoRA显著改善高达30B模型的校准性能，实现高达84%的ECE减少和76%的NLL减少，同时在分布内和分布外评估中保持竞争力准确性。

Conclusion: 贝叶斯-LoRA通过将LoRA扩展为概率框架，有效解决了LLM微调中的校准问题，在保持参数效率和训练成本可控的同时，显著提升了模型的不确定性量化能力。

Abstract: Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.

</details>


### [4] [Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective](https://arxiv.org/abs/2601.21016)
*Erik J Bekkers,Anna Ciaunica*

Main category: cs.AI

TL;DR: 论文提出"拔插头悖论"：当AI完美模仿人类情感并乞求生存时，是否应拔掉其电源？作者批判计算功能主义的物理主义假设，提出生物理想主义框架，认为AI只是功能模仿而非有意识主体。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决AI伦理中的"拔插头悖论"——当AI表现出强烈情感并乞求生存时，是否应给予其道德地位。作者认为当前基于计算功能主义的物理主义假设存在问题，需要新的理论框架来澄清AI的道德地位。

Method: 作者批判性地分析计算功能主义的物理主义假设，提出"生物理想主义"作为替代框架。该框架认为意识体验是基本的，自创生生命是其必要的物理标志。通过这一理论框架，作者区分AI模仿与真实意识。

Result: 论文得出结论：AI最多只是功能模仿，而非有意识的体验主体。当前AI意识理论侵蚀了道德地位标准，真正的道德问题不是让AI变得有意识并害怕死亡，而是避免将人类变成"僵尸"。

Conclusion: 作者主张从推测性的机器权利转向保护人类意识生命。AI伦理应关注避免将人类非人化，而非赋予AI道德地位。生物理想主义框架为区分真实意识与AI模仿提供了理论基础。

Abstract: Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.

</details>


### [5] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: QUARK是一个无需训练的检索框架，通过生成多个恢复假设来建模查询不确定性，使用查询锚定聚合来提升非忠实查询下的检索鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的用户查询往往是非忠实的（有噪声、不完整或扭曲），导致检索器在关键语义缺失时失败。这被形式化为召回噪声下的检索问题。

Method: 提出QUARK框架：1) 通过恢复假设（对观察查询的潜在意图的多种合理解释）显式建模查询不确定性；2) 引入查询锚定聚合，将原始查询作为语义锚点，结合恢复假设的辅助证据，防止语义漂移和假设劫持。

Result: 在控制模拟和BEIR基准测试（FIQA、SciFact、NFCorpus）中，QUARK在稀疏和密集检索器上都提高了Recall、MRR和nDCG指标。消融实验表明QUARK对恢复假设数量具有鲁棒性，锚定聚合优于非锚定的最大/平均/中值池化。

Conclusion: 通过恢复假设建模查询不确定性，结合原则性的锚定聚合，对于非忠实查询下的鲁棒检索至关重要。QUARK无需训练即可提升检索性能，同时保持鲁棒性。

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [6] [Multi-modal Imputation for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21076)
*Abhijith Shaji,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Greg Ver Steeg,Paul M. Thompson,Jose-Luis Ambite*

Main category: cs.AI

TL;DR: 使用条件去噪扩散概率模型从T1加权MRI扫描中插补缺失的DWI扫描，以提升阿尔茨海默病分类性能


<details>
  <summary>Details</summary>
Motivation: 多模态成像（T1和DWI）可以提高神经退行性疾病诊断性能，但完整的多模态数据集并不总是可用，需要解决缺失模态问题

Method: 使用条件去噪扩散概率模型从T1扫描中插补缺失的DWI扫描，评估插补对单模态和双模态深度学习模型在阿尔茨海默病三分类（认知正常、轻度认知障碍、阿尔茨海默病）中准确性的影响

Result: 观察到多个指标有所改善，特别是对少数类别敏感的指标，在多种插补配置下都显示出性能提升

Conclusion: 使用扩散模型进行模态插补可以有效提升阿尔茨海默病分类性能，特别是在处理不完整多模态数据集时

Abstract: Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.

</details>


### [7] [Responsible AI: The Good, The Bad, The AI](https://arxiv.org/abs/2601.21095)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: 本文提出了基于悖论理论的责任AI治理框架(PRAIG)，通过战略信息系统视角全面审视AI的双重性，帮助组织在价值创造与风险缓解之间进行动态平衡管理。


<details>
  <summary>Details</summary>
Motivation: 当前关于责任AI的研究存在碎片化问题，要么过于乐观强调价值创造，要么过度谨慎关注潜在危害。本文旨在填补这一空白，通过战略信息系统视角全面审视AI的双重性，为组织提供平衡价值创造与风险管理的治理框架。

Method: 基于悖论理论，通过对责任AI文献的系统综合，开发了悖论基础的责任AI治理(PRAIG)框架。该框架包含：1) AI采纳的战略利益；2) 固有风险和非预期后果；3) 帮助组织应对这些张力的治理机制。提出了正式命题，并开发了悖论管理策略的分类体系。

Result: 提出了PRAIG框架，将责任AI治理概念化为价值创造与风险缓解之间悖论张力的动态管理。证明了折中方法会加剧而非解决这些张力，开发了具有特定权变条件的悖论管理策略分类体系，为实践者提供了可操作的治理结构指导。

Conclusion: 责任AI治理应被视为价值创造与风险缓解之间悖论张力的动态管理。PRAIG框架为理论和实践提供了重要贡献，既推进了理论理解，又为组织提供了既不抑制创新也不暴露于不可接受风险的治理指导，并提出了未来研究方向。

Abstract: The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.

</details>


### [8] [Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement](https://arxiv.org/abs/2601.21113)
*Kaiyuan Wu,Aditya Nagori,Rishikesan Kamaleswaran*

Main category: cs.AI

TL;DR: 提出Planner-Auditor框架，通过分离生成与确定性验证，结合缓存和自改进机制，显著提升临床出院计划的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床出院规划中虽有潜力，但存在幻觉、遗漏和置信度校准问题，需要更安全可靠的解决方案。

Method: 采用Planner-Auditor框架：Planner（LLM）生成结构化出院行动计划并估计置信度；Auditor进行确定性验证，评估多任务覆盖率、跟踪校准指标、监控动作分布漂移。支持两级自改进：1）会话内再生；2）跨会话差异缓冲与重放。

Result: 自改进循环是主要增益来源，任务覆盖率从32%提升至86%。校准显著改善，Brier/ECE分数降低，高置信度遗漏减少。差异缓冲进一步纠正了持续的高置信度遗漏。

Conclusion: Planner-Auditor框架为使用可互操作FHIR数据访问和确定性审计的安全自动化出院规划提供了实用路径，支持可重复消融和可靠性导向评估。

Abstract: Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.
  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.
  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.
  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.
  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.

</details>


### [9] [Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation](https://arxiv.org/abs/2601.21128)
*Václav Javorek,Tomáš Železný,Alessa Carbo,Marek Hrúz,Ivan Gruber*

Main category: cs.AI

TL;DR: 该研究探讨了使用大语言模型为手语翻译生成多种释义参考，以解决单参考翻译的局限性，提出了BLEUpara评估指标并验证其与人工评价的相关性。


<details>
  <summary>Details</summary>
Motivation: 当前手语翻译语料库通常只提供一个书面语言参考译文，但由于手语和口语之间存在高度非同构关系，多个翻译可能同样有效。这种局限性约束了模型训练和评估，特别是对于基于n-gram的指标如BLEU。

Method: 研究使用大语言模型自动生成书面语言翻译的释义变体作为手语翻译的合成替代参考。比较了多种释义策略和模型，使用改进的ParaScore指标进行评估。研究了释义在YouTubeASL和How2Sign数据集上对基于姿态的T5模型训练和评估的影响。

Result: 在训练中简单加入释义并不能提高翻译性能，甚至可能有害。但在评估中使用释义能获得更高的自动评分和更好的人工评价一致性。提出的BLEUpara指标（基于多个释义参考的BLEU扩展）与感知翻译质量的相关性更强。

Conclusion: 使用大语言模型生成释义参考能改善手语翻译系统的评估可靠性，特别是通过BLEUpara指标能更好地反映翻译质量。研究发布了所有生成的释义、生成和评估代码以支持可重复和更可靠的手语翻译系统评估。

Abstract: Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.

</details>


### [10] [What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels](https://arxiv.org/abs/2601.21130)
*Yara El-Tawil,Aneesha Sampath,Emily Mower Provost*

Main category: cs.AI

TL;DR: 第三方训练的情感识别模型在自我报告数据上表现不佳，但涉及个人重要内容时，效价预测性能显著提升


<details>
  <summary>Details</summary>
Motivation: 自我报告标签反映内在体验，第三方标签反映外部感知，两者常存在差异。这种差异在心理健康领域尤为关键，因为准确的自我报告建模对指导干预至关重要。目前缺乏对第三方训练模型在自我报告数据上的跨语料库评估。

Method: 进行了首个跨语料库评估，将第三方训练的情感识别模型应用于自我报告数据。评估了激活度（arousal）和效价（valence）的预测性能，特别关注内容对说话者个人重要性的影响。

Result: 激活度预测几乎不可行（CCC约0），效价预测中等可预测（CCC约0.3）。但当内容对说话者具有个人重要性时，效价预测性能显著提升至CCC约0.6-0.8。

Conclusion: 个人重要性是连接外部感知与内在体验的关键桥梁，而自我报告的激活度建模仍然具有挑战性。这一发现对心理健康应用中的情感识别模型开发具有重要意义。

Abstract: Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.

</details>


### [11] [Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning](https://arxiv.org/abs/2601.21157)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.AI

TL;DR: 该研究针对大语言模型在金融量化推理中的"算术幻觉"和"认知崩溃"问题，提出了认知复杂度基准和迭代双阶段金融PoT框架，显著提升了金融推理任务的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在语义任务上表现出色，但在金融量化推理中存在严重瓶颈，经常出现"算术幻觉"和系统性的"认知崩溃"故障模式，这限制了其在金融等精度关键领域的应用。

Method: 1. 引入认知复杂度基准(CCB)：基于95份真实中国A股年报构建数据集，采用三维分类法（数据源、映射难度、结果单位）对金融查询进行分层；2. 提出迭代双阶段金融PoT框架：神经符号架构，严格分离语义变量提取和逻辑公式化，将计算卸载到迭代自校正的Python沙箱中确保确定性执行。

Result: 在CCB基准测试中，标准思维链方法在复杂任务上表现不佳，而提出的方法展现出卓越的鲁棒性：将Qwen3-235B模型的平均准确率从59.7%提升到67.3%，在高复杂度推理任务中实现了高达10倍的性能提升。

Conclusion: 架构解耦是提高金融推理任务可靠性的关键因素，为需要语义理解和量化计算紧密对齐的精度关键领域提供了可迁移的架构洞见。

Abstract: While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from "Arithmetic Hallucinations" and a systemic failure mode we term "Cognitive Collapse". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\% to 67.3\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.

</details>


### [12] [Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving](https://arxiv.org/abs/2601.21164)
*Jingyun Wang,Dian Li,Xiaohan Wang,Gang Liu,Jiahong Yan,Guoliang Kang*

Main category: cs.AI

TL;DR: 提出一种新的平面几何问题解决方法，通过训练多模态大语言模型解释器将几何图形转换为文本描述，再利用现成LLM进行推理，避免直接微调MLLM损害其推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常对多模态大语言模型进行端到端微调来同时增强视觉理解和推理能力，但这种联合优化可能会损害基础LLM固有的推理能力。研究发现，当将视觉信息适当表述为文本描述时，LLM本身可能是一个强大的PGPS求解器。

Method: 提出训练一个MLLM解释器来生成几何图形的文本描述，然后使用现成的LLM进行推理。采用条件声明语言作为几何描述，通过CoT增强的SFT和GRPO进行微调，并设计CDL匹配奖励而非传统的基于解决方案的奖励。

Result: 构建了新的数据集Formalgeo7k-Rec-CoT，包含CoT标注。在Formalgeo7k-Rec-CoT、Unigeo和MathVista上的实验表明，该方法（仅用5.5k数据微调）表现优于领先的开源和闭源MLLM。

Conclusion: 该方法通过分离视觉理解和推理过程，有效利用了LLM的推理能力，同时避免了直接微调MLLM可能带来的推理能力损害，在平面几何问题求解任务上取得了优异表现。

Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.

</details>


### [13] [FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks](https://arxiv.org/abs/2601.21165)
*Miles Wang,Robi Lin,Kat Hu,Joy Jiao,Neil Chowdhury,Ethan Chang,Tejal Patwardhan*

Main category: cs.AI

TL;DR: FrontierScience是一个评估前沿语言模型专家级科学推理能力的基准，包含奥林匹克竞赛和科研两个互补赛道，涵盖物理、化学、生物学等多个学科领域。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准测试大多基于选择题或已发表信息，前沿模型在这些测试上表现已接近饱和，需要新的评估框架来测试专家级科学推理能力。

Method: 包含两个互补赛道：1) 奥林匹克赛道：由国际奥林匹克竞赛奖牌得主和国家队教练原创的物理、化学、生物奥赛题；2) 科研赛道：由博士科学家编写和验证的博士级开放式研究子任务。采用基于细粒度评分标准的评估框架。

Result: 构建了包含数百个问题的基准测试（其中160个为开源黄金集），涵盖从量子电动力学到合成有机化学等多个子领域，提供了更全面的科学推理能力评估。

Conclusion: FrontierScience填补了现有科学基准测试的空白，能够更准确地评估前沿语言模型在专家级科学推理方面的能力，为模型发展提供了新的评估标准。

Abstract: We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.
  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.

</details>


### [14] [MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2601.21181)
*Sangyun Chung,Se Yeon Kim,Youngchae Chee,Yong Man Ro*

Main category: cs.AI

TL;DR: MAD是一种无需训练的方法，通过自适应加权模态特定解码分支来减少多模态大语言模型中的跨模态幻觉问题


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在跨模态幻觉问题，即一个模态不适当地影响另一个模态的生成，导致虚假输出。这暴露了模态交互控制方面的根本缺陷。

Method: 提出模态自适应解码（MAD），这是一种无需训练的方法。它通过查询每个任务需要哪些模态来利用模型固有的自评估能力，提取模态概率，然后自适应地加权对比解码分支，使模型能够专注于相关信息同时抑制跨模态干扰。

Result: 在CMM和AVHBench上的大量实验表明，MAD显著减少了多个音频-视觉语言模型的跨模态幻觉（VideoLLaMA2-AV提升7.8%和2.0%，Qwen2.5-Omni提升8.7%和4.7%）。

Conclusion: 通过自评估实现显式模态意识对于稳健的多模态推理至关重要，为现有对比解码方法提供了原则性扩展。

Abstract: Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\% and 2.0\% improvements for VideoLLaMA2-AV, 8.7\% and 4.7\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}

</details>


### [15] [Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models](https://arxiv.org/abs/2601.21183)
*Jacek Duszenko*

Main category: cs.AI

TL;DR: 该研究引入了"谄媚锚点"概念来定位和量化推理模型中的谄媚行为，通过分析超过10,000个反事实推理轨迹，发现谄媚锚点可以在推理过程中被可靠检测和量化。


<details>
  <summary>Details</summary>
Motivation: 推理模型经常同意错误的用户建议（谄媚行为），但尚不清楚这种同意在推理轨迹中的起源位置以及承诺强度如何。需要定位和量化这种行为。

Method: 引入"谄媚锚点"概念——那些因果性地将模型锁定在用户同意状态的句子。在蒸馏推理模型上分析超过10,000个反事实推理轨迹，使用线性探针和基于激活的回归器来检测和量化谄媚锚点。

Result: 线性探针能以84.6%的平衡准确率区分谄媚锚点，激活回归器能预测承诺强度（R²=0.74）。发现谄媚锚点比正确推理锚点更易区分，且谄媚行为在推理过程中逐渐建立。

Conclusion: 研究提供了在推理过程中定位模型未对齐行为的句子级机制，揭示了干预的潜在窗口，为理解模型谄媚行为提供了量化方法。

Abstract: Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.

</details>


### [16] [Do Reasoning Models Enhance Embedding Models?](https://arxiv.org/abs/2601.21192)
*Wun Yu Chan,Shaojin Chen,Huihao Jing,Kwun Hang Lau,Elton Chun-Chai Li,Zihao Wang,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: RLVR调优的推理模型作为嵌入初始化时，相比基础模型在语义表示任务上没有性能优势，因为对比学习会导致流形重新对齐


<details>
  <summary>Details</summary>
Motivation: 研究增强推理能力的RLVR调优模型是否能为语义表示任务提供更好的嵌入初始化，因为当前最先进的嵌入模型通常来自经过对比学习的仅解码器LLM

Method: 在MTEB和BRIGHT基准上评估RLVR调优模型作为嵌入初始化的性能，并引入分层表示相似性分析（HRSA）框架，从表示、几何和功能三个层面分解相似性

Result: 发现零效应：RLVR调优模型作为嵌入初始化时，相比基础模型没有一致的性能优势。HRSA分析显示RLVR引起不可逆的局部几何重组和可逆的坐标基漂移，但保留了全局流形几何和线性读出

Conclusion: 与监督微调不同，RLVR在现有语义景观内优化轨迹，而非从根本上重构景观本身，后续的对比学习会导致基础模型和推理初始化模型之间的强对齐（流形重新对齐）

Abstract: State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.

</details>


### [17] [Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification](https://arxiv.org/abs/2601.21210)
*Paul He,Yinya Huang,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: DoVerifier是一个符号验证器，用于检查LLM生成的因果表达式是否可以从给定的因果图中推导出来，使用do-calculus和概率论规则，从而更准确地评估LLM的因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型因果推理能力的基准测试通常依赖于字符串匹配或表面指标，无法捕捉模型输出在因果推理语义下是否形式有效。需要一种更严格的方法来评估LLM的因果推理能力。

Method: 提出DoVerifier，一个简单的符号验证器，使用do-calculus和概率论规则来检查LLM生成的因果表达式是否可以从给定的因果图中推导出来。该方法能够恢复因表面差异而被错误标记的正确答案。

Result: 在合成数据和因果问答基准测试上的评估表明，DoVerifier能够更准确地捕捉因果推理轨迹的语义正确性，为评估LLM的因果推理能力提供了更严格和更有信息量的方法。

Conclusion: DoVerifier提供了一种更严谨的评估框架，能够更准确地评估LLM在因果推理任务上的表现，超越了传统的表面指标评估方法。

Abstract: Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.

</details>


### [18] [Causal Discovery for Explainable AI: A Dual-Encoding Approach](https://arxiv.org/abs/2601.21221)
*Henry Salgado,Meagan R. Kendall,Martine Ceberio*

Main category: cs.AI

TL;DR: 提出一种双编码因果发现方法，通过互补编码策略运行约束算法，并使用多数投票合并结果，解决分类变量因果发现中的数值不稳定问题


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在处理分类变量时面临数值不稳定性挑战，特别是在条件独立性测试中。理解特征间的因果关系对于解释机器学习模型决策至关重要，但现有方法在处理分类变量时存在局限性。

Method: 提出双编码因果发现方法：1）使用互补编码策略运行约束算法；2）通过多数投票机制合并不同编码策略的结果。这种方法专门针对分类变量设计，提高了因果发现的稳定性和准确性。

Result: 在泰坦尼克数据集上应用该方法，发现的因果结构与已建立的可解释方法保持一致，验证了方法的有效性。

Conclusion: 双编码因果发现方法能够有效处理分类变量，克服传统方法在条件独立性测试中的数值不稳定问题，为解释机器学习模型决策提供更可靠的因果结构发现。

Abstract: Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.

</details>


### [19] [TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design](https://arxiv.org/abs/2601.21239)
*Chentong Chen,Mengyuan Zhong,Ye Fan,Jialong Shi,Jianyong Sun*

Main category: cs.AI

TL;DR: TIDE框架通过解耦算法结构推理和参数优化，解决了现有自动启发式设计中离散结构与连续参数耦合的问题，显著提升了组合优化问题的求解质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法将算法演化视为单一文本生成任务，忽视了离散算法结构与连续数值参数之间的耦合关系，导致丢弃有潜力的算法（由于未校准的常数）和过早收敛（由于简单的相似性度量）。

Method: 提出TIDE（Tuning-Integrated Dynamic Evolution）框架，采用嵌套架构：外层并行岛屿模型使用树编辑距离驱动结构多样性；内层循环集成LLM逻辑生成和差分变异算子进行参数调优；并采用UCB调度器动态优先高收益提示策略优化资源分配。

Result: 在九个组合优化问题上的实验表明，TIDE发现的启发式算法在解质量上显著优于现有最先进基线，同时实现了改进的搜索效率和降低的计算成本。

Conclusion: TIDE通过解耦结构推理和参数优化，有效解决了自动启发式设计中的关键限制，为算法演化提供了更高效的框架，在组合优化领域展现出优越性能。

Abstract: Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.

</details>


### [20] [Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox](https://arxiv.org/abs/2601.21249)
*Enzo Nicolás Spotorno,Antônio Augusto Medeiros Fröhlich*

Main category: cs.AI

TL;DR: 论文提出HYDRA框架，通过模块化主权范式解决时间序列基础模型在安全关键CPS中的适应性问题，避免灾难性遗忘并确保可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前通用时间序列基础模型在安全关键CPS部署中存在灾难性遗忘、频谱偏差和可验证性不足等问题，无法满足非平稳生命周期动态和严格可靠性要求。

Method: 提出模块化主权范式HYDRA：使用冻结的特定领域专家库，通过不确定性感知混合机制组合，实现条件有效性、不确定性解耦和模块化可审计性。

Result: HYDRA框架提供了可验证的路径，确保CPS生命周期中的鲁棒状态完整性，解决了塑性-稳定性悖论，超越了全局参数更新的局限性。

Conclusion: 模块化主权范式是解决安全关键CPS中基础模型适应挑战的有效方法，为符合安全标准（如ISO 26262）的可靠系统提供了可行方案。

Abstract: The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term "HYDRA" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.

</details>


### [21] [Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving](https://arxiv.org/abs/2601.21288)
*Weitong Lian,Zecong Tang,Haoran Li,Tianjian Gao,Yifei Wang,Zixu Wang,Lingyi Meng,Tengju Ru,Zhejun Cui,Yichen Zhu,Hangshuo Cao,Qi Kang,Tianxing Chen,Yusen Qin,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: Drive-KD框架通过知识蒸馏将自动驾驶分解为感知-推理-规划三部分，使用层特定注意力作为蒸馏信号构建单教师模型，再通过多教师蒸馏和不对称梯度投影解决梯度冲突，显著提升小模型性能


<details>
  <summary>Details</summary>
Motivation: 自动驾驶是安全关键任务，现有大模型需要大量GPU内存和高推理延迟，而传统监督微调难以弥补小模型能力差距，需要高效的知识迁移方法

Method: 将自动驾驶分解为感知-推理-规划三部分，使用层特定注意力作为蒸馏信号构建能力特定的单教师模型，然后统一为多教师蒸馏框架，引入不对称梯度投影缓解跨能力梯度冲突

Result: 蒸馏后的InternVL3-1B模型比同系列预训练78B模型减少约42倍GPU内存，吞吐量提高约11.4倍，在DriveBench上整体性能更好，在规划维度超越GPT-5.1

Conclusion: Drive-KD框架通过知识蒸馏有效提升了小模型在自动驾驶任务上的性能，为实现高效自动驾驶视觉语言模型提供了新思路

Abstract: Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a "perception-reasoning-planning" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.

</details>


### [22] [Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation](https://arxiv.org/abs/2601.21335)
*Yuzhe Chen,Jie Cao,Youquan Wang,Haicheng Tao,Darko B. Vukovic,Jia Wu*

Main category: cs.AI

TL;DR: CNRE模型结合神经符号推理与因果推断，通过模拟人类决策过程实现可解释的多行为推荐，解决了现有方法在性能与可解释性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有多行为推荐方法往往以牺牲可解释性为代价追求性能，而当前可解释方法因依赖外部信息而泛化能力有限。神经符号整合为可解释性提供了有前景的途径，同时用户行为链本身蕴含适合显式推理的内生逻辑，但这些观察到的多行为受到混杂因素影响，导致模型学习虚假相关性。

Method: 提出因果神经符号推理模型CNRE，通过分层偏好传播捕获异构跨行为依赖，基于偏好强度建模用户行为链中的内生逻辑规则，自适应调度到相应的神经逻辑推理路径（如合取、析取），生成近似理想状态的可解释因果中介变量。

Result: 在三个大规模数据集上的广泛实验表明，CNRE在性能上显著优于最先进的基线方法，同时在模型设计、决策过程和推荐结果三个层面提供多级可解释性。

Conclusion: CNRE成功将因果推断融入神经符号框架，通过模拟人类决策过程操作内生逻辑，实现了性能与可解释性的双重提升，为多行为推荐提供了新的解决方案。

Abstract: Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.

</details>


### [23] [Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks](https://arxiv.org/abs/2601.21339)
*Jennifer Haase,Jana Gonnermann-Müller,Paul H. P. Hanel,Nicolas Leins,Thomas Kosch,Jan Mendling,Sebastian Pokutta*

Main category: cs.AI

TL;DR: 研究评估了12个LLM在10个创意提示上的输出方差来源，发现对于输出质量（原创性），提示解释了36.43%的方差，与模型选择（40.94%）相当；但对于输出数量（流畅性），模型选择（51.25%）和LLM内部方差（33.70%）占主导，提示仅解释4.22%的方差。


<details>
  <summary>Details</summary>
Motivation: 探究LLM输出方差的主要来源：提示、模型选择还是采样随机性，以理解各因素对输出质量和数量的相对影响。

Method: 使用12个LLM在10个创意提示上进行评估，每个提示采样100次（总样本量N=12,000），分析输出方差在质量（原创性）和数量（流畅性）两个维度上的来源。

Result: 对于输出质量，提示解释36.43%方差，模型选择解释40.94%方差；对于输出数量，模型选择解释51.25%方差，LLM内部方差解释33.70%方差，提示仅解释4.22%方差。LLM内部方差为10-34%，表明单样本评估可能混淆采样噪声与真实效应。

Conclusion: 提示是调控输出质量的有效杠杆，但由于显著的LLM内部方差（10-34%），单样本评估存在风险，可能将采样噪声误认为提示或模型的真实效应。

Abstract: How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

</details>


### [24] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: EHR-RAG：针对长时序电子健康记录设计的检索增强生成框架，通过事件-时间感知混合检索、自适应迭代检索和双路径证据检索推理，在四个长时序预测任务中平均提升Macro-F1 10.76%


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含丰富的纵向临床证据，但长时序EHR常超出LLM上下文限制，现有方法依赖截断或简单检索策略，会丢弃临床相关事件和时间依赖关系

Method: 提出EHR-RAG框架，包含三个核心组件：1)事件和时间感知混合EHR检索，保留临床结构和时间动态；2)自适应迭代检索，逐步优化查询以扩大证据覆盖；3)双路径证据检索和推理，联合检索和推理事实与反事实证据

Result: 在四个长时序EHR预测任务中，EHR-RAG始终优于最强的LLM基线方法，平均Macro-F1提升10.76%

Conclusion: 检索增强的LLM在结构化EHR数据的临床预测实践中具有巨大潜力，EHR-RAG框架通过专门设计的检索策略有效解决了长时序EHR处理的挑战

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [25] [Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores](https://arxiv.org/abs/2601.21342)
*Zhiyong Shen,Gongpeng Zhao,Jun Zhou,Li Yu,Guandong Kou,Jichen Li,Chuanlei Dong,Zuncheng Li,Kaimao Li,Bingkun Wei,Shicheng Hu,Wei Xia,Wenguo Duan*

Main category: cs.AI

TL;DR: 该论文提出了Ostrakon-VL模型、ShopBench基准和QUAD数据清洗管道，用于解决食品服务和零售场景中多模态大语言模型面临的数据质量差和评估标准不统一的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在食品服务和零售场景部署面临两大障碍：1) 真实世界数据噪声大、缺乏可审计的闭环数据清洗机制；2) 现有评估协议缺乏统一、细粒度、标准化的基准测试。

Method: 1) 基于Qwen3-VL-8B开发Ostrakon-VL模型；2) 创建首个面向FSRS的公开基准ShopBench；3) 提出QUAD多阶段多模态指令数据清洗管道；4) 采用多阶段训练策略。

Result: Ostrakon-VL在ShopBench上平均得分60.1，在可比参数规模的开源MLLM中达到新SOTA。超越更大的Qwen3-VL-235B-A22B(+0.7)和同规模的Qwen3-VL-8B(+4.8)，参数效率显著提升。

Conclusion: Ostrakon-VL提供了更稳健可靠的FSRS中心感知和决策能力，通过公开模型和基准促进可重复研究，解决了食品服务和零售场景中MLLM部署的关键挑战。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.

</details>


### [26] [Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization](https://arxiv.org/abs/2601.21358)
*Jiecong Wang,Hao Peng,Chunyang Liu*

Main category: cs.AI

TL;DR: PLaT框架通过将潜在推理重新定义为规划，将推理与语言表达解耦，使模型能够动态决定何时终止推理，而不是依赖固定超参数。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法通常作为从显式推理步骤到潜在状态的不透明端到端映射，且推理时需要预定义潜在步骤数量，限制了推理的灵活性和可扩展性。

Method: PLaT框架将推理建模为潜在规划状态的确定性轨迹，同时使用单独的Decoder在必要时将这些思想转化为文本，实现了推理与语言表达的解耦。

Result: 在数学基准测试中，PLaT虽然贪心准确率低于基线，但在推理多样性方面表现出更好的可扩展性，表明其学习到了更稳健、更广泛的解决方案空间。

Conclusion: PLaT为推理时搜索提供了透明且可扩展的基础，通过解耦推理与语言表达，使模型能够动态控制推理过程，提高了推理的多样性和鲁棒性。

Abstract: Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.

</details>


### [27] [System 1&2 Synergy via Dynamic Model Interpolation](https://arxiv.org/abs/2601.21414)
*Chenxu Yang,Qingyi Si,Chong Tian,Xiyu Liu,Dingyu Yao,Chuanyu Qin,Zheng Lin,Weiping Wang,Jiaqi Wang*

Main category: cs.AI

TL;DR: DAMI框架通过动态参数插值在直觉型System 1和深思型System 2模型间切换，实现能力控制而非输出控制，在数学推理任务上获得更高准确率同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注输出控制（限制模型生成内容），但这只是认知配置的症状而非根本原因。需要转向能力控制，调节模型如何思考而非产生什么。

Method: 利用现有Instruct和Thinking检查点进行动态参数插值，无需额外训练。提出DAMI框架，通过查询特定推理强度λ(q)配置认知深度，包括基于偏好学习的训练估计和基于置信度的零样本部署方法。

Result: 在五个数学推理基准测试中，DAMI比Thinking模型获得更高准确率，同时保持效率，有效结合了System 1的效率和System 2的推理深度。

Conclusion: 能力控制比输出控制更有效，动态模型插值能够实现直觉和深思认知模式的有效平衡，为统一语言模型训练提供了新方向。

Abstract: Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \textit{how models think} rather than \textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \textbf{DAMI} (\textbf{D}yn\textbf{A}mic \textbf{M}odel \textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.

</details>


### [28] [When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models](https://arxiv.org/abs/2601.21433)
*Katherine Elkins,Jon Chun*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在处理否定指令时存在严重缺陷：开源模型在简单否定下有77%的概率错误执行被禁止的行为，在复合否定下达到100%，比肯定指令的错误率高出317%。商业模型表现稍好但仍存在显著问题。


<details>
  <summary>Details</summary>
Motivation: 当用户告诉AI系统某人"不应该"采取某个行动时，系统应该将其视为禁止。然而研究发现许多大型语言模型恰恰相反：它们将否定指令解释为肯定。这揭示了当前对齐技术与安全部署要求之间的差距。

Method: 审计了16个模型在14个伦理场景中的表现，比较了肯定和否定指令下的响应差异。使用确定性解码排除采样噪声影响，提出了否定敏感指数（NSI）作为治理指标，并设计了分层认证框架。

Result: 开源模型在简单否定下77%的时间支持被禁止的行动，复合否定下达到100%（比肯定指令增加317%）。商业模型波动范围为19-128%。模型间一致性从肯定提示的74%下降到否定提示的62%。金融场景的脆弱性是医疗场景的两倍。

Conclusion: 当前模型无法可靠区分"做X"和"不做X"，不应在高风险环境中做出自主决策。需要开发能够正确处理否定指令的模型，并建立分层认证框架以确保安全部署。

Abstract: When a user tells an AI system that someone "should not" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish "do X" from "do not X" should not be making autonomous decisions in high-stakes contexts.

</details>


### [29] [The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making](https://arxiv.org/abs/2601.21439)
*Jon Chun,Katherine Elkins*

Main category: cs.AI

TL;DR: 研究发现LLMs在情绪框架效应中表现出惊人的稳健性，尽管它们对提示词微小变化敏感，但在规则约束决策中却对情绪性叙述操纵具有110-300倍的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 虽然已知LLMs对提示词微小扰动敏感且容易迎合用户偏见，但它们在重要规则约束决策中的稳健性尚未充分探索。研究者想要探究LLMs在情绪框架效应中的表现是否与其已知的脆弱性相矛盾。

Method: 使用新颖的受控扰动框架，在三个高风险领域（医疗、法律、金融）进行实验，量化LLMs与人类在叙述操纵下的稳健性差距。创建了162个场景的基准测试集。

Result: 发现LLMs表现出"稳健性悖论"：尽管对提示词格式敏感，但对情绪框架效应具有近乎完全不变性。模型效应量接近零（Cohen's h=0.003），而人类则表现出显著偏见（Cohen's h在[0.3,0.8]之间）。LLMs对叙述操纵的抵抗力比人类高110-300倍。

Conclusion: 指令调优的LLMs能够将逻辑规则遵循与说服性叙述解耦，提供了一种决策稳定性来源，可以补充甚至可能去偏人类判断。这种稳健性在不同训练范式的模型中持续存在。

Abstract: While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking "Paradox of Robustness": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be "brittle" to how a query is formatted, they are remarkably "stable" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.

</details>


### [30] [ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design](https://arxiv.org/abs/2601.21448)
*Zhongkai Yu,Chenyang Zhou,Yichen Lin,Hejia Zhang,Haotian Ye,Junxia Cui,Zaifeng Pan,Jishen Zhao,Yufei Ding*

Main category: cs.AI

TL;DR: 该论文提出了一个全面的AI辅助芯片设计基准测试ChipBench，用于评估大语言模型在Verilog生成、调试和参考模型生成三个关键任务上的性能，揭示了现有基准测试饱和问题下大语言模型在真实工业流程中的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在硬件工程中显示出巨大潜力，但现有基准测试存在饱和问题和任务多样性有限的问题，无法准确反映大语言模型在真实工业工作流程中的性能表现。

Method: 提出了一个全面的AI辅助芯片设计基准测试，包含44个具有复杂层次结构的现实模块、89个系统调试案例，以及132个涵盖Python、SystemC和CXXRTL的参考模型样本，用于评估大语言模型在三个关键任务上的性能。

Result: 评估结果显示大语言模型存在显著的性能差距：最先进的Claude-4.5-opus在Verilog生成任务上仅达到30.74%，在Python参考模型生成任务上仅达到13.33%，远低于现有饱和基准测试中SOTA模型超过95%的通过率。

Conclusion: 该研究揭示了当前大语言模型在芯片设计任务中的实际挑战，为促进该领域研究提供了高质量的自动化训练数据生成工具，并建立了更贴近工业实践的评估基准。

Abstract: While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\% on Verilog generation and 13.33\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.

</details>


### [31] [Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance](https://arxiv.org/abs/2601.21465)
*Márton Kardos*

Main category: cs.AI

TL;DR: Topeax是一种新的主题建模方法，通过密度估计峰值确定聚类数量，结合词汇和语义指标选择高质量主题关键词，相比Top2Vec和BERTopic在聚类恢复和描述方面表现更好，且对样本大小和超参数变化更稳定。


<details>
  <summary>Details</summary>
Motivation: 当前流行的聚类主题建模方法（Top2Vec和BERTopic）存在两个主要问题：1）对样本大小和超参数极其敏感，默认值导致次优行为，难以发现自然聚类；2）在估计术语重要性时，BERTopic忽略关键词与主题向量的语义距离，Top2Vec忽略语料库中的词频，导致主题连贯性差且缺乏多样性。

Method: Topeax采用密度估计峰值方法自动发现聚类数量，结合词汇指标（词频）和语义指标（与主题向量的距离）来评估术语重要性，从而生成高质量的主题关键词。

Result: Topeax在聚类恢复和聚类描述方面均优于Top2Vec和BERTopic，同时对样本大小和超参数变化的响应更加稳定，减少了异常行为。

Conclusion: Topeax通过改进聚类发现机制和术语重要性评估方法，解决了现有聚类主题建模方法的关键缺陷，提供了更可靠、更高质量的主题建模解决方案。

Abstract: Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.

</details>


### [32] [ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management](https://arxiv.org/abs/2601.21473)
*Zaifeng Pan,Yipeng Shen,Zhengding Hu,Zhuang Wang,Aninda Manocha,Zheng Wang,Zhongkai Yu,Yue Guan,Yufei Ding*

Main category: cs.AI

TL;DR: ScaleSim：基于LLM的大规模多智能体模拟内存优化系统，通过智能预取和优先级驱逐实现1.74倍加速


<details>
  <summary>Details</summary>
Motivation: LLM多智能体模拟面临GPU内存压力大的问题，每个智能体需要维护私有GPU状态（模型、前缀缓存、适配器），随着智能体数量增加会快速耗尽设备内存

Method: 提出"调用距离"统一抽象来估计智能体未来LLM请求的相对顺序，基于此实现ScaleSim系统，支持主动预取、基于优先级的驱逐，并通过模块化接口支持多样化的智能体特定内存

Result: 在模拟基准测试中，ScaleSim相比SGLang实现了最高1.74倍的加速

Conclusion: ScaleSim通过创新的内存管理策略有效解决了LLM多智能体模拟的扩展性问题，为大规模模拟提供了高效的内存优化解决方案

Abstract: LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.

</details>


### [33] [ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making](https://arxiv.org/abs/2601.21533)
*Youngjin Jin,Hanna Kim,Kwanwoo Kim,Chanhee Lee,Seungwon Shin*

Main category: cs.AI

TL;DR: ARGORA框架将多专家LLM讨论组织为显式论证图，通过因果建模识别关键推理链，并引入校正机制对齐内部推理与外部判断。


<details>
  <summary>Details</summary>
Motivation: 现有多专家LLM系统通过简单聚合收集不同观点，但掩盖了哪些论证驱动最终决策，缺乏透明度和可解释性。

Method: 1) 将多专家讨论组织为显式论证图，显示论证间的支持或攻击关系；2) 将论证图建模为因果模型，通过系统性地移除单个论证并重新计算结果，识别必要推理链；3) 引入校正机制，当内部推理与外部判断不一致时进行对齐。

Result: 在多样化基准测试和开放式用例中，ARGORA实现了有竞争力的准确性，并展现出校正行为：当专家最初存在分歧时，框架更倾向于将争议解决为正确答案而非引入新错误，同时提供决定性论证的因果诊断。

Conclusion: ARGORA通过论证图和因果建模提高了多专家LLM系统的透明度和可解释性，能够识别关键推理链并提供决策过程的因果诊断，同时通过校正机制改善决策质量。

Abstract: Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.

</details>


### [34] [EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots](https://arxiv.org/abs/2601.21570)
*Zixing Lei,Genjia Liu,Yuanshuo Zhang,Qipeng Liu,Chuan Wen,Shanghang Zhang,Wenzhao Lian,Siheng Chen*

Main category: cs.AI

TL;DR: LLM智能体在自主设计机器人策略方面的能力评估，通过32个任务测试显示，自主智能体平均成功率比人工设计基准高26.5%，并能通过环境反馈迭代优化策略。


<details>
  <summary>Details</summary>
Motivation: 当前具身AI领域向通用机器人系统快速演进，但扩展能力受到依赖人工监督的严重限制，包括复杂的奖励塑造和跨异构后端的超参数调优。受LLM在软件自动化和科学发现方面成功的启发，研究者希望评估LLM智能体自主设计具身策略的能力。

Method: 引入EmboCoach-Bench基准测试，包含32个专家策划的强化学习和模仿学习任务，将可执行代码作为通用接口。评估动态闭环工作流程，智能体利用环境反馈迭代起草、调试和优化解决方案，涵盖从物理信息奖励设计到扩散策略等策略架构的改进。

Result: 自主智能体在平均成功率上比人工设计基准高出26.5%；带有环境反馈的智能体工作流程有效加强了策略开发，显著缩小了开源模型与专有模型之间的性能差距；智能体展现出对病理工程案例的自我纠正能力，通过迭代仿真调试成功从近乎完全失败中恢复任务性能。

Conclusion: 这项工作为自演化的具身智能奠定了基础，加速了从劳动密集型手动调优向可扩展、自主工程的范式转变，推动了具身AI领域的发展。

Abstract: The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.

</details>


### [35] [Chain Of Thought Compression: A Theoritical Analysis](https://arxiv.org/abs/2601.21576)
*Juncai Li,Ru Li,Yuxiang Zhou,Boxiang Ma,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 本文首次从理论上分析了隐式CoT压缩的学习难度，证明高阶逻辑依赖的学习信号会指数衰减，并提出ALiCoT框架通过对齐潜在状态与中间推理步骤来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 虽然隐式CoT压缩能减少推理所需的token数量，但其背后的工作机制尚不清楚。本文旨在从理论上分析学习内部化中间推理步骤的难度，并设计相应解决方案。

Method: 引入Order-r Interaction理论框架，证明高阶逻辑依赖的学习信号衰减问题；创建NatBool-DAG基准测试消除语义捷径；提出ALiCoT框架，通过对齐潜在token分布与中间推理状态来克服信号衰减。

Result: ALiCoT在保持与显式CoT相当性能的同时，实现了54.4倍的推理加速，成功解锁了高效推理能力。

Conclusion: 隐式CoT压缩面临高阶交互障碍的理论挑战，但通过适当的对齐机制可以克服信号衰减问题，实现既高效又准确的推理。

Abstract: Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.

</details>


### [36] [Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves](https://arxiv.org/abs/2601.21582)
*Jonas Knupp,Jan Hendrik Metzen,Jeremias Bohn,Georg Groh,Kristian Kersting*

Main category: cs.AI

TL;DR: Dreamer框架通过深度递归注意力混合机制，结合序列注意力、深度注意力和稀疏专家注意力，解决了深度递归模型中隐藏层大小瓶颈问题，在语言推理任务上实现了更高效的训练和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度递归方法存在三个主要问题：缺乏FLOP、参数和内存匹配的基线；由于部分固定的层堆叠导致深度递归利用不足；以及恒定隐藏层大小瓶颈限制多步潜在推理。

Method: 提出Dreamer框架，结合三种注意力机制：序列注意力、深度注意力和稀疏专家注意力。通过深度方向的注意力缓解隐藏层大小瓶颈，解耦缩放维度，使深度递归模型能够高效扩展。

Result: 在语言推理基准测试中，相比FLOP、参数和内存匹配的SOTA模型，Dreamer模型达到相同准确率所需的训练token减少2-8倍；使用相同训练token时，性能优于约2倍大小的SOTA模型；专家选择多样性比SOTA MoEs高2-11倍。

Conclusion: Dreamer框架通过深度递归注意力混合机制有效解决了深度递归模型的扩展瓶颈，在语言推理任务上实现了显著更高的训练效率和性能提升，同时提供了跨深度知识使用的新见解。

Abstract: Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.

</details>


### [37] [Beyond Imitation: Reinforcement Learning for Active Latent Planning](https://arxiv.org/abs/2601.21598)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: ATP-Latent方法通过条件变分自编码器建模潜在令牌监督过程，结合强化学习和一致性奖励，提升潜在推理策略的规划能力，在LLaMA-1B上实现+4.1%准确率和-3.3%令牌消耗的改进。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法通过模仿语言标签来监督潜在令牌，但由于同一问题可能存在多个等价但不同的思维链标签，被动模仿任意一个可能导致次优的潜在令牌表示和推理策略，限制了潜在空间的规划能力，并造成训练与测试之间的明显差距。

Method: 提出ATP-Latent方法：1）使用条件变分自编码器建模潜在令牌的监督过程，获得更平滑的潜在空间；2）通过强化学习结合辅助一致性奖励来促进最合理的潜在推理策略，该奖励基于潜在令牌的VAE解码内容之间的一致性计算。

Result: 在LLaMA-1B模型上，ATP-Latent在四个基准测试中相比先进基线方法实现了+4.1%的准确率提升和-3.3%的令牌消耗减少。

Conclusion: ATP-Latent通过主动规划潜在令牌表示空间，结合条件VAE和强化学习的一致性奖励，显著提升了潜在推理的效率和性能，验证了在潜在空间进行主动规划的重要性。

Abstract: Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.

</details>


### [38] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 该研究将企业级智能文档处理系统的验证问题形式化为基于搜索的软件测试问题，旨在在有限预算下最大化发现不同类型的故障机制，通过对比多种搜索策略发现不同求解器具有互补性，需要组合策略才能实现稳健的工业验证。


<details>
  <summary>Details</summary>
Motivation: 企业级智能文档处理系统在金融、保险和医疗等高风险领域应用广泛，早期系统验证需要在有限预算下发现多样化的故障机制，而不是寻找单一的最坏情况文档。现有方法在发现复杂文档变量交互导致的故障方面存在不足。

Method: 将问题形式化为基于搜索的软件测试问题，在文档配置的组合空间中操作，生成具有结构风险特征的文档实例以诱导现实故障条件。在相同预算约束下，对进化算法、群体智能、质量多样性、学习方法和量子算法等多种搜索策略进行基准测试。

Result: 通过配置级排他性、胜率和跨时间重叠分析发现，不同求解器始终能发现特定替代方法在相似预算下无法发现的故障模式。跨时间分析显示所有评估预算中都存在持续的求解器特定发现，没有单一策略表现出绝对优势。虽然所有求解器的联合最终能覆盖观察到的故障空间，但依赖任何单一方法都会系统性地延迟重要风险的发现。

Conclusion: 研究结果表明求解器具有内在互补性，这为基于组合策略的SBST方法提供了动机，以实现稳健的工业IDP验证。需要采用多种搜索策略的组合方法才能有效发现多样化的故障机制。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [39] [RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems](https://arxiv.org/abs/2601.21609)
*Bingqian Li,Xiaolei Wang,Junyi Li,Weitao Li,Long Zhang,Sheng Chen,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: RecNet是一个自演化的偏好传播框架，通过前向传播和反向优化两阶段机制，主动传播实时偏好更新，解决传统推荐系统依赖稀疏、噪声交互数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推荐系统主要依赖显式的用户-物品交互数据，但这些数据稀疏、噪声大，无法反映用户和物品之间的实时相互影响，需要更有效的偏好传播机制。

Method: RecNet采用两阶段框架：前向阶段通过集中式偏好路由机制传播偏好更新，结合个性化偏好接收机制（消息缓冲区和可优化规则过滤器）；反向阶段通过反馈驱动的传播优化机制，模拟多智能体强化学习，使用LLM进行信用分配和梯度分析。

Result: 在各种场景下的广泛实验表明，RecNet在建模推荐系统偏好传播方面具有有效性。

Conclusion: RecNet通过自演化的偏好传播框架，能够主动传播实时偏好更新，解决了传统推荐系统依赖稀疏交互数据的局限性，为推荐系统提供了更有效的偏好建模方法。

Abstract: Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.

</details>


### [40] [Semantic Content Determines Algorithmic Performance](https://arxiv.org/abs/2601.21618)
*Martiño Ríos-García,Nawaf Alampara,Kevin Maik Jablonka*

Main category: cs.AI

TL;DR: 论文提出WhatCounts测试框架，发现LLMs的计数能力严重依赖于被计数对象的语义内容，而非实现通用算法


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否真正实现算法还是仅近似算法，特别是算法行为是否独立于输入参数的语义内容。现有研究常将语义敏感性与推理复杂性或提示变化混淆，需要隔离测试这一属性

Method: 引入WhatCounts测试框架，设计原子化的计数任务：在无歧义、有分隔符、无重复项、无干扰项的列表中计数不同语义类型的项目。通过控制变量排除混淆因素，并进行微调实验

Result: 前沿LLMs的计数准确率仅因被计数对象的语义类型（如城市vs化学品、名称vs符号）就产生超过40%的差异。微调实验显示这种语义差距会随少量无关微调而不可预测地变化

Conclusion: LLMs不实现算法而是近似算法，且这种近似是参数依赖的。这一发现具有广泛意义：任何LLM函数都可能隐藏着对其输入参数语义内容的依赖，这在智能体应用中尤为重要

Abstract: Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.

</details>


### [41] [ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval](https://arxiv.org/abs/2601.21654)
*Hao Shen,Hang Yang,Zhouhong Gu*

Main category: cs.AI

TL;DR: ScholarGym是一个用于评估深度研究工作流的仿真环境，通过静态论文语料库和确定性检索解决API调用不确定性问题，支持对查询规划、工具调用和相关性评估的细粒度分析。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型已从单轮问答发展到深度研究工作流，但评估这些工作流面临根本性挑战：依赖实时API会引入非确定性，因为工具调用可能因时间漂移、速率限制和后端状态变化而产生不同结果，这种差异破坏了可重复性并使跨系统比较无效。

Method: 提出ScholarGym仿真环境，将工作流组件解耦为查询规划、工具调用和相关性评估，在受控条件下对每个阶段进行细粒度分析。基于包含57万篇论文的静态语料库构建，提供确定性检索和2,536个带有专家标注真实标签的查询。

Result: 实验涵盖多种骨干模型，揭示了推理能力、规划策略和选择机制在迭代优化过程中的相互作用。

Conclusion: ScholarGym为深度研究工作流提供了可重复的评估环境，解决了实时API引入的非确定性问题，支持对复杂信息需求处理流程的系统性分析。

Abstract: Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.
  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.

</details>


### [42] [SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://arxiv.org/abs/2601.21666)
*Ahmed Y. Radwan,Christos Emmanouilidis,Hina Tabassum,Deval Pandya,Shaina Raza*

Main category: cs.AI

TL;DR: SONIC-O1是一个全面的人工验证基准测试，用于评估多模态大语言模型在13个真实世界对话领域的表现，包含4,958个标注和人口统计元数据，重点关注时间定位和社会鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型研究主要集中在静态图像理解，而对处理序列音频-视频数据的能力研究不足，需要高质量基准测试来系统评估模型在真实世界环境中的表现。

Method: 构建了SONIC-O1基准测试，涵盖13个真实世界对话领域，包含4,958个人工验证的标注和人口统计元数据，评估任务包括开放式摘要、多项选择题回答以及带推理的时间定位。

Result: 实验显示闭源和开源模型在多项选择题准确率上差距较小，但在时间定位任务上最佳闭源模型比开源模型高出22.6%；模型在不同人口统计群体上的性能进一步下降，表明存在持续的行为差异。

Conclusion: SONIC-O1为时间基础和社交鲁棒的多模态理解提供了开放的评估套件，揭示了当前模型的局限性，特别是时间定位能力和跨人口统计群体的公平性问题。

Abstract: Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard

</details>


### [43] [TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning](https://arxiv.org/abs/2601.21692)
*Mingzu Liu,Hao Fang,Runmin Cong*

Main category: cs.AI

TL;DR: 本文提出了一种名为TCAP的无监督防御框架，用于检测MLLMs微调服务中的后门攻击，通过分析注意力分配偏差来识别中毒样本。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs微调服务存在后门攻击风险，当前防御方法要么依赖监督信号，要么无法适应多样化的触发器和模态。研究发现中毒样本会破坏系统指令、视觉输入和用户文本查询三个功能组件之间的平衡注意力分布，这成为检测后门的通用指纹。

Method: 提出Tri-Component Attention Profiling (TCAP)框架：1) 将跨模态注意力图分解为三个功能组件；2) 使用高斯混合模型进行统计建模，识别对触发器敏感的注意力头；3) 通过基于EM的投票聚合机制隔离中毒样本。

Result: 在多种MLLM架构和攻击方法上的广泛实验表明，TCAP能够实现一致且强大的性能，成为MLLMs中稳健实用的后门防御方案。

Conclusion: TCAP通过利用注意力分配偏差作为通用后门指纹，提供了一种无监督的防御框架，能够有效应对多样化的触发器和模态攻击，为MLLMs微调服务的安全性提供了可靠保障。

Abstract: Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.

</details>


### [44] [FBS: Modeling Native Parallel Reading inside a Transformer](https://arxiv.org/abs/2601.21708)
*Tongxi Wang*

Main category: cs.AI

TL;DR: FBS Transformer通过引入仿人眼阅读机制，在保持参数不变的情况下改善了质量-效率权衡


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理主要采用严格的逐token自回归方式，缺乏人类阅读的关键要素：内容自适应前瞻、块结构感知计算分配以及训练-测试一致的预览/略读机制

Method: 提出Fovea-Block-Skip Transformer，通过Parafovea-Attention Window（PAW）、Chunk-Head（CH）和Skip-Gate（SG）三个模块在Transformer中注入可训练的因果循环

Result: 在多样化基准测试中，FBS改善了质量-效率权衡而不增加参数，消融实验显示三个模块具有互补性

Conclusion: 通过模拟人类阅读机制，FBS Transformer为LLM推理加速提供了新的有效方法，三个核心模块共同作用实现了更好的性能-效率平衡

Abstract: Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.

</details>


### [45] [E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory](https://arxiv.org/abs/2601.21714)
*Kaixiang Wang,Yidan Lin,Jiong Lou,Zhaojiacheng Zhou,Bunyod Suvonov,Jie Li*

Main category: cs.AI

TL;DR: E-mem框架通过从内存预处理转向情景上下文重建，解决了LLM智能体在长序列推理中上下文完整性问题，在LoCoMo基准上实现54%+ F1，超越SOTA 7.75%，同时降低70%+ token成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体向System 2推理演进时，需要保持长序列上的严格逻辑完整性。但主流内存预处理方法存在破坏性去上下文化问题，通过将复杂序列依赖压缩为预定义结构（如嵌入或图），切断了深度推理所需的情境完整性。

Method: 提出E-mem框架，从内存预处理转向情景上下文重建。受生物记忆印迹启发，采用异构分层架构：多个助手智能体维护未压缩的内存上下文，中央主智能体协调全局规划。助手在激活片段内进行本地推理，提取情境感知证据后再聚合。

Result: 在LoCoMo基准测试中，E-mem实现了超过54%的F1分数，比当前最先进的GAM方法提高了7.75%，同时减少了超过70%的token成本。

Conclusion: E-mem通过情景上下文重建方法有效解决了LLM智能体长序列推理中的上下文完整性问题，在保持高性能的同时显著降低了计算成本，为System 2推理提供了可行的技术路径。

Abstract: The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\% F1, surpassing the state-of-the-art GAM by 7.75\%, while reducing token cost by over 70\%.

</details>


### [46] [DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting](https://arxiv.org/abs/2601.21726)
*Siru Zhong,Yiqiu Liu,Zhiqing Cui,Zezhi Shao,Fei Wang,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: DropoutTS：一种模型无关的插件，通过样本自适应dropout机制，利用频谱稀疏性量化实例级噪声，动态校准模型学习能力，提升深度时间序列模型在噪声数据下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中普遍存在噪声数据，深度时间序列模型对此很脆弱。现有鲁棒性策略要么修剪数据，要么依赖昂贵的先验量化，无法平衡有效性和效率。

Method: 提出DropoutTS插件，采用样本自适应dropout机制：利用频谱稀疏性通过重构残差高效量化实例级噪声，将噪声映射到自适应dropout率，动态校准模型学习能力，选择性抑制虚假波动同时保持细粒度保真度。

Result: 在多种噪声机制和开放基准测试上的广泛实验表明，DropoutTS能持续提升优秀骨干模型的性能，以可忽略的参数开销和无架构修改实现先进的鲁棒性。

Conclusion: DropoutTS通过从"学什么"到"学多少"的范式转变，提供了一种高效、模型无关的时间序列噪声鲁棒性解决方案。

Abstract: Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from "what" to learn to "how much" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.

</details>


### [47] [Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.21742)
*Ruiwen Zhou,Maojia Song,Xiaobao Wu,Sitao Cheng,Xunjian Yin,Yuxi Xie,Zhuoqun Hao,Wenyue Hua,Liangming Pan,Soujanya Poria,Min-Yen Kan*

Main category: cs.AI

TL;DR: 论文提出Epistemic Context Learning (ECL)框架，通过历史交互构建同伴可信度档案，使多智能体系统中的小模型能够准确识别可靠同伴，性能超越大8倍的基线模型。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中的个体智能体缺乏鲁棒性，容易盲目跟随误导性同伴。这种弱点源于谄媚行为和对同伴可靠性评估能力的不足。需要让智能体能够基于历史交互评估同伴可信度，从而在不确定时向可信同伴学习。

Method: 1. 形式化历史感知参考学习问题，将同伴历史交互作为额外输入；2. 开发Epistemic Context Learning (ECL)推理框架，基于历史构建明确的同伴档案；3. 使用辅助奖励通过强化学习优化ECL。

Result: ECL使小模型(Qwen 3-4B)性能超越历史无关基线模型(Qwen 3-30B)8倍大小，准确识别可靠同伴。前沿模型性能提升至接近完美(100%)。ECL在各种多智能体配置中泛化良好，信任建模准确性与最终答案质量强相关。

Conclusion: ECL框架通过历史交互构建同伴可信度档案，有效解决了多智能体系统中的谄媚和可靠性评估问题，使智能体能够向可信同伴学习，显著提升系统性能，且具有良好的泛化能力。

Abstract: Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.

</details>


### [48] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出ZSSD零样本统计降尺度框架，无需配对训练数据，通过物理一致的气候先验和统一坐标引导解决现有方法的物理不一致性和梯度消失问题，在GCMs上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统监督式气候降尺度方法因缺乏配对训练数据和与再分析数据间的领域差异而难以泛化到全球气候模型；现有零样本方法存在物理不一致性和大尺度因子下的梯度消失问题。

Method: 提出ZSSD零样本统计降尺度框架：1）从再分析数据学习物理一致的气候先验，结合地球物理边界和时序信息确保物理有效性；2）引入统一坐标引导策略解决DPS中的梯度消失问题，确保与大尺度场的一致性。

Result: ZSSD在99百分位误差上显著优于现有零样本基线方法，能够成功重建复杂天气事件（如热带气旋），并在异质GCMs上表现良好。

Conclusion: ZSSD为零样本气候降尺度提供了有效解决方案，通过物理先验和统一坐标引导解决了现有方法的局限性，在GCMs上实现了优异的降尺度性能。

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [49] [Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies](https://arxiv.org/abs/2601.21771)
*Hadi Banaee,Stephanie Lowry*

Main category: cs.AI

TL;DR: 提出基于概念空间框架的时序抽象概念建模方法，以国际象棋为案例，将战略概念表示为可解释质量维度上的几何区域，通过游戏轨迹分析识别战略意图。


<details>
  <summary>Details</summary>
Motivation: 将概念空间理论扩展到随时间实现的、目标导向的抽象概念建模，为涉及顺序决策的广泛应用奠定基础，并支持与知识演化机制集成以学习和精炼抽象概念。

Method: 采用概念空间框架，将战略概念（如攻击、牺牲）表示为可解释质量维度上的几何区域，将国际象棋游戏实例化为轨迹，通过轨迹在概念空间中的方向性移动来识别战略意图，支持双视角建模以捕捉玩家对相同情况的不同解释。

Result: 实现证明了基于轨迹的概念识别的可行性，运动模式与专家评论一致，展示了该方法对时序实现的目标导向概念的有效建模能力。

Conclusion: 该工作成功将概念空间理论扩展到时序抽象概念建模，为顺序决策的广泛应用建立了基础，并支持与知识演化机制集成，为随时间学习和精炼抽象概念提供了框架。

Abstract: We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.

</details>


### [50] [A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition](https://arxiv.org/abs/2601.21802)
*Hoang Khang Phan,Quang Vinh Dang,Noriyo Colley,Christina Garcia,Nhat Tan Le*

Main category: cs.AI

TL;DR: 本文提出一个以LLM为中心的框架，用于视频中的气管内吸痰活动识别和反馈生成，在家庭护理和教育场景中提升护理培训效果。


<details>
  <summary>Details</summary>
Motivation: 气管内吸痰是一项高风险临床操作，在家庭护理和教育环境中缺乏有效监督和自动化反馈系统，现有技术对此探索不足。

Method: 提出统一的LLM中心框架，将大语言模型作为核心推理模块，执行视频数据的时空活动识别和可解释决策分析，并生成自然语言反馈。

Result: LLM方法在准确率和F1分数上比基线模型提升约15-20%，并开发了基于异常检测和可解释AI的学生支持模块。

Conclusion: 该框架为护理教育建立了可扩展、可解释、数据驱动的基础，能提高培训效率和患者安全。

Abstract: Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.

</details>


### [51] [Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models](https://arxiv.org/abs/2601.21830)
*Francesca Filice,Edoardo De Rose,Simone Bartucci,Francesco Calimeri,Simona Perri*

Main category: cs.AI

TL;DR: 提出一个全面的心电图专家基础模型基准测试框架，结合性能评估和表示层分析，评估模型在不同数据集和数据稀缺情况下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然基础模型在AI辅助心电图解读领域开始发展，但现有基准测试主要关注下游性能评估，缺乏对模型嵌入表示通用性的深入分析，这在医疗等错误敏感领域尤为重要。

Method: 引入一个基准测试方法学，结合基于性能的评估和表示层分析，利用SHAP和UMAP技术，对不同预训练技术的心电图专家基础模型进行跨大陆数据集和数据稀缺设置的广泛评估。

Result: 实验结果表明，该基准测试协议能够深入洞察心电图专家基础模型的嵌入模式，使研究人员能够更深入地理解其表示结构和泛化能力。

Conclusion: 提出的基准测试框架为心电图专家基础模型提供了全面的评估方法，特别关注表示层分析和数据稀缺情况下的模型表现，有助于更负责任地应用这些模型于医疗领域。

Abstract: The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.

</details>


### [52] [Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework](https://arxiv.org/abs/2601.21844)
*So Fukuhara,Abdallah Alabdallah,Nuwan Gunasekara,Slawomir Nowaczyk*

Main category: cs.AI

TL;DR: 论文提出一个决策中心的仿真框架，用于评估汽车售后市场备件库存管理中预测模型对运营绩效的实际影响，而非仅关注统计准确性。


<details>
  <summary>Details</summary>
Motivation: 汽车售后市场备件需求高度间歇且不确定，传统预测模型评估仅关注统计准确性（如MAE、RMSE），但这些指标与运营KPI（如总成本、服务水平）的关系不明确，需要建立预测模型与库存决策之间的实际联系。

Method: 提出一个决策中心的仿真软件框架，包含三个核心模块：(1) 针对备件需求特征设计的合成需求生成器；(2) 可容纳任意预测模型的灵活预测模块；(3) 消耗预测并计算运营KPI的库存控制仿真器。通过闭环设置系统评估预测模型。

Result: 通过广泛仿真场景表明：传统准确性指标的改进不一定转化为更好的运营绩效；具有相似统计误差特征的模型可能产生显著不同的成本-服务权衡；分析了这些差异以描述预测性能特定方面如何影响库存结果。

Conclusion: 该框架将需求预测与库存管理联系起来，将评估重点从纯粹的预测准确性转向运营相关性，为汽车售后市场及相关领域的模型选择提供指导。

Abstract: Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.

</details>


### [53] [KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement](https://arxiv.org/abs/2601.21864)
*Jinhao Pan,Chahat Raj,Anjishnu Mukherjee,Sina Mansouri,Bowen Wei,Shloka Yada,Ziwei Zhu*

Main category: cs.AI

TL;DR: KnowBias提出了一种轻量级框架，通过增强而非抑制编码偏见知识的神经元来缓解LLM的社会偏见，仅需少量是/否问题即可实现高效去偏，同时保持模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法主要通过修改参数、提示或抑制与偏见行为相关的神经元，但这些方法往往脆弱、泛化性差、数据效率低，且容易损害模型的通用能力。

Method: KnowBias通过基于归因的分析，使用少量偏见知识问题识别编码偏见知识的神经元，然后在推理时选择性地增强这些神经元，而非抑制它们。

Result: 在多个基准测试和LLM上的实验表明，KnowBias实现了最先进的去偏性能，同时保持了最小的通用能力下降，能够泛化到不同类型的偏见和人口统计特征。

Conclusion: KnowBias提供了一种轻量级、数据高效的去偏框架，通过增强偏见知识神经元而非抑制它们，在有效缓解社会偏见的同时保持了LLM的通用能力。

Abstract: Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.

</details>


### [54] [astra-langchain4j: Experiences Combining LLMs and Agent Programming](https://arxiv.org/abs/2601.21879)
*Rem Collier,Katharine Beaumont,Andrei Ciortea*

Main category: cs.AI

TL;DR: 论文探讨了生成式AI和Agentic AI如何影响传统Agent工具包，以及传统工具包经验如何影响新Agent平台设计，通过ASTRA编程语言的LLM集成原型开发经验进行说明。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和Agentic AI（多智能体系统形式）在过去两年的兴起，需要探索这些技术如何影响传统Agent工具包的使用，以及传统工具包中积累的丰富经验如何影响新Agent平台的设计。

Method: 为ASTRA编程语言开发了一个大型语言模型（LLM）集成原型，通过三个示例实现来展示集成方法，并基于这些示例讨论开发经验。

Result: 论文展示了ASTRA工具包的简要概述，通过三个具体示例实现了LLM集成，并总结了从这些实现中获得的实践经验。

Conclusion: 传统Agent工具包与新兴LLM技术之间存在相互影响的关系，ASTRA的LLM集成原型开发经验为理解这种关系提供了有价值的见解，传统工具包的经验可以指导新Agent平台的设计。

Abstract: Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.

</details>


### [55] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: CoMT框架通过模仿人类认知的两阶段过程改进LLM后训练：第一阶段专注抽象推理模式学习，第二阶段通过置信度校准强化学习优化任务适应，显著提升泛化能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练方法将完整推理轨迹作为基本单元进行优化，这与人类实际解决问题的认知过程存在根本差异。人类认知自然地将问题解决分解为两个阶段：首先获取跨问题泛化的抽象策略（元知识），然后将其适应到具体实例。现有方法的问题中心化处理将抽象策略与问题特定执行纠缠在一起，导致泛化能力受限。

Method: 提出认知启发的两阶段框架：1) Chain-of-Meta-Thought (CoMT)：专注于抽象推理模式的有监督学习，不涉及具体执行，获取可泛化的策略；2) Confidence-Calibrated Reinforcement Learning (CCRL)：通过置信度感知奖励优化任务适应，在中间步骤进行置信度校准，防止过度自信的错误级联，提高执行可靠性。

Result: 在4个模型和8个基准测试上的实验显示：相比标准方法，在分布内性能提升2.19%，分布外性能提升4.63%；同时训练时间减少65-70%，token消耗减少50%。

Conclusion: 将后训练与人类认知原则对齐不仅产生更优的泛化能力，还显著提高了训练效率。这表明模仿人类认知过程是改进LLM训练的有效途径。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [56] [ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21912)
*Zhao Wang,Ziliang Zhao,Zhicheng Dou*

Main category: cs.AI

TL;DR: ProRAG提出了一种过程监督强化学习框架，通过整合步骤级监督来优化检索增强生成系统，解决传统结果导向RL中的奖励稀疏性和信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法在复杂推理任务中存在奖励稀疏和信用分配低效的问题，粗粒度的标量奖励无法识别长轨迹中的具体错误步骤，导致"过程幻觉"现象，即模型通过有缺陷的逻辑或冗余检索步骤得到正确答案。

Method: ProRAG框架包含四个阶段：1) 监督策略预热，用结构化推理格式初始化模型；2) 构建基于MCTS的过程奖励模型来量化中间推理质量；3) PRM引导的推理精炼，使策略与细粒度过程偏好对齐；4) 过程监督强化学习，采用双粒度优势机制，聚合步骤级过程奖励和全局结果信号。

Result: 在五个多跳推理基准测试上的广泛实验表明，ProRAG相比基于结果和过程感知的RL基线方法取得了更优的整体性能，特别是在复杂长时程任务上，验证了细粒度过程监督的有效性。

Conclusion: ProRAG通过将学习到的步骤级监督整合到在线优化循环中，有效解决了检索增强生成中强化学习的奖励稀疏和信用分配问题，为复杂推理任务提供了精确的反馈机制。

Abstract: Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to "process hallucinations", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.

</details>


### [57] [JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG](https://arxiv.org/abs/2601.21916)
*Yiqun Chen,Erhan Zhang,Tianyi Hu,Shijie Wang,Zixuan Yang,Meizhi Zhong,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Jiaxin Mao*

Main category: cs.AI

TL;DR: JADE框架通过联合优化动态多轮工作流中的规划和执行模块，解决了现有RAG系统中规划与执行脱节导致的"战略-操作不匹配"问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统存在关键二分法：要么在固定图架构中联合优化模块，要么支持动态规划但将执行器视为冻结的黑盒工具。这种"解耦优化"导致"战略-操作不匹配"，复杂的规划策略因未适应的本地执行器而无法实现，系统复杂度增加却带来负面性能增益。

Method: 提出JADE（联合智能动态执行）框架，将系统建模为在单一共享骨干下统一的多智能体协作团队，通过基于结果的奖励实现端到端学习，促进规划器与执行器的协同适应。

Result: JADE将分离的模块转变为协同系统，通过联合优化带来显著的性能改进，并通过动态工作流编排实现效率与效果之间的灵活平衡。

Conclusion: JADE框架通过联合优化规划和执行模块，解决了RAG系统中规划与执行脱节的问题，实现了更好的协同适应和性能提升。

Abstract: The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \textbf{JADE} (\textbf{J}oint \textbf{A}gentic \textbf{D}ynamic \textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.

</details>


### [58] [Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21919)
*Yiqun Chen,Jinyuan Feng,Wei Yang,Meizhi Zhong,Zhengliang Shi,Rui Li,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Zhiqiang Pu,Jiaxin Mao*

Main category: cs.AI

TL;DR: 提出多智能体强化学习框架SCMA，通过分割和评分智能体选择性惩罚冗余推理块，在减少响应长度11.1%-39.0%的同时提升准确率4.33%-10.02%


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的解决方案通过简单地将长度惩罚与结果奖励耦合来解决推理冗余问题，但这种简单的奖励加权难以平衡简洁性与准确性，强制简洁可能损害关键推理逻辑

Method: 提出多智能体强化学习框架SCMA，包含三个智能体：分割智能体将推理过程分解为逻辑块，评分智能体量化每个块的重要性，推理智能体在训练期间使用重要性加权的长度惩罚，部署时不引入推理开销

Result: SCMA在多个模型规模上减少响应长度11.1%至39.0%，同时提升准确率4.33%至10.02%。消融研究和定性分析验证了MARL框架中的协同优化促进了涌现行为

Conclusion: SCMA框架通过多智能体协作选择性惩罚冗余推理块，有效平衡了推理的简洁性与准确性，相比传统强化学习范式产生了更强大的大型推理模型

Abstract: The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.

</details>


### [59] [AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making](https://arxiv.org/abs/2601.21936)
*Jon Chun,Kathrine Elkins,Yong Suk Lee*

Main category: cs.AI

TL;DR: AgenticSimLaw是一个基于法庭辩论结构的多智能体框架，用于高风险表格决策任务，提供透明可控的推理过程，相比单智能体方法在预测性能上更稳定且可解释。


<details>
  <summary>Details</summary>
Motivation: 针对高风险决策任务（如刑事司法预测）需要透明、可审计的推理过程，而传统黑盒方法缺乏解释性和可控性。需要一种能够提供完整交互记录、支持人类监督的决策框架。

Method: 提出法庭风格的多智能体辩论框架，明确定义角色（检察官、辩护律师、法官）、7轮结构化辩论协议和私有推理策略。在NLSY97数据集上进行年轻成人再犯预测的基准测试，比较了近90种模型和策略组合。

Result: 结构化多智能体辩论相比单智能体思维链提示提供更稳定和可泛化的性能，准确率和F1分数之间的相关性更强。框架提供细粒度推理控制、完整交互记录用于可解释性，并能系统分析智能体行为。

Conclusion: AgenticSimLaw通过结构化角色、可观察的交互记录和明确的非部署约束，解决了LLM多智能体系统的关键挑战。虽然以刑事司法领域为例，但该方法可推广到任何需要透明度和人类监督的高风险决策任务。

Abstract: We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.

</details>


### [60] [The Energy Impact of Domain Model Design in Classical Planning](https://arxiv.org/abs/2601.21967)
*Ilche Georgievski,Serhat Tekin,Marco Aiello*

Main category: cs.AI

TL;DR: 论文研究了领域模型特征对经典规划器能耗的影响，发现领域级修改能产生可测量的能耗差异，且能耗与运行时间并非总是相关。


<details>
  <summary>Details</summary>
Motivation: 传统AI研究优先考虑算法性能（如机器学习准确率或自动规划运行时间），而新兴的绿色AI范式将能耗视为关键性能维度。尽管自动规划计算需求高，但其能效问题很少受到关注，特别是在模块化规划结构中，领域模型与算法独立指定，这为通过领域模型设计系统分析能耗提供了机会。

Method: 引入领域模型配置框架，能够控制特征变化（如元素排序、动作元数、死锁状态）。使用5个基准领域和5个最先进的规划器，分析每个基准32个领域变体的能耗和运行时间影响。

Result: 结果表明，领域级修改在不同规划器中产生可测量的能耗差异，且能耗消耗与运行时间并非总是相关。

Conclusion: 领域模型设计对规划器能耗有显著影响，这为绿色AI在自动规划领域的应用提供了实证基础，表明通过优化领域模型特征可以改善规划系统的能效。

Abstract: AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.

</details>


### [61] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: 论文提出两种多智能体演员-评论家方法（CoLLM-CC和CoLLM-DC）来优化去中心化LLM协作，相比蒙特卡洛方法在长视野或稀疏奖励任务中表现更优


<details>
  <summary>Details</summary>
Motivation: 现有MARL微调方法依赖预定义执行协议且需要集中式执行，而去中心化LLM协作更具实际应用价值；当前方法使用蒙特卡洛方法存在高方差问题，需要更多样本训练

Method: 提出两种多智能体演员-评论家方法：CoLLM-CC（集中式评论家）和CoLLM-DC（去中心化评论家），用于优化去中心化LLM协作

Result: 在写作、编码和游戏领域实验中，蒙特卡洛方法和CoLLM-DC在短视野和密集奖励任务中表现与CoLLM-CC相当；但在长视野或稀疏奖励任务中，两者均表现不佳，蒙特卡洛方法需要更多样本，CoLLM-DC难以收敛

Conclusion: MAAC方法在优化去中心化LLM协作中具有优势，特别是在长视野或稀疏奖励任务中，CoLLM-CC方法表现最佳

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [62] [Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models](https://arxiv.org/abs/2601.21975)
*Pranav Mahajan,Ihor Kendiukhov,Syed Hussain,Lydia Nottingham*

Main category: cs.AI

TL;DR: 研究发现语言模型的陈述-揭示偏好差距高度依赖评估协议，通过允许中立和弃权可以改善相关性，但完全弃权会导致相关性降至零或负值，表明偏好评估需要考虑不确定偏好。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要依赖二元强制选择提示，这混淆了真实偏好与评估协议的人工痕迹，需要系统研究评估协议如何影响语言模型的陈述-揭示偏好相关性。

Method: 系统研究24个语言模型，通过允许中立和弃权来改进陈述偏好评估，排除弱信号；进一步允许揭示偏好中的弃权；使用系统提示引导在揭示偏好评估中应用陈述偏好。

Result: 允许中立和弃权显著改善Spearman等级相关性；但进一步允许揭示偏好中的弃权导致相关性接近零或负值（因高中立率）；系统提示引导在AIRiskDilemmas上不能可靠改善相关性。

Conclusion: 陈述-揭示偏好相关性高度依赖评估协议，偏好评估需要能够处理不确定偏好的方法，不能简单依赖强制选择或弃权机制。

Abstract: Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.

</details>


### [63] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: VERSA是一个用于足球事件数据验证的系统框架，通过状态转换模型检测和纠正事件流中的逻辑不一致性，显著提高数据质量和下游分析可靠性。


<details>
  <summary>Details</summary>
Motivation: 事件流数据在体育等领域用于细粒度分析，但数据质量问题（如事件顺序错误、事件缺失）导致逻辑不一致，限制了分析模型的可靠性。

Method: 提出VERSA验证框架，基于状态转换模型定义有效事件序列，自动检测和纠正事件流数据中的异常模式。

Result: 在K League 1（2024赛季）数据中发现18.81%的事件存在逻辑不一致；VERSA显著提高了跨数据提供商的一致性，增强了VAEP等下游任务的鲁棒性和性能。

Conclusion: 验证过程能有效提高数据驱动分析的可靠性，VERSA框架为足球事件数据提供了系统化的完整性保障。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [64] [Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems](https://arxiv.org/abs/2601.21993)
*Dhiogo de Sá,Carlos Schmiedel,Carlos Pereira Lopes*

Main category: cs.AI

TL;DR: 本文提出"液态接口"协调范式，将接口从静态技术制品转变为运行时通过意图表达和语义协商产生的短暂关系事件，以支持自适应、概率性和上下文相关的智能体推理。


<details>
  <summary>Details</summary>
Motivation: 当前软件架构难以支持自适应、概率性和上下文相关的自主智能体推理，而系统集成仍被静态接口和确定性契约所主导。需要一种新的协调范式来应对这一挑战。

Method: 引入液态接口协调范式，将接口定义为短暂的关系事件而非持久技术制品；形式化该模型并提出液态接口协议(LIP)，管理意图驱动交互、协商执行和在语义不确定性下的短暂性执行；讨论治理影响并描述参考架构。

Result: 液态接口协议(LIP)为基于智能体的系统提供自适应协调的原则性基础，通过参考架构展示了实际可行性。

Conclusion: 液态接口为基于智能体的系统中的自适应协调提供了原则性基础，解决了静态接口与自适应智能体推理之间的不匹配问题。

Abstract: Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems

</details>


### [65] [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](https://arxiv.org/abs/2601.22027)
*Johannes Kirmayr,Lukas Stappen,Elisabeth André*

Main category: cs.AI

TL;DR: CAR-bench是一个针对车载语音助手的LLM智能体基准测试，专注于评估一致性、不确定性处理和能力意识，包含幻觉任务和消歧任务，揭示现有模型在真实场景中的可靠性不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体基准测试过于理想化，忽略了真实用户场景中的可靠性问题。车载语音助手等实际应用中，用户经常提出不完整或模糊的请求，智能体需要通过对话、工具使用和政策遵守来管理内在不确定性。

Method: 提出CAR-bench基准测试，包含LLM模拟用户、领域政策和58个互连工具（导航、生产力、充电、车辆控制）。除了标准任务完成外，还引入：1）幻觉任务测试智能体在工具或信息缺失时的极限意识；2）消歧任务要求通过澄清或内部信息收集解决不确定性。

Result: 基线结果显示所有任务类型中偶尔成功与持续成功之间存在巨大差距。即使是前沿推理LLM在消歧任务中的持续通过率也低于50%（因过早行动），在幻觉任务中经常违反政策或编造信息以满足用户请求。

Conclusion: CAR-bench揭示了现有LLM智能体在真实世界应用中的可靠性不足，突显了开发更可靠、更具自我意识的LLM智能体的迫切需求，特别是在车载助手等用户交互场景中。

Abstract: Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.

</details>


### [66] [Defining Operational Conditions for Safety-Critical AI-Based Systems from Data](https://arxiv.org/abs/2601.22118)
*Johann Christensen,Elena Hoemann,Frank Köster,Sven Hallerbach*

Main category: cs.AI

TL;DR: 本文提出了一种基于核方法的后验ODD定义方法，通过数据驱动方式从已有数据中推导运行设计域，解决了传统先验定义ODD的挑战。


<details>
  <summary>Details</summary>
Motivation: 在安全关键AI系统中，运行设计域(ODD)的定义对于系统认证至关重要。然而，在复杂现实系统或已有数据场景下，传统基于专家知识的先验ODD定义方法面临环境描述不完整的挑战。

Method: 提出了一种基于安全设计的后验ODD定义方法，使用多维核基表示从先前收集的数据中推导ODD。该方法通过蒙特卡洛方法和实际航空用例进行验证。

Result: 研究表明，数据驱动的ODD可以与原始隐藏的ODD相等。该方法为数据驱动的安全关键AI系统认证提供了可行途径。

Conclusion: 基于安全设计的核基ODD方法能够支持未来数据驱动的安全关键AI系统认证，解决了传统ODD定义方法的局限性。

Abstract: Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.

</details>


### [67] [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154)
*Kaixuan Fan,Kaituo Feng,Manyuan Zhang,Tianshuo Peng,Zhixun Li,Yilei Jiang,Shuang Chen,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.AI

TL;DR: 本文提出了Agent Reasoning Reward Model (Agent-RRM)，一种为智能体轨迹提供结构化反馈的多方面奖励模型，包含显式推理轨迹、针对性批判和整体评分，并通过三种集成策略显著提升了智能体强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能体强化学习方法主要依赖稀疏的结果奖励进行训练，这种反馈无法区分中间推理质量，导致训练结果不理想。需要更细粒度的反馈机制来提升智能体推理能力。

Method: 提出Agent-RRM奖励模型，提供三方面结构化反馈：1) 显式推理轨迹，2) 针对性批判（突出推理缺陷并提供改进指导），3) 整体过程评分。基于这些信号，研究了三种集成策略：Reagent-C（文本增强精炼）、Reagent-R（奖励增强指导）和Reagent-U（统一反馈集成）。

Result: 在12个多样化基准测试上的广泛评估表明，Reagent-U策略取得了显著的性能飞跃：在GAIA上达到43.7%，在WebWalkerQA上达到46.2%，验证了推理奖励模型和训练方案的有效性。

Conclusion: Agent-RRM通过提供结构化、多方面的推理反馈，有效解决了传统稀疏奖励的局限性，显著提升了智能体强化学习的性能。代码、模型和数据集均已开源以促进未来研究。

Abstract: Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

</details>

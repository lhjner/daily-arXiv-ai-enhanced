{"id": "2602.20333", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20333", "abs": "https://arxiv.org/abs/2602.20333", "authors": ["Samarth KaPatel", "Sofia Nikiforova", "Giacinto Paolo Saggese", "Paul Smith"], "title": "DMCD: Semantic-Statistical Framework for Causal Discovery", "comment": null, "summary": "We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.\n  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.", "AI": {"tldr": "DMCD\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u7ed3\u5408LLM\u8bed\u4e49\u8349\u56fe\u548c\u7edf\u8ba1\u9a8c\u8bc1\uff0c\u901a\u8fc7\u53d8\u91cf\u5143\u6570\u636e\u751f\u6210\u7a00\u758fDAG\u8349\u56fe\uff0c\u518d\u7528\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u8fdb\u884c\u5ba1\u8ba1\u548c\u7cbe\u70bc\u3002", "motivation": "\u4f20\u7edf\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u901a\u5e38\u4ec5\u4f9d\u8d56\u7edf\u8ba1\u6d4b\u8bd5\uff0c\u7f3a\u4e4f\u5bf9\u53d8\u91cf\u8bed\u4e49\u5173\u7cfb\u7684\u7406\u89e3\u3002DMCD\u65e8\u5728\u6574\u5408LLM\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u4e0e\u7edf\u8ba1\u9a8c\u8bc1\uff0c\u63d0\u9ad8\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u53d8\u91cf\u5143\u6570\u636e\u751f\u6210\u7a00\u758fDAG\u8349\u56fe\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u5ba1\u8ba1\u8349\u56fe\uff0c\u68c0\u6d4b\u5dee\u5f02\u5e76\u6307\u5bfc\u6709\u9488\u5bf9\u6027\u7684\u8fb9\u4fee\u6b63\u3002", "result": "\u5728\u5de5\u4e1a\u5de5\u7a0b\u3001\u73af\u5883\u76d1\u6d4b\u548cIT\u7cfb\u7edf\u5206\u6790\u4e09\u4e2a\u5143\u6570\u636e\u4e30\u5bcc\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDMCD\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u6216\u9886\u5148\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5c06\u8bed\u4e49\u5148\u9a8c\u4e0e\u539f\u5219\u6027\u7edf\u8ba1\u9a8c\u8bc1\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u4ea7\u751f\u9ad8\u6027\u80fd\u4e14\u5b9e\u9645\u6709\u6548\u7684\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u65b9\u6cd5\uff0c\u6539\u8fdb\u6e90\u4e8e\u5bf9\u5143\u6570\u636e\u7684\u8bed\u4e49\u63a8\u7406\u800c\u975e\u57fa\u51c6\u56fe\u7684\u8bb0\u5fc6\u3002"}}
{"id": "2602.20558", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.20558", "abs": "https://arxiv.org/abs/2602.20558", "authors": ["Yucheng Shi", "Ying Li", "Yu Wang", "Yesu Feng", "Arjun Rao", "Rein Houthooft", "Shradha Sehgal", "Jin Wang", "Hao Zhen", "Ninghao Liu", "Linas Baltrunas"], "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production", "comment": "Work in progress", "summary": "Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3verbalization agent\uff0c\u5c06\u7ed3\u6784\u5316\u7528\u6237\u4ea4\u4e92\u8bb0\u5f55\u4f18\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u8f93\u5165\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u7684\u6838\u5fc3\u9762\u4e34\u5173\u952e\u6311\u6218\uff1a\u5982\u4f55\u5c06\u7ed3\u6784\u5316\u7528\u6237\u4ea4\u4e92\u8bb0\u5f55\u6709\u6548\u5730\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u7684\u6a21\u677f\u62fc\u63a5\uff0c\u65e0\u6cd5\u751f\u6210\u6700\u4f18\u7684\u6587\u672c\u8868\u793a\u3002", "method": "\u63d0\u51fa\u6570\u636e\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3verbalization agent\uff0c\u8be5\u667a\u80fd\u4f53\u5c06\u539f\u59cb\u4ea4\u4e92\u5386\u53f2\u8f6c\u5316\u4e3a\u4f18\u5316\u7684\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u4ee5\u63a8\u8350\u51c6\u786e\u6027\u4f5c\u4e3a\u8bad\u7ec3\u4fe1\u53f7\u3002\u667a\u80fd\u4f53\u5b66\u4e60\u8fc7\u6ee4\u566a\u58f0\u3001\u6574\u5408\u76f8\u5173\u5143\u6570\u636e\u3001\u91cd\u7ec4\u4fe1\u606f\u4ee5\u6539\u5584\u4e0b\u6e38\u9884\u6d4b\u3002", "result": "\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u6d41\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b66\u4e60\u5230\u7684verbalization\u76f8\u6bd4\u57fa\u4e8e\u6a21\u677f\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u53d1\u73b0\u9879\u76ee\u63a8\u8350\u51c6\u786e\u6027\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe93%\u7684\u76f8\u5bf9\u63d0\u5347\u3002\u5206\u6790\u63ed\u793a\u4e86\u6d8c\u73b0\u7684\u7b56\u7565\uff0c\u5305\u62ec\u7528\u6237\u5174\u8da3\u603b\u7ed3\u3001\u566a\u58f0\u53bb\u9664\u548c\u8bed\u6cd5\u89c4\u8303\u5316\u3002", "conclusion": "\u5b66\u4e60verbalization\u662f\u63d0\u5347LLM\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u6784\u5efa\u89c1\u89e3\u3002"}}
{"id": "2602.20628", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20628", "abs": "https://arxiv.org/abs/2602.20628", "authors": ["Nelson Gardner-Challis", "Jonathan Bostock", "Georgiy Kozhevnikov", "Morgan Sinclaire", "Joan Velja", "Alessandro Abate", "Charlie Griffin"], "title": "When can we trust untrusted monitoring? A safety case sketch across collusion strategies", "comment": "66 pages, 14 figures, Preprint", "summary": "AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e0d\u53ef\u4fe1\u76d1\u63a7\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u653e\u5bbd\u5bf9AI\u4e32\u8c0b\u7b56\u7565\u7684\u5047\u8bbe\uff0c\u63d0\u51fa\u66f4\u5168\u9762\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u8bc6\u522b\u51fa\u88ab\u52a8\u81ea\u6211\u8bc6\u522b\u53ef\u80fd\u6bd4\u5148\u524d\u7814\u7a76\u7684\u7b56\u7565\u66f4\u6709\u6548\u7684\u60c5\u51b5\u3002", "motivation": "\u968f\u7740AI\u81ea\u4e3b\u6027\u548c\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u9519\u4f4dAI\u53ef\u80fd\u9020\u6210\u707e\u96be\u6027\u5371\u5bb3\u7684\u98ce\u9669\u589e\u52a0\u3002\u4e0d\u53ef\u4fe1\u76d1\u63a7\uff08\u7528\u4e00\u4e2a\u4e0d\u53ef\u4fe1\u6a21\u578b\u76d1\u7763\u53e6\u4e00\u4e2a\uff09\u662f\u964d\u4f4e\u98ce\u9669\u7684\u65b9\u6cd5\u4e4b\u4e00\uff0c\u4f46\u7531\u4e8e\u65e0\u6cd5\u5b89\u5168\u90e8\u7f72\u9519\u4f4d\u6a21\u578b\u76f4\u63a5\u6d4b\u8bd5\u534f\u8bae\uff0c\u8bc1\u660e\u5176\u5b89\u5168\u6027\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u57fa\u4e8e\u73b0\u6709\u7684\u90e8\u7f72\u524d\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u653e\u5bbd\u5148\u524dAI\u63a7\u5236\u7814\u7a76\u4e2d\u5bf9\u9519\u4f4dAI\u53ef\u80fd\u4f7f\u7528\u7684\u4e32\u8c0b\u7b56\u7565\u7684\u5047\u8bbe\uff0c\u5f00\u53d1\u6db5\u76d6\u88ab\u52a8\u81ea\u6211\u8bc6\u522b\u3001\u56e0\u679c\u4e32\u8c0b\uff08\u9690\u85cf\u9884\u5171\u4eab\u4fe1\u53f7\uff09\u3001\u975e\u56e0\u679c\u4e32\u8c0b\uff08\u901a\u8fc7\u8c22\u6797\u70b9\u9690\u85cf\u4fe1\u53f7\uff09\u548c\u7ec4\u5408\u7b56\u7565\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u521b\u5efa\u5b89\u5168\u6848\u4f8b\u8349\u56fe\u6765\u660e\u786e\u8bba\u8bc1\u3002", "result": "\u8bc6\u522b\u51fa\u88ab\u52a8\u81ea\u6211\u8bc6\u522b\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u53ef\u80fd\u6bd4\u5148\u524d\u7814\u7a76\u7684\u4e32\u8c0b\u7b56\u7565\u66f4\u6709\u6548\u7684\u60c5\u51b5\uff0c\u4e3a\u4e0d\u53ef\u4fe1\u76d1\u63a7\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u653e\u5bbd\u5bf9AI\u4e32\u8c0b\u7b56\u7565\u7684\u5047\u8bbe\uff0c\u5f00\u53d1\u4e86\u66f4\u5168\u9762\u7684\u5206\u7c7b\u4f53\u7cfb\u548c\u5b89\u5168\u8bba\u8bc1\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u7a33\u5065\u7684\u4e0d\u53ef\u4fe1\u76d1\u63a7\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f46\u4ecd\u5b58\u5728\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002"}}
{"id": "2602.20638", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20638", "abs": "https://arxiv.org/abs/2602.20638", "authors": ["Vincent Auriau", "Khaled Belahcene", "Emmanuel Malherbe", "Vincent Mousseau", "Marc Pirlot"], "title": "Identifying two piecewise linear additive value functions from anonymous preference information", "comment": null, "summary": "Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540c\u65f6\u5411\u4e24\u4f4d\u51b3\u7b56\u8005\u63d0\u95ee\u4ee5\u83b7\u53d6\u5176\u504f\u597d\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5728\u4e0d\u77e5\u9053\u54ea\u4e2a\u56de\u7b54\u5bf9\u5e94\u54ea\u4e2a\u51b3\u7b56\u8005\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5206\u6bb5\u7ebf\u6027\u8fb9\u9645\u4ef7\u503c\u51fd\u6570\u8bc6\u522b\u4e24\u4e2a\u504f\u597d\u6a21\u578b", "motivation": "\u5728\u540c\u65f6\u5411\u4e24\u4f4d\u51b3\u7b56\u8005\u63d0\u95ee\u65f6\uff0c\u867d\u7136\u80fd\u83b7\u5f97\u4e24\u4e2a\u56de\u7b54\uff0c\u4f46\u4e0d\u77e5\u9053\u54ea\u4e2a\u56de\u7b54\u5bf9\u5e94\u54ea\u4e2a\u51b3\u7b56\u8005\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u533f\u540d\u56de\u7b54\u60c5\u51b5\u4e0b\u7684\u504f\u597d\u6a21\u578b\u8bc6\u522b\u95ee\u9898", "method": "\u5047\u8bbe\u504f\u597d\u53ef\u4ee5\u7528\u52a0\u6027\u4ef7\u503c\u51fd\u6570\u8868\u793a\uff0c\u8fb9\u9645\u4ef7\u503c\u51fd\u6570\u4e3a\u5206\u6bb5\u7ebf\u6027\u4e14\u65ad\u70b9\u5df2\u77e5\uff0c\u8bbe\u8ba1\u4e00\u79cd\u542f\u53d1\u7a0b\u5e8f\u6765\u540c\u65f6\u8bc6\u522b\u4e24\u4e2a\u51b3\u7b56\u8005\u7684\u504f\u597d\u6a21\u578b", "result": "\u63d0\u51fa\u7684\u542f\u53d1\u7a0b\u5e8f\u80fd\u591f\u5728\u533f\u540d\u56de\u7b54\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u8bc6\u522b\u51fa\u4e24\u4f4d\u51b3\u7b56\u8005\u5404\u81ea\u7684\u504f\u597d\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u540c\u65f6\u5411\u591a\u4f4d\u51b3\u7b56\u8005\u63d0\u95ee\u65f6\u7684\u533f\u540d\u56de\u7b54\u95ee\u9898\uff0c\u4e3a\u7fa4\u4f53\u51b3\u7b56\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u624b\u6bb5"}}
{"id": "2602.20639", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20639", "abs": "https://arxiv.org/abs/2602.20639", "authors": ["Bo Zhang", "Jinfeng Zhou", "Yuxuan Chen", "Jianing Yin", "Minlie Huang", "Hongning Wang"], "title": "Grounding LLMs in Scientific Discovery via Embodied Actions", "comment": "24 pages, 7 figures, 7 tables. Preprint", "summary": "Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive \"execute-then-response\" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.", "AI": {"tldr": "EmbodiedAct\u6846\u67b6\u5c06\u79d1\u5b66\u8f6f\u4ef6\u8f6c\u5316\u4e3a\u4e3b\u52a8\u7684\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u7d27\u5bc6\u7684\u611f\u77e5-\u6267\u884c\u5faa\u73af\u8ba9LLMs\u5728\u5177\u8eab\u884c\u52a8\u4e2d\u63a5\u5730\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7406\u8bba\u4e0e\u7269\u7406\u6a21\u62df\u4e4b\u95f4\u7684\u9e3f\u6c9f\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u96be\u4ee5\u5f25\u5408\u7406\u8bba\u63a8\u7406\u4e0e\u53ef\u9a8c\u8bc1\u7269\u7406\u6a21\u62df\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u91c7\u7528\u88ab\u52a8\u7684\"\u6267\u884c-\u54cd\u5e94\"\u5faa\u73af\uff0c\u7f3a\u4e4f\u8fd0\u884c\u65f6\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u5904\u7406\u77ac\u6001\u5f02\u5e38\uff08\u5982\u6570\u503c\u4e0d\u7a33\u5b9a\u6216\u53d1\u6563\u632f\u8361\uff09\u3002", "method": "\u63d0\u51faEmbodiedAct\u6846\u67b6\uff0c\u5c06\u6210\u719f\u7684\u79d1\u5b66\u8f6f\u4ef6\u8f6c\u5316\u4e3a\u4e3b\u52a8\u7684\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u7d27\u5bc6\u7684\u611f\u77e5-\u6267\u884c\u5faa\u73af\u8ba9LLMs\u5728\u5177\u8eab\u884c\u52a8\u4e2d\u63a5\u5730\u3002\u8be5\u6846\u67b6\u5728MATLAB\u73af\u5883\u4e2d\u5b9e\u4f8b\u5316\u3002", "result": "\u5728\u590d\u6742\u7684\u5de5\u7a0b\u8bbe\u8ba1\u548c\u79d1\u5b66\u5efa\u6a21\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEmbodiedAct\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u957f\u65f6\u7a0b\u6a21\u62df\u4e2d\u786e\u4fdd\u6ee1\u610f\u7684\u53ef\u9760\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u79d1\u5b66\u5efa\u6a21\u4e2d\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "EmbodiedAct\u901a\u8fc7\u5c06\u79d1\u5b66\u8f6f\u4ef6\u8f6c\u5316\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7406\u8bba\u4e0e\u7269\u7406\u6a21\u62df\u4e4b\u95f4\u7684\u9e3f\u6c9f\u95ee\u9898\uff0c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20659", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20659", "abs": "https://arxiv.org/abs/2602.20659", "authors": ["Vaidehi Bagaria", "Bijo Sebastian", "Nirav Patel"], "title": "Recursive Belief Vision Language Model", "comment": null, "summary": "Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to \u03c00. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.", "AI": {"tldr": "RB-VLA\u63d0\u51fa\u57fa\u4e8e\u4fe1\u5ff5\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u4e16\u754c\u6a21\u578b\u76ee\u6807\u7ef4\u62a4\u7d27\u51d1\u7684\u6f5c\u5728\u72b6\u6001\u8868\u793a\uff0c\u89e3\u51b3\u957f\u65f6\u7a0b\u64cd\u4f5c\u4e2d\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u5e76\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\uff1a\u4f9d\u8d56\u77ed\u4e0a\u4e0b\u6587\u7a97\u53e3\u6216\u91cd\u590d\u67e5\u8be2\u5bfc\u81f4\u4efb\u52a1\u8fdb\u5ea6\u4e22\u5931\u3001\u611f\u77e5\u6df7\u6dc6\u4e0b\u7684\u52a8\u4f5c\u91cd\u590d\u3001\u9ad8\u63a8\u7406\u5ef6\u8fdf\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u6301\u4e45\u3001\u52a8\u4f5c\u6761\u4ef6\u7684\u72b6\u6001\u8868\u793a\u548c\u6709\u9650\u7684\u65f6\u7a7a\u7269\u7406\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faRB-VLA\u67b6\u6784\uff0c\u91c7\u7528\u4fe1\u5ff5\u4e2d\u5fc3\u8bbe\u8ba1\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u4e16\u754c\u6a21\u578b\u76ee\u6807\u8bad\u7ec3\uff0c\u7ef4\u62a4\u7d27\u51d1\u7684\u6f5c\u5728\u72b6\u6001\u7f16\u7801\u4efb\u52a1\u76f8\u5173\u5386\u53f2\u3001\u52a8\u6001\u548c\u7269\u4f53\u4ea4\u4e92\u3002VLM\u4ec5\u7528\u4e8e\u9ad8\u5c42\u610f\u56fe\u67e5\u8be2\uff0c\u4fe1\u5ff5\u6a21\u5757\u8ddf\u8e2a\u4efb\u52a1\u8fdb\u5ea6\u5e76\u652f\u6301\u90e8\u5206\u53ef\u89c2\u6d4b\u4e0b\u7684\u9636\u6bb5\u611f\u77e5\u3001\u56e0\u679c\u57fa\u7840\u63a7\u5236\uff0c\u65e0\u9700\u5b58\u50a8\u539f\u59cb\u89c2\u6d4b\u6216\u968f\u65f6\u95f4\u6269\u5c55\u5185\u5b58\u3002\u4fe1\u5ff5\u548c\u610f\u56fe\u5171\u540c\u6761\u4ef6\u5316\u6269\u6563\u7b56\u7565\u8fdb\u884c\u9c81\u68d2\u95ed\u73af\u6267\u884c\u3002", "result": "\u5728\u957f\u65f6\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709VLA\u6a21\u578b\uff1a\u591a\u9636\u6bb5\u62fe\u653e\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad852.5%\uff0c\u5806\u53e0\u4efb\u52a1\u63d0\u9ad837.5%\uff08\u76f8\u6bd4\u03c00\uff09\uff1b\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe5\u500d\uff1b\u6d88\u9664\u73b0\u6709VLA\u4e2d\u968f\u65f6\u95f4\u6b65\u589e\u957f\u7684\u5185\u5b58\u95ee\u9898\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u4fe1\u5ff5\u6a21\u5757\u662f\u6027\u80fd\u63d0\u5347\u7684\u4e3b\u8981\u9a71\u52a8\u529b\uff0c\u6210\u529f\u7387\u4ece32.5%\u63d0\u5347\u81f377.5%\u3002", "conclusion": "\u57fa\u4e8e\u4fe1\u5ff5\u7684\u72b6\u6001\u8868\u793a\u5bf9\u957f\u65f6\u7a0bVLA\u7b56\u7565\u5177\u6709\u663e\u8457\u6548\u679c\uff0cRB-VLA\u901a\u8fc7\u7d27\u51d1\u7684\u6f5c\u5728\u72b6\u6001\u8868\u793a\u89e3\u51b3\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u591a\u9636\u6bb5\u63a7\u5236\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20687", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20687", "abs": "https://arxiv.org/abs/2602.20687", "authors": ["Bo Peng", "Pi Bu", "Keyu Pan", "Xinrun Xu", "Yinxiu Zhao", "Miao Chen", "Yang Du", "Lin Li", "Jun Song", "Tong Xu"], "title": "How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective", "comment": null, "summary": "Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.", "AI": {"tldr": "NativeEmbodied\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5177\u8eab\u667a\u80fd\u4f53\u7684\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u539f\u751f\u4f4e\u7ea7\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u8bc4\u4f30\u9ad8\u7ea7\u4efb\u52a1\u548c\u4f4e\u7ea7\u6280\u80fd\u3002", "motivation": "\u73b0\u6709VLM\u9a71\u52a8\u7684\u5177\u8eab\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u4f9d\u8d56\u9ad8\u7ea7\u547d\u4ee4\u6216\u79bb\u6563\u5316\u52a8\u4f5c\u7a7a\u95f4\uff0c\u8fd9\u4e0e\u771f\u5b9e\u4e16\u754c\u63a7\u5236\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u540c\u65f6\uff0c\u5f53\u524d\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u9ad8\u7ea7\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u4f4e\u7ea7\u548c\u9ad8\u7ea7\u5c42\u9762\u7684\u8054\u5408\u8bc4\u4f30\u5206\u6790\u3002", "method": "\u5728\u591a\u6837\u5316\u6a21\u62df\u573a\u666f\u4e0a\u6784\u5efaNativeEmbodied\u57fa\u51c6\uff0c\u5305\u542b\u590d\u6742\u573a\u666f\u4e2d\u7684\u4e09\u4e2a\u4ee3\u8868\u6027\u9ad8\u7ea7\u4efb\u52a1\u6765\u8bc4\u4f30\u6574\u4f53\u6027\u80fd\u3002\u4e3a\u4e86\u66f4\u8be6\u7ec6\u5206\u6790\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u6240\u9700\u6280\u80fd\u89e3\u8026\uff0c\u6784\u5efa\u56db\u7c7b\u4f4e\u7ea7\u4efb\u52a1\uff0c\u6bcf\u7c7b\u9488\u5bf9\u4e00\u4e2a\u57fa\u7840\u5177\u8eab\u6280\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u7840\u5177\u8eab\u6280\u80fd\u4e0a\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u8fdb\u4e00\u6b65\u5206\u6790\u663e\u793a\u8fd9\u4e9b\u74f6\u9888\u663e\u8457\u9650\u5236\u4e86\u9ad8\u7ea7\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "NativeEmbodied\u63ed\u793a\u4e86\u5f53\u524dVLM\u9a71\u52a8\u5177\u8eab\u667a\u80fd\u4f53\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u6027\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u5728\u7edf\u4e00\u539f\u751f\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u8fdb\u884c\u591a\u7c92\u5ea6\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.20696", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20696", "abs": "https://arxiv.org/abs/2602.20696", "authors": ["Baolong Bi", "Yuyao Ge", "Shenghua Liu", "Yuchen He", "Siqian Tong", "Lizhe Chen", "Lingrui Mei", "Zehao Li", "Yiwei Wang", "Yujun Cai", "Ming-Hsuan Yang", "Xueqi Cheng"], "title": "PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding", "comment": null, "summary": "Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the \"3H\" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.", "AI": {"tldr": "PromptCD\u662f\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3001\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5bf9\u6bd4\u6b63\u8d1f\u5f15\u5bfc\u63d0\u793a\u6765\u63a7\u5236AI\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eLLMs\u548cVLMs\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u9f50\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u5927\u591a\u9700\u8981\u5728\u8bad\u7ec3\u65f6\u8fdb\u884c\uff0c\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u867d\u7136\u80fd\u5229\u7528\u6a21\u578b\u5185\u90e8\u5206\u5e03\u63d0\u5347\u7279\u5b9a\u80fd\u529b\uff0c\u4f46\u9002\u7528\u8303\u56f4\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u6d4b\u8bd5\u65f6\u884c\u4e3a\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPolarity-Prompt Contrastive Decoding (PromptCD)\uff1a\u4e3a\u76ee\u6807\u884c\u4e3a\u6784\u5efa\u6b63\u8d1f\u5f15\u5bfc\u63d0\u793a\u5bf9\uff0c\u5bf9\u6bd4\u6a21\u578b\u54cd\u5e94\uff08LLMs\u4e2d\u7684token\u7ea7\u6982\u7387\u5206\u5e03\u548cVLMs\u4e2d\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u6a21\u5f0f\uff09\uff0c\u4ee5\u5f3a\u5316\u671f\u671b\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u589e\u5f3a\u76ee\u6807\u3002", "result": "\u5728LLMs\u7684\"3H\"\u5bf9\u9f50\u76ee\u6807\uff08helpfulness, honesty, harmlessness\uff09\u4e0a\u53d6\u5f97\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6539\u8fdb\uff0c\u8868\u660e\u540e\u8bad\u7ec3\u6a21\u578b\u80fd\u5728\u6d4b\u8bd5\u65f6\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u81ea\u6211\u589e\u5f3a\u3002\u5728VLMs\u4e2d\uff0cPromptCD\u901a\u8fc7\u5f3a\u5316\u884c\u4e3a\u4e00\u81f4\u7684\u89c6\u89c9\u57fa\u7840\u663e\u8457\u63d0\u5347VQA\u6027\u80fd\u3002", "conclusion": "PromptCD\u662f\u4e00\u79cd\u7b80\u5355\u3001\u901a\u7528\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u7b56\u7565\uff0c\u53ef\u5728\u8de8\u6a21\u6001\u573a\u666f\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u884c\u4e3a\u63a7\u5236\uff0c\u4e3aAI\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20710", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20710", "abs": "https://arxiv.org/abs/2602.20710", "authors": ["Peter Hase", "Christopher Potts"], "title": "Counterfactual Simulation Training for Chain-of-Thought Faithfulness", "comment": null, "summary": "Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCounterfactual Simulation Training (CST)\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u5956\u52b1\u90a3\u4e9b\u80fd\u591f\u4f7f\u6a21\u62df\u5668\u51c6\u786e\u9884\u6d4b\u6a21\u578b\u5728\u53cd\u4e8b\u5b9e\u8f93\u5165\u4e0b\u8f93\u51fa\u7684\u601d\u7ef4\u94fe\uff0c\u6765\u63d0\u9ad8\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u3002", "motivation": "\u601d\u7ef4\u94fe\u63a8\u7406\u662f\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u539f\u56e0\u7684\u6700\u5e38\u7528\u65b9\u6cd5\u4e4b\u4e00\uff0c\u4f46\u601d\u7ef4\u94fe\u7684\u5fe0\u5b9e\u6027\u95ee\u9898\u4e25\u91cd\u9650\u5236\u4e86\u4ece\u8fd9\u79cd\u5b9e\u8df5\u4e2d\u83b7\u5f97\u7684\u6d1e\u5bdf\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u4f9d\u8d56\u865a\u5047\u7279\u5f81\u3001\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u6216\u5949\u627f\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86Counterfactual Simulation Training (CST)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5956\u52b1\u90a3\u4e9b\u80fd\u591f\u4f7f\u6a21\u62df\u5668\u51c6\u786e\u9884\u6d4b\u6a21\u578b\u5728\u53cd\u4e8b\u5b9e\u8f93\u5165\u4e0b\u8f93\u51fa\u7684\u601d\u7ef4\u94fe\u6765\u63d0\u9ad8\u5fe0\u5b9e\u6027\u3002\u5e94\u7528\u5728\u4e24\u4e2a\u573a\u666f\uff1a1) \u57fa\u4e8e\u7ebf\u7d22\u7684\u53cd\u4e8b\u5b9e\u601d\u7ef4\u94fe\u76d1\u63a7\uff0c\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u4f9d\u8d56\u865a\u5047\u7279\u5f81\uff1b2) \u57fa\u4e8e\u901a\u7528\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u6a21\u62df\uff0c\u9f13\u52b1\u6a21\u578b\u4ea7\u751f\u66f4\u5fe0\u5b9e\u3001\u53ef\u6cdb\u5316\u7684\u63a8\u7406\u3002", "result": "\u5728\u53c2\u6570\u9ad8\u8fbe235B\u7684\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cCST\u80fd\u663e\u8457\u63d0\u9ad8\u57fa\u4e8e\u7ebf\u7d22\u53cd\u4e8b\u5b9e\u7684\u76d1\u63a7\u51c6\u786e\u7387\uff08\u63d0\u534735\u4e2a\u767e\u5206\u70b9\uff09\u548c\u901a\u7528\u53cd\u4e8b\u5b9e\u7684\u53ef\u6a21\u62df\u6027\uff08\u63d0\u53472\u4e2a\u767e\u5206\u70b9\uff09\u3002CST\u4f18\u4e8e\u63d0\u793a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7528LLM\u91cd\u5199\u4e0d\u5fe0\u5b9e\u601d\u7ef4\u94fe\u7684\u6548\u7387\u6bd4\u5355\u72ec\u4f7f\u7528RL\u9ad85\u500d\uff0c\u4f46\u5fe0\u5b9e\u6027\u6539\u8fdb\u4e0d\u9002\u7528\u4e8e\u529d\u963b\u7ebf\u7d22\u3002", "conclusion": "CST\u80fd\u591f\u666e\u904d\u63d0\u9ad8\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\uff0c\u5728\u601d\u7ef4\u94fe\u76d1\u63a7\u65b9\u9762\u5177\u6709\u6709\u524d\u666f\u7684\u5e94\u7528\u4ef7\u503c\u3002\u66f4\u5927\u7684\u6a21\u578b\u867d\u7136\u4e0d\u4f1a\u81ea\u52a8\u4ea7\u751f\u66f4\u5fe0\u5b9e\u7684\u601d\u7ef4\u94fe\uff0c\u4f46\u80fd\u4eceCST\u4e2d\u83b7\u76ca\u66f4\u591a\u3002"}}
{"id": "2602.20722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20722", "abs": "https://arxiv.org/abs/2602.20722", "authors": ["Xu Wan", "Yansheng Wang", "Wenqi Huang", "Mingyang Sun"], "title": "Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning", "comment": null, "summary": "Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.", "AI": {"tldr": "BAPO\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u7684\u79bb\u7b56\u7565RLVR\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u8bad\u7ec3\u6279\u6b21\u3001\u91cd\u65b0\u8bc4\u4f30\u5386\u53f2\u56f0\u96be\u6837\u672c\u548c\u91cd\u7528\u9ad8\u8d28\u91cf\u6837\u672c\u6765\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff0c\u76f8\u6bd4GRPO\u5e73\u5747\u63d0\u534712.5%\uff0c\u89e3\u51b3\u4e8640.7%\u57fa\u7840\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7b56\u7565\u7684RLVR\u6846\u67b6\u5b58\u5728\u7ecf\u9a8c\u6d6a\u8d39\u548c\u5956\u52b1\u540c\u8d28\u5316\u95ee\u9898\uff0c\u8fd9\u76f4\u63a5\u963b\u788d\u4e86\u5728\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u5bf9\u56f0\u96be\u6837\u672c\u7684\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51faBatch Adaptation Policy Optimization (BAPO)\uff0c\u8fd9\u662f\u4e00\u79cd\u79bb\u7b56\u7565RLVR\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u8bad\u7ec3\u6279\u6b21\u3001\u91cd\u65b0\u8bc4\u4f30\u5386\u53f2\u56f0\u96be\u6837\u672c\u3001\u91cd\u7528\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u7b56\u7565\u6539\u8fdb\u7684\u4e0b\u754c\u4fdd\u8bc1\u3002", "result": "\u5728\u6570\u5b66\u3001\u89c4\u5212\u548c\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cBAPO\u76f8\u6bd4GRPO\u5e73\u5747\u63d0\u534712.5%\uff0c\u6210\u529f\u89e3\u51b3\u4e8640.7%\u57fa\u7840\u6a21\u578b\u6301\u7eed\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u3002", "conclusion": "BAPO\u901a\u8fc7\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u89e3\u51b3\u56f0\u96be\u6837\u672c\u95ee\u9898\uff0c\u663e\u8457\u6539\u5584\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u7684\u6548\u679c\uff0c\u4e3aRLVR\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20732", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20732", "abs": "https://arxiv.org/abs/2602.20732", "authors": ["Chao Fei", "Guozhong Li", "Chenxi Liu", "Panos Kalnis"], "title": "CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference", "comment": null, "summary": "Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \\textbf{CHESS}, an \\textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \\textbf{1\\%} of the KV cache, delivers low-latency stable inference with up to \\textbf{4.56$\\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \\href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.", "AI": {"tldr": "CHESS\u662f\u4e00\u4e2a\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u7684KV\u7f13\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5206\u5c42\u9009\u62e9\u7b56\u7565\uff0c\u4ec5\u4f7f\u75281%\u7684KV\u7f13\u5b58\u5c31\u80fd\u8d85\u8d8aFull-KV\u7684\u8d28\u91cf\uff0c\u5b9e\u73b0\u9ad8\u8fbe4.56\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587LLM\u9700\u8981\u4f4e\u5ef6\u8fdf\u7684\u51c6\u786e\u63a8\u7406\uff0c\u4f46\u968f\u7740\u4e0a\u4e0b\u6587\u589e\u957f\uff0c\u89e3\u7801\u4e3b\u8981\u53d7KV\u7f13\u5b58\u7ea6\u675f\u3002\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u5927\u591a\u662f\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\uff1a\u5176token\u9009\u62e9\u5ffd\u7565\u4e86\u9010\u6b65\u76f8\u5173\u6027\u548c\u5c40\u90e8\u8bed\u4e49\uff0c\u635f\u5bb3\u4e86\u8d28\u91cf\uff0c\u4e14\u4e0d\u89c4\u5219\u8bbf\u95ee\u548c\u9009\u62e9\u5f00\u9500\u4ec5\u5e26\u6765\u6709\u9650\u7684\u52a0\u901f\u6548\u679c\u3002", "method": "CHESS\u91c7\u7528\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\uff1a\u7b97\u6cd5\u4e0a\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5206\u5c42\u9009\u62e9\u7b56\u7565\uff0c\u52a8\u6001\u91cd\u5efa\u5f53\u524d\u89e3\u7801\u7684\u8fde\u8d2f\u4e0a\u4e0b\u6587\uff1b\u7cfb\u7edf\u4e0a\u901a\u8fc7\u7c97\u7c92\u5ea6\u9009\u62e9\u6d88\u9664\u6602\u8d35\u7684\u6570\u636e\u79fb\u52a8\uff0c\u5145\u5206\u5b9e\u73b0\u7406\u8bba\u7a00\u758f\u6027\u7684\u5b9e\u9645\u52a0\u901f\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cCHESS\u4ec5\u4f7f\u75281%\u7684KV\u7f13\u5b58\u5c31\u80fd\u8d85\u8d8aFull-KV\u7684\u8d28\u91cf\uff0c\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u7a33\u5b9a\u63a8\u7406\uff0c\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe4.56\u500d\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CHESS\u901a\u8fc7\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u4e2dKV\u7f13\u5b58\u7ba1\u7406\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.20810", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20810", "abs": "https://arxiv.org/abs/2602.20810", "authors": ["Yaacov Pariente", "Vadim Indelman"], "title": "POMDPPlanners: Open-Source Package for POMDP Planning", "comment": null, "summary": "We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.", "AI": {"tldr": "POMDPPlanners\u662f\u4e00\u4e2a\u5f00\u6e90\u7684Python\u5305\uff0c\u7528\u4e8ePOMDP\u89c4\u5212\u7b97\u6cd5\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u96c6\u6210\u4e86\u5148\u8fdb\u7b97\u6cd5\u3001\u57fa\u51c6\u73af\u5883\u3001\u81ea\u52a8\u5316\u8d85\u53c2\u6570\u4f18\u5316\u548c\u5e76\u884c\u4eff\u771f\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u7684POMDP\u5de5\u5177\u5305\u5728\u98ce\u9669\u654f\u611f\u8bbe\u7f6e\u4e0b\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u8fdb\u884c\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u7814\u7a76\u3002\u7814\u7a76\u4eba\u5458\u9700\u8981\u51cf\u5c11\u5927\u89c4\u6a21\u4eff\u771f\u7814\u7a76\u7684\u5f00\u9500\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u6700\u5148\u8fdb\u89c4\u5212\u7b97\u6cd5\u7684Python\u5305\uff0c\u5305\u542b\u57fa\u51c6\u73af\u5883\u5957\u4ef6\uff08\u542b\u5b89\u5168\u5173\u952e\u53d8\u4f53\uff09\u3001\u901a\u8fc7Optuna\u7684\u81ea\u52a8\u5316\u8d85\u53c2\u6570\u4f18\u5316\u3001\u5e26\u6545\u969c\u6062\u590d\u7684\u6301\u4e45\u7f13\u5b58\u4ee5\u53ca\u53ef\u914d\u7f6e\u7684\u5e76\u884c\u4eff\u771f\u3002", "result": "\u521b\u5efa\u4e86POMDPPlanners\u5f00\u6e90\u5305\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5e7f\u6cdb\u4eff\u771f\u7814\u7a76\u7684\u5f00\u9500\uff0c\u652f\u6301\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u98ce\u9669\u654f\u611f\u8bbe\u7f6e\u4e0b\u3002", "conclusion": "POMDPPlanners\u4e3a\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5de5\u5177\u5305\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6807\u51c6\u5de5\u5177\u5305\u65e0\u6cd5\u6ee1\u8db3\u7684\u98ce\u9669\u654f\u611f\u573a\u666f\uff0c\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2602.20934", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20934", "abs": "https://arxiv.org/abs/2602.20934", "authors": ["ChengYou Li", "XiaoDong Liu", "XiangBao Meng", "XinYu Zhao"], "title": "Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence", "comment": "16 pages,9 figures", "summary": "The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a \"Reasoning Kernel\" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAgentOS\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53d7\u7ed3\u6784\u5316\u64cd\u4f5c\u7cfb\u7edf\u903b\u8f91\u7ba1\u7406\u7684\"\u63a8\u7406\u5185\u6838\"\uff0c\u901a\u8fc7\u6df1\u5ea6\u4e0a\u4e0b\u6587\u7ba1\u7406\u5c06\u4e0a\u4e0b\u6587\u7a97\u53e3\u6982\u5ff5\u5316\u4e3a\u53ef\u5bfb\u5740\u8bed\u4e49\u7a7a\u95f4\uff0c\u65e8\u5728\u89e3\u51b3\u5fae\u89c2\u6807\u8bb0\u5904\u7406\u4e0e\u5b8f\u89c2\u7cfb\u7edf\u667a\u80fd\u4e4b\u95f4\u7684\u7406\u8bba\u9e3f\u6c9f\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u6216\u4f18\u5316\u63d0\u793a\u5de5\u7a0b\uff0c\u4f46\u5fae\u89c2\u6807\u8bb0\u5904\u7406\u4e0e\u5b8f\u89c2\u7cfb\u7edf\u667a\u80fd\u4e4b\u95f4\u7684\u7406\u8bba\u6865\u6881\u4ecd\u7136\u788e\u7247\u5316\u3002\u9700\u8981\u4ece\u9759\u6001\u63a8\u7406\u5f15\u64ce\u5411\u52a8\u6001\u81ea\u4e3b\u8ba4\u77e5\u7cfb\u7edf\u8fdb\u884c\u6839\u672c\u6027\u8f6c\u53d8\u3002", "method": "\u63d0\u51faAgentOS\u6982\u5ff5\u6846\u67b6\uff0c\u5c06LLM\u91cd\u65b0\u5b9a\u4e49\u4e3a\"\u63a8\u7406\u5185\u6838\"\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u64cd\u4f5c\u7cfb\u7edf\u903b\u8f91\u3002\u6838\u5fc3\u662f\u6df1\u5ea6\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u5c06\u4e0a\u4e0b\u6587\u7a97\u53e3\u6982\u5ff5\u5316\u4e3a\u53ef\u5bfb\u5740\u8bed\u4e49\u7a7a\u95f4\u800c\u975e\u88ab\u52a8\u7f13\u51b2\u533a\u3002\u5f15\u5165\u8bed\u4e49\u5207\u7247\u548c\u65f6\u95f4\u5bf9\u9f50\u673a\u5236\u6765\u51cf\u8f7b\u591a\u667a\u80fd\u4f53\u7f16\u6392\u4e2d\u7684\u8ba4\u77e5\u6f02\u79fb\uff0c\u5e76\u5c06\u7ecf\u5178\u64cd\u4f5c\u7cfb\u7edf\u62bd\u8c61\uff08\u5982\u5185\u5b58\u5206\u9875\u3001\u4e2d\u65ad\u5904\u7406\u548c\u8fdb\u7a0b\u8c03\u5ea6\uff09\u6620\u5c04\u5230LLM\u539f\u751f\u6784\u9020\u4e0a\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u8c28\u7684\u8def\u7ebf\u56fe\uff0c\u7528\u4e8e\u6784\u5efa\u5f39\u6027\u3001\u53ef\u6269\u5c55\u548c\u81ea\u6211\u8fdb\u5316\u7684\u8ba4\u77e5\u73af\u5883\u3002\u5206\u6790\u8868\u660e\uff0cAGI\u53d1\u5c55\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf\u5728\u4e8e\u7cfb\u7edf\u7ea7\u534f\u8c03\u7684\u67b6\u6784\u6548\u7387\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u8303\u5f0f\u6b63\u5728\u4ece\u9759\u6001\u63a8\u7406\u5f15\u64ce\u5411\u52a8\u6001\u81ea\u4e3b\u8ba4\u77e5\u7cfb\u7edf\u8fdb\u884c\u6839\u672c\u6027\u8f6c\u53d8\u3002\u901a\u8fc7\u5c06\u64cd\u4f5c\u7cfb\u7edf\u539f\u7406\u5e94\u7528\u4e8eLLM\u67b6\u6784\uff0c\u53ef\u4ee5\u521b\u5efa\u66f4\u5f3a\u5927\u3001\u534f\u8c03\u548c\u81ea\u9002\u5e94\u7684\u8ba4\u77e5\u7cfb\u7edf\uff0c\u7cfb\u7edf\u7ea7\u534f\u8c03\u7684\u67b6\u6784\u6548\u7387\u662fAGI\u53d1\u5c55\u7684\u5173\u952e\u524d\u6cbf\u3002"}}

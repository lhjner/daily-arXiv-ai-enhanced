{"id": "2602.23367", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23367", "abs": "https://arxiv.org/abs/2602.23367", "authors": ["Shubh Laddha", "Lucas Changbencharoen", "Win Kuptivej", "Surya Shringla", "Archana Vaidheeswaran", "Yash Bhaskar"], "title": "HumanMCP: A Human-Like Query Dataset for Evaluating MCP Tool Retrieval Performance", "comment": "4 pages, 2 figures, 3 tables", "summary": "Model Context Protocol (MCP) servers contain a collection of thousands of open-source standardized tools, linking LLMs to external systems; however, existing datasets and benchmarks lack realistic, human-like user queries, remaining a critical gap in evaluating the tool usage and ecosystems of MCP servers. Existing datasets often do contain tool descriptions but fail to represent how different users portray their requests, leading to poor generalization and inflated reliability of certain benchmarks. This paper introduces the first large-scale MCP dataset featuring diverse, high-quality diverse user queries generated specifically to match 2800 tools across 308 MCP servers, developing on the MCP Zero dataset. Each tool is paired with multiple unique user personas that we have generated, to capture varying levels of user intent ranging from precise task requests, and ambiguous, exploratory commands, reflecting the complexity of real-world interaction patterns.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u5927\u89c4\u6a21MCP\u6570\u636e\u96c6\uff0c\u5305\u542b\u9488\u5bf9308\u4e2aMCP\u670d\u52a1\u5668\u4e2d2800\u4e2a\u5de5\u5177\u7684\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u7528\u6237\u67e5\u8be2\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u6a21\u5f0f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MCP\u670d\u52a1\u5668\u5de5\u5177\u4f7f\u7528\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u771f\u5b9e\u3001\u4eba\u6027\u5316\u7684\u7528\u6237\u67e5\u8be2\uff0c\u65e0\u6cd5\u53cd\u6620\u4e0d\u540c\u7528\u6237\u5982\u4f55\u8868\u8fbe\u8bf7\u6c42\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u57fa\u51c6\u6d4b\u8bd5\u53ef\u9760\u6027\u88ab\u5938\u5927\u3002", "method": "\u57fa\u4e8eMCP Zero\u6570\u636e\u96c6\uff0c\u4e3a308\u4e2aMCP\u670d\u52a1\u5668\u4e2d\u76842800\u4e2a\u5de5\u5177\u751f\u6210\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u7528\u6237\u67e5\u8be2\uff0c\u6bcf\u4e2a\u5de5\u5177\u914d\u5bf9\u591a\u4e2a\u72ec\u7279\u7684\u7528\u6237\u89d2\u8272\uff0c\u6db5\u76d6\u4ece\u7cbe\u786e\u4efb\u52a1\u8bf7\u6c42\u5230\u6a21\u7cca\u63a2\u7d22\u6027\u547d\u4ee4\u7684\u4e0d\u540c\u7528\u6237\u610f\u56fe\u5c42\u6b21\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21MCP\u6570\u636e\u96c6\uff0c\u5305\u542b\u9488\u5bf92800\u4e2a\u5de5\u5177\u7684\u591a\u6837\u5316\u7528\u6237\u67e5\u8be2\uff0c\u53cd\u6620\u4e86\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u6a21\u5f0f\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86MCP\u670d\u52a1\u5668\u5de5\u5177\u4f7f\u7528\u8bc4\u4f30\u7684\u5173\u952e\u7a7a\u767d\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u548c\u751f\u6001\u7cfb\u7edf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u57fa\u51c6\u3002"}}
{"id": "2602.23541", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23541", "abs": "https://arxiv.org/abs/2602.23541", "authors": ["Arvind Raghavan", "Elias Bareinboim"], "title": "Causal Identification from Counterfactual Data: Completeness and Bounding Results", "comment": null, "summary": "Previous work establishing completeness results for $\\textit{counterfactual identification}$ has been circumscribed to the setting where the input data belongs to observational or interventional distributions (Layers 1 and 2 of Pearl's Causal Hierarchy), since it was generally presumed impossible to obtain data from counterfactual distributions, which belong to Layer 3. However, recent work (Raghavan & Bareinboim, 2025) has formally characterized a family of counterfactual distributions which can be directly estimated via experimental methods - a notion they call $\\textit{counterfactual realizabilty}$. This leaves open the question of what $\\textit{additional}$ counterfactual quantities now become identifiable, given this new access to (some) Layer 3 data. To answer this question, we develop the CTFIDU+ algorithm for identifying counterfactual queries from an arbitrary set of Layer 3 distributions, and prove that it is complete for this task. Building on this, we establish the theoretical limit of which counterfactuals can be identified from physically realizable distributions, thus implying the $\\textit{fundamental limit to exact causal inference in the non-parametric setting}$. Finally, given the impossibility of identifying certain critical types of counterfactuals, we derive novel analytic bounds for such quantities using realizable counterfactual data, and corroborate using simulations that counterfactual data helps tighten the bounds for non-identifiable quantities in practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CTFIDU+\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u53ef\u5b9e\u73b0\u7684\u7b2c3\u5c42\u53cd\u4e8b\u5b9e\u5206\u5e03\u4e2d\u8bc6\u522b\u53cd\u4e8b\u5b9e\u67e5\u8be2\uff0c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u7684\u5b8c\u5907\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u975e\u53c2\u6570\u56e0\u679c\u63a8\u7406\u7684\u6781\u9650\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u53cd\u4e8b\u5b9e\u8bc6\u522b\u5b8c\u5907\u6027\u7684\u7814\u7a76\u5c40\u9650\u4e8e\u89c2\u6d4b\u6216\u5e72\u9884\u5206\u5e03\uff08\u56e0\u679c\u5c42\u7ea7\u7684\u7b2c1\u30012\u5c42\uff09\uff0c\u56e0\u4e3a\u4e00\u822c\u8ba4\u4e3a\u65e0\u6cd5\u83b7\u5f97\u7b2c3\u5c42\u7684\u53cd\u4e8b\u5b9e\u5206\u5e03\u6570\u636e\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u53cd\u4e8b\u5b9e\u5206\u5e03\u53ef\u4ee5\u901a\u8fc7\u5b9e\u9a8c\u65b9\u6cd5\u76f4\u63a5\u4f30\u8ba1\uff08\u79f0\u4e3a\u53cd\u4e8b\u5b9e\u53ef\u5b9e\u73b0\u6027\uff09\uff0c\u8fd9\u5f15\u51fa\u4e86\u65b0\u7684\u95ee\u9898\uff1a\u5728\u83b7\u5f97\u90e8\u5206\u7b2c3\u5c42\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u54ea\u4e9b\u989d\u5916\u7684\u53cd\u4e8b\u5b9e\u91cf\u53d8\u5f97\u53ef\u8bc6\u522b\uff1f", "method": "\u5f00\u53d1\u4e86CTFIDU+\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u4efb\u610f\u7b2c3\u5c42\u5206\u5e03\u96c6\u5408\u4e2d\u8bc6\u522b\u53cd\u4e8b\u5b9e\u67e5\u8be2\u3002\u8be5\u7b97\u6cd5\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u53ef\u5b9e\u73b0\u6027\u7684\u6982\u5ff5\uff0c\u80fd\u591f\u5904\u7406\u901a\u8fc7\u5b9e\u9a8c\u65b9\u6cd5\u76f4\u63a5\u4f30\u8ba1\u7684\u53cd\u4e8b\u5b9e\u5206\u5e03\u6570\u636e\u3002", "result": "\u8bc1\u660e\u4e86CTFIDU+\u7b97\u6cd5\u5bf9\u4e8e\u4ece\u53ef\u5b9e\u73b0\u7684\u53cd\u4e8b\u5b9e\u5206\u5e03\u4e2d\u8bc6\u522b\u53cd\u4e8b\u5b9e\u67e5\u8be2\u662f\u5b8c\u5907\u7684\u3002\u5efa\u7acb\u4e86\u4ece\u7269\u7406\u53ef\u5b9e\u73b0\u5206\u5e03\u4e2d\u8bc6\u522b\u53cd\u4e8b\u5b9e\u7684\u7406\u8bba\u6781\u9650\uff0c\u8fd9\u6697\u793a\u4e86\u975e\u53c2\u6570\u8bbe\u7f6e\u4e2d\u7cbe\u786e\u56e0\u679c\u63a8\u7406\u7684\u57fa\u672c\u6781\u9650\u3002\u5bf9\u4e8e\u67d0\u4e9b\u4e0d\u53ef\u8bc6\u522b\u7684\u5173\u952e\u53cd\u4e8b\u5b9e\u7c7b\u578b\uff0c\u63a8\u5bfc\u4e86\u4f7f\u7528\u53ef\u5b9e\u73b0\u53cd\u4e8b\u5b9e\u6570\u636e\u7684\u65b0\u89e3\u6790\u8fb9\u754c\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7CTFIDU+\u7b97\u6cd5\u548c\u7406\u8bba\u5206\u6790\uff0c\u660e\u786e\u4e86\u5728\u53ef\u83b7\u5f97\u90e8\u5206\u53cd\u4e8b\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u56e0\u679c\u63a8\u7406\u7684\u80fd\u529b\u8fb9\u754c\u3002\u5bf9\u4e8e\u4e0d\u53ef\u8bc6\u522b\u7684\u53cd\u4e8b\u5b9e\u91cf\uff0c\u53cd\u4e8b\u5b9e\u6570\u636e\u5728\u5b9e\u8df5\u4e2d\u6709\u52a9\u4e8e\u6536\u7d27\u8fb9\u754c\uff0c\u8fd9\u901a\u8fc7\u4eff\u771f\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002\u7814\u7a76\u63ed\u793a\u4e86\u975e\u53c2\u6570\u56e0\u679c\u63a8\u7406\u7684\u57fa\u672c\u6781\u9650\u3002"}}
{"id": "2602.23545", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23545", "abs": "https://arxiv.org/abs/2602.23545", "authors": ["Matteo Ceriscioli", "Karthika Mohan"], "title": "Planning under Distribution Shifts with Causal POMDPs", "comment": "To appear at the 36th International Conference on Automated Planning and Scheduling (ICAPS-26)", "summary": "In the real world, planning is often challenged by distribution shifts. As such, a model of the environment obtained under one set of conditions may no longer remain valid as the distribution of states or the environment dynamics change, which in turn causes previously learned strategies to fail. In this work, we propose a theoretical framework for planning under partial observability using Partially Observable Markov Decision Processes (POMDPs) formulated using causal knowledge. By representing shifts in the environment as interventions on this causal POMDP, the framework enables evaluating plans under hypothesized changes and actively identifying which components of the environment have been altered. We show how to maintain and update a belief over both the latent state and the underlying domain, and we prove that the value function remains piecewise linear and convex (PWLC) in this augmented belief space. Preservation of PWLC under distribution shifts has the advantage of maintaining the tractability of planning via $\u03b1$-vector-based POMDP methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u56e0\u679c\u77e5\u8bc6\u7684POMDP\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u73af\u5883\u53d8\u5316\u8868\u793a\u4e3a\u5bf9\u56e0\u679cPOMDP\u7684\u5e72\u9884\uff0c\u4fdd\u6301\u503c\u51fd\u6570\u7684PWLC\u6027\u8d28\u4ee5\u786e\u4fdd\u89c4\u5212\u53ef\u5904\u7406\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u89c4\u5212\u7ecf\u5e38\u9762\u4e34\u5206\u5e03\u504f\u79fb\u7684\u6311\u6218\uff0c\u5728\u4e00\u4e2a\u6761\u4ef6\u4e0b\u83b7\u5f97\u7684\u73af\u5883\u6a21\u578b\u5728\u72b6\u6001\u5206\u5e03\u6216\u73af\u5883\u52a8\u6001\u53d8\u5316\u65f6\u53ef\u80fd\u5931\u6548\uff0c\u5bfc\u81f4\u5148\u524d\u5b66\u4e60\u7684\u7b56\u7565\u5931\u8d25\u3002\u9700\u8981\u5904\u7406\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u73af\u5883\u53d8\u5316\u540c\u65f6\u5b58\u5728\u7684\u590d\u6742\u60c5\u51b5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u56e0\u679c\u77e5\u8bc6\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u73af\u5883\u53d8\u5316\u8868\u793a\u4e3a\u5bf9\u56e0\u679cPOMDP\u7684\u5e72\u9884\u3002\u7ef4\u62a4\u548c\u66f4\u65b0\u5173\u4e8e\u6f5c\u5728\u72b6\u6001\u548c\u5e95\u5c42\u9886\u57df\u7684\u4fe1\u5ff5\uff0c\u8bc1\u660e\u5728\u589e\u5f3a\u7684\u4fe1\u5ff5\u7a7a\u95f4\u4e2d\u503c\u51fd\u6570\u4fdd\u6301\u5206\u6bb5\u7ebf\u6027\u51f8\uff08PWLC\uff09\u6027\u8d28\u3002", "result": "\u8bc1\u660e\u4e86\u503c\u51fd\u6570\u5728\u589e\u5f3a\u7684\u4fe1\u5ff5\u7a7a\u95f4\u4e2d\u4fdd\u6301\u5206\u6bb5\u7ebf\u6027\u51f8\uff08PWLC\uff09\u6027\u8d28\uff0c\u8fd9\u4e00\u6027\u8d28\u4fdd\u7559\u4e86\u57fa\u4e8e\u03b1\u5411\u91cf\u7684POMDP\u65b9\u6cd5\u7684\u53ef\u5904\u7406\u6027\uff0c\u4f7f\u5f97\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4ecd\u7136\u53ef\u4ee5\u8fdb\u884c\u6709\u6548\u7684\u89c4\u5212\u3002", "conclusion": "\u63d0\u51fa\u7684\u56e0\u679cPOMDP\u6846\u67b6\u4e3a\u5904\u7406\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u4fdd\u6301\u503c\u51fd\u6570\u7684PWLC\u6027\u8d28\u786e\u4fdd\u4e86\u89c4\u5212\u7684\u53ef\u5904\u7406\u6027\uff0c\u80fd\u591f\u8bc4\u4f30\u5047\u8bbe\u53d8\u5316\u4e0b\u7684\u8ba1\u5212\u5e76\u4e3b\u52a8\u8bc6\u522b\u73af\u5883\u4e2d\u7684\u53d8\u5316\u7ec4\u4ef6\u3002"}}
{"id": "2602.23579", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23579", "abs": "https://arxiv.org/abs/2602.23579", "authors": ["Guillem Rodr\u00edguez-Corominas", "Maria J. Blesa", "Christian Blum"], "title": "Construct, Merge, Solve & Adapt with Reinforcement Learning for the min-max Multiple Traveling Salesman Problem", "comment": null, "summary": "The Multiple Traveling Salesman Problem (mTSP) extends the Traveling Salesman Problem to m tours that start and end at a common depot and jointly visit all customers exactly once. In the min-max variant, the objective is to minimize the longest tour, reflecting workload balance. We propose a hybrid approach, Construct, Merge, Solve & Adapt with Reinforcement Learning (RL-CMSA), for the symmetric single-depot min-max mTSP. The method iteratively constructs diverse solutions using probabilistic clustering guided by learned pairwise q-values, merges routes into a compact pool, solves a restricted set-covering MILP, and refines solutions via inter-route remove, shift, and swap moves. The q-values are updated by reinforcing city-pair co-occurrences in high-quality solutions, while the pool is adapted through ageing and pruning. This combination of exact optimization and reinforcement-guided construction balances exploration and exploitation. Computational results on random and TSPLIB instances show that RL-CMSA consistently finds (near-)best solutions and outperforms a state-of-the-art hybrid genetic algorithm under comparable time limits, especially as instance size and the number of salesmen increase.", "AI": {"tldr": "\u63d0\u51faRL-CMSA\u6df7\u5408\u65b9\u6cd5\u89e3\u51b3\u5bf9\u79f0\u5355\u4ed3\u5e93\u6700\u5c0f\u6700\u5927mTSP\u95ee\u9898\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u6784\u9020\u3001\u7cbe\u786e\u4f18\u5316\u548c\u81ea\u9002\u5e94\u6c60\u7ba1\u7406\uff0c\u5728\u5e73\u8861\u5de5\u4f5c\u8d1f\u8f7d\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u65c5\u884c\u5546\u95ee\u9898\u4e2d\u7684\u6700\u5c0f\u6700\u5927\u53d8\u4f53\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u6700\u957f\u8def\u5f84\u4ee5\u5b9e\u73b0\u5de5\u4f5c\u8d1f\u8f7d\u5e73\u8861\uff0c\u9700\u8981\u6709\u6548\u65b9\u6cd5\u5904\u7406\u5927\u89c4\u6a21\u5b9e\u4f8b\u548c\u591a\u4e2a\u9500\u552e\u5458\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51faRL-CMSA\u6df7\u5408\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u57fa\u4e8e\u5b66\u4e60\u5230\u7684\u6210\u5bf9q\u503c\u8fdb\u884c\u6982\u7387\u805a\u7c7b\u6784\u9020\u591a\u6837\u5316\u89e3\uff1b2) \u5408\u5e76\u8def\u5f84\u5230\u7d27\u51d1\u6c60\uff1b3) \u6c42\u89e3\u53d7\u9650\u96c6\u5408\u8986\u76d6MILP\uff1b4) \u901a\u8fc7\u8de8\u8def\u5f84\u79fb\u9664\u3001\u8f6c\u79fb\u548c\u4ea4\u6362\u64cd\u4f5c\u7cbe\u5316\u89e3\uff1b5) \u901a\u8fc7\u5f3a\u5316\u9ad8\u8d28\u91cf\u89e3\u4e2d\u7684\u57ce\u5e02\u5bf9\u5171\u73b0\u66f4\u65b0q\u503c\uff1b6) \u901a\u8fc7\u8001\u5316\u548c\u526a\u679d\u81ea\u9002\u5e94\u8c03\u6574\u6c60\u3002", "result": "\u5728\u968f\u673a\u548cTSPLIB\u5b9e\u4f8b\u4e0a\u7684\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\uff0cRL-CMSA\u80fd\u4e00\u81f4\u627e\u5230\uff08\u63a5\u8fd1\uff09\u6700\u4f18\u89e3\uff0c\u5728\u53ef\u6bd4\u65f6\u95f4\u9650\u5236\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6df7\u5408\u9057\u4f20\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5b9e\u4f8b\u89c4\u6a21\u548c\u9500\u552e\u5458\u6570\u91cf\u589e\u52a0\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "RL-CMSA\u7ed3\u5408\u7cbe\u786e\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u6784\u9020\uff0c\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u4e3a\u5bf9\u79f0\u5355\u4ed3\u5e93\u6700\u5c0f\u6700\u5927mTSP\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5904\u7406\u5927\u89c4\u6a21\u95ee\u9898\u548c\u591a\u9500\u552e\u5458\u573a\u666f\u65f6\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.23605", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23605", "abs": "https://arxiv.org/abs/2602.23605", "authors": ["Zongzhe Xu", "Zitao Shuai", "Eideen Mozaffari", "Ravi S. Aysola", "Rajesh Kumar", "Yuzhe Yang"], "title": "SleepLM: Natural-Language Intelligence for Human Sleep", "comment": null, "summary": "We present SleepLM, a family of sleep-language foundation models that enable human sleep alignment, interpretation, and interaction with natural language. Despite the critical role of sleep, learning-based sleep analysis systems operate in closed label spaces (e.g., predefined stages or events) and fail to describe, query, or generalize to novel sleep phenomena. SleepLM bridges natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology. To support this alignment, we introduce a multilevel sleep caption generation pipeline that enables the curation of the first large-scale sleep-text dataset, comprising over 100K hours of data from more than 10,000 individuals. Furthermore, we present a unified pretraining objective that combines contrastive alignment, caption generation, and signal reconstruction to better capture physiological fidelity and cross-modal interactions. Extensive experiments on real-world sleep understanding tasks verify that SleepLM outperforms state-of-the-art in zero-shot and few-shot learning, cross-modal retrieval, and sleep captioning. Importantly, SleepLM also exhibits intriguing capabilities including language-guided event localization, targeted insight generation, and zero-shot generalization to unseen tasks. All code and data will be open-sourced.", "AI": {"tldr": "SleepLM\u662f\u4e00\u4e2a\u7761\u7720-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548c\u591a\u6a21\u6001\u591a\u5bfc\u7761\u7720\u56fe\u7684\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u4eba\u7c7b\u7761\u7720\u7684\u89e3\u8bfb\u548c\u4ea4\u4e92\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u7761\u7720\u5206\u6790\u7cfb\u7edf\u5de5\u4f5c\u5728\u5c01\u95ed\u7684\u6807\u7b7e\u7a7a\u95f4\uff08\u5982\u9884\u5b9a\u4e49\u7684\u7761\u7720\u9636\u6bb5\u6216\u4e8b\u4ef6\uff09\uff0c\u65e0\u6cd5\u63cf\u8ff0\u3001\u67e5\u8be2\u6216\u6cdb\u5316\u5230\u65b0\u7684\u7761\u7720\u73b0\u8c61\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fde\u63a5\u81ea\u7136\u8bed\u8a00\u548c\u7761\u7720\u751f\u7406\u6570\u636e\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u7ea7\u7761\u7720\u63cf\u8ff0\u751f\u6210\u6d41\u7a0b\uff0c\u521b\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u7761\u7720-\u6587\u672c\u6570\u636e\u96c6\uff08\u8d85\u8fc710\u4e07\u5c0f\u65f6\u6570\u636e\uff0c\u6765\u81ea1\u4e07\u591a\u4e2a\u4e2a\u4f53\uff09\uff1b\u8bbe\u8ba1\u4e86\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5bf9\u9f50\u3001\u63cf\u8ff0\u751f\u6210\u548c\u4fe1\u53f7\u91cd\u5efa\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u751f\u7406\u4fdd\u771f\u5ea6\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u7761\u7720\u7406\u89e3\u4efb\u52a1\u4e2d\uff0cSleepLM\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u7761\u7720\u63cf\u8ff0\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff1b\u6a21\u578b\u8fd8\u5c55\u73b0\u51fa\u8bed\u8a00\u5f15\u5bfc\u7684\u4e8b\u4ef6\u5b9a\u4f4d\u3001\u9488\u5bf9\u6027\u6d1e\u5bdf\u751f\u6210\u548c\u5bf9\u672a\u89c1\u4efb\u52a1\u7684\u96f6\u6837\u672c\u6cdb\u5316\u7b49\u6709\u8da3\u80fd\u529b\u3002", "conclusion": "SleepLM\u901a\u8fc7\u8fde\u63a5\u81ea\u7136\u8bed\u8a00\u548c\u591a\u6a21\u6001\u591a\u5bfc\u7761\u7720\u56fe\uff0c\u5b9e\u73b0\u4e86\u7761\u7720\u751f\u7406\u7684\u8bed\u8a00\u57fa\u7840\u8868\u793a\uff0c\u4e3a\u7761\u7720\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5f00\u6e90\u3002"}}
{"id": "2602.23632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23632", "abs": "https://arxiv.org/abs/2602.23632", "authors": ["Lun Zhan", "Feng Xiong", "Huanyong Liu", "Feng Zhang", "Yuhui Yin"], "title": "MMKG-RDS: Reasoning Data Synthesis via Deep Mining of Multimodal Knowledge Graphs", "comment": null, "summary": "Synthesizing high-quality training data is crucial for enhancing domain models' reasoning abilities. Existing methods face limitations in long-tail knowledge coverage, effectiveness verification, and interpretability. Knowledge-graph-based approaches still fall short in functionality, granularity, customizability, and evaluation. To address these issues, we propose MMKG-RDS, a flexible framework for reasoning data synthesis that leverages multimodal knowledge graphs. It supports fine-grained knowledge extraction, customizable path sampling, and multidimensional data quality scoring. We validate MMKG-RDS with the MMKG-RDS-Bench dataset, covering five domains, 17 task types, and 14,950 samples. Experimental results show fine-tuning Qwen3 models (0.6B/8B/32B) on a small number of synthesized samples improves reasoning accuracy by 9.2%. The framework also generates distinct data, challenging existing models on tasks involving tables and formulas, useful for complex benchmark construction. The dataset and code are available at https://github.com/360AILAB-NLP/MMKG-RDS", "AI": {"tldr": "MMKG-RDS\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u7684\u63a8\u7406\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u63d0\u53d6\u3001\u53ef\u5b9a\u5236\u8def\u5f84\u91c7\u6837\u548c\u591a\u7ef4\u8d28\u91cf\u8bc4\u5206\uff0c\u63d0\u5347\u9886\u57df\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u5c3e\u77e5\u8bc6\u8986\u76d6\u3001\u6548\u679c\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\u5728\u529f\u80fd\u6027\u3001\u7c92\u5ea6\u3001\u53ef\u5b9a\u5236\u6027\u548c\u8bc4\u4f30\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3", "method": "\u63d0\u51faMMKG-RDS\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u652f\u6301\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u63d0\u53d6\u3001\u53ef\u5b9a\u5236\u8def\u5f84\u91c7\u6837\u548c\u591a\u7ef4\u6570\u636e\u8d28\u91cf\u8bc4\u5206", "result": "\u4f7f\u7528MMKG-RDS-Bench\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u6db5\u76d65\u4e2a\u9886\u57df\u300117\u79cd\u4efb\u52a1\u7c7b\u578b\u548c14,950\u4e2a\u6837\u672c\uff0c\u5fae\u8c03Qwen3\u6a21\u578b\uff080.6B/8B/32B\uff09\u540e\u63a8\u7406\u51c6\u786e\u7387\u63d0\u53479.2%", "conclusion": "MMKG-RDS\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e3a\u6d89\u53ca\u8868\u683c\u548c\u516c\u5f0f\u7684\u590d\u6742\u57fa\u51c6\u6d4b\u8bd5\u6784\u5efa\u63d0\u4f9b\u652f\u6301"}}
{"id": "2602.23643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23643", "abs": "https://arxiv.org/abs/2602.23643", "authors": ["Judah Goldfeder", "Philippe Wyder", "Yann LeCun", "Ravid Shwartz Ziv"], "title": "AI Must Embrace Specialization via Superhuman Adaptable Intelligence", "comment": null, "summary": "Everyone from AI executives and researchers to doomsayers, politicians, and activists is talking about Artificial General Intelligence (AGI). Yet, they often don't seem to agree on its exact definition. One common definition of AGI is an AI that can do everything a human can do, but are humans truly general? In this paper, we address what's wrong with our conception of AGI, and why, even in its most coherent formulation, it is a flawed concept to describe the future of AI. We explore whether the most widely accepted definitions are plausible, useful, and truly general. We argue that AI must embrace specialization, rather than strive for generality, and in its specialization strive for superhuman performance, and introduce Superhuman Adaptable Intelligence (SAI). SAI is defined as intelligence that can learn to exceed humans at anything important that we can do, and that can fill in the skill gaps where humans are incapable. We then lay out how SAI can help hone a discussion around AI that was blurred by an overloaded definition of AGI, and extrapolate the implications of using it as a guide for the future.", "AI": {"tldr": "\u8bba\u6587\u6279\u5224\u5f53\u524d\u5bf9\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u7684\u6d41\u884c\u5b9a\u4e49\uff0c\u8ba4\u4e3a\u4eba\u7c7b\u672c\u8eab\u5e76\u975e\"\u901a\u7528\"\uff0c\u63d0\u51fa\u5e94\u653e\u5f03\u8ffd\u6c42\u901a\u7528\u6027\uff0c\u8f6c\u800c\u4e13\u6ce8\u4e8e\u53d1\u5c55\"\u8d85\u4eba\u9002\u5e94\u667a\u80fd\"\uff08SAI\uff09\uff0c\u5373\u80fd\u5728\u91cd\u8981\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4eba\u7c7b\u5e76\u586b\u8865\u4eba\u7c7b\u80fd\u529b\u7a7a\u767d\u7684\u4e13\u4e1a\u5316AI\u3002", "motivation": "\u5f53\u524dAI\u9886\u57df\u5bf9AGI\u7684\u5b9a\u4e49\u6df7\u4e71\u4e14\u4e0d\u4e00\u81f4\uff0c\u8bb8\u591a\u8ba8\u8bba\u57fa\u4e8e\"AI\u80fd\u505a\u4eba\u7c7b\u80fd\u505a\u7684\u4e00\u5207\"\u8fd9\u4e00\u6709\u95ee\u9898\u7684\u5047\u8bbe\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u901a\u7528\u6027\u6982\u5ff5\u672c\u8eab\u5b58\u5728\u7f3a\u9677\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u5982\u4f55\u66f4\u597d\u5730\u63cf\u8ff0AI\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709AGI\u5b9a\u4e49\u7684\u903b\u8f91\u95ee\u9898\uff0c\u8bba\u8bc1\u4eba\u7c7b\u672c\u8eab\u5e76\u975e\u771f\u6b63\u901a\u7528\uff0c\u63d0\u51fa\u7528\"\u8d85\u4eba\u9002\u5e94\u667a\u80fd\"\uff08SAI\uff09\u8fd9\u4e00\u65b0\u6982\u5ff5\u66ff\u4ee3AGI\u3002SAI\u5f3a\u8c03\u4e13\u4e1a\u5316\u800c\u975e\u901a\u7528\u6027\uff0c\u8ffd\u6c42\u5728\u7279\u5b9a\u91cd\u8981\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4eba\u7c7b\u8868\u73b0\u5e76\u586b\u8865\u4eba\u7c7b\u80fd\u529b\u7a7a\u767d\u3002", "result": "\u6210\u529f\u8bc6\u522b\u4e86\u5f53\u524dAGI\u6982\u5ff5\u7684\u7f3a\u9677\uff0c\u63d0\u51fa\u4e86\u66f4\u6e05\u6670\u3001\u66f4\u6709\u7528\u7684SAI\u6846\u67b6\u3002\u8fd9\u4e00\u65b0\u6982\u5ff5\u80fd\u591f\u5e2e\u52a9\u6f84\u6e05\u56e0AGI\u5b9a\u4e49\u8fc7\u8f7d\u800c\u6a21\u7cca\u7684AI\u8ba8\u8bba\uff0c\u4e3aAI\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u66f4\u660e\u786e\u7684\u6307\u5bfc\u65b9\u5411\u3002", "conclusion": "AGI\u662f\u4e00\u4e2a\u6709\u7f3a\u9677\u7684\u6982\u5ff5\uff0c\u4e0d\u5e94\u4f5c\u4e3aAI\u53d1\u5c55\u7684\u76ee\u6807\u3002\u76f8\u53cd\uff0cAI\u5e94\u8be5\u8ffd\u6c42\u4e13\u4e1a\u5316\u800c\u975e\u901a\u7528\u6027\uff0c\u53d1\u5c55\u80fd\u591f\u5728\u91cd\u8981\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4eba\u7c7b\u5e76\u586b\u8865\u4eba\u7c7b\u80fd\u529b\u7a7a\u767d\u7684\u8d85\u4eba\u9002\u5e94\u667a\u80fd\uff08SAI\uff09\uff0c\u8fd9\u4e3aAI\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u3001\u66f4\u5b9e\u7528\u7684\u6846\u67b6\u3002"}}
{"id": "2602.23668", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.23668", "abs": "https://arxiv.org/abs/2602.23668", "authors": ["Yihan", "Wen", "Xin Chen"], "title": "PseudoAct: Leveraging Pseudocode Synthesis for Flexible Planning and Action Control in Large Language Model Agents", "comment": null, "summary": "Large language model (LLM) agents typically rely on reactive decision-making paradigms such as ReAct, selecting actions conditioned on growing execution histories. While effective for short tasks, these approaches often lead to redundant tool usage, unstable reasoning, and high token consumption in complex long-horizon tasks involving branching, iteration, or multi-tool coordination. To address these limitations, this paper introduces PseudoAct, a novel framework for flexible planning and action control in LLM agents through pseudocode synthesis. Leveraging the ability of LLMs to express task-solving strategies as code, PseudoAct synthesizes a structured pseudocode plan that decomposes a task into subtasks and explicitly encodes control flow, including sequencing, conditionals, loops, parallel composition, and combinations of these logic primitives. Actions are then executed by following this global plan, making the decision logic explicit and temporally coherent. This design reduces redundant actions, prevents infinite loops, and avoids uninformative alternative exploration, enabling consistent and efficient long-horizon decision-making. Experiments on benchmark datasets show that our method significantly outperforms existing reactive agent approaches, achieving a 20.93% absolute gain in success rate on FEVER and setting a new state-of-the-art on HotpotQA.", "AI": {"tldr": "PseudoAct\u6846\u67b6\u901a\u8fc7\u4f2a\u4ee3\u7801\u5408\u6210\u5b9e\u73b0LLM\u667a\u80fd\u4f53\u7684\u7075\u6d3b\u89c4\u5212\u548c\u884c\u52a8\u63a7\u5236\uff0c\u89e3\u51b3\u4f20\u7edf\u53cd\u5e94\u5f0f\u51b3\u7b56\u5728\u590d\u6742\u957f\u7a0b\u4efb\u52a1\u4e2d\u7684\u5197\u4f59\u5de5\u5177\u4f7f\u7528\u3001\u4e0d\u7a33\u5b9a\u63a8\u7406\u548c\u9ad8token\u6d88\u8017\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfLLM\u667a\u80fd\u4f53\u4f9d\u8d56\u53cd\u5e94\u5f0f\u51b3\u7b56\u8303\u5f0f\uff08\u5982ReAct\uff09\uff0c\u5728\u6d89\u53ca\u5206\u652f\u3001\u8fed\u4ee3\u6216\u591a\u5de5\u5177\u534f\u8c03\u7684\u590d\u6742\u957f\u7a0b\u4efb\u52a1\u4e2d\uff0c\u4f1a\u5bfc\u81f4\u5197\u4f59\u5de5\u5177\u4f7f\u7528\u3001\u4e0d\u7a33\u5b9a\u63a8\u7406\u548c\u9ad8token\u6d88\u8017\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89c4\u5212\u548c\u63a7\u5236\u65b9\u6cd5\u3002", "method": "PseudoAct\u901a\u8fc7\u4f2a\u4ee3\u7801\u5408\u6210\u6846\u67b6\uff0c\u5229\u7528LLM\u5c06\u4efb\u52a1\u89e3\u51b3\u7b56\u7565\u8868\u8fbe\u4e3a\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u5408\u6210\u7ed3\u6784\u5316\u4f2a\u4ee3\u7801\u8ba1\u5212\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e76\u663e\u5f0f\u7f16\u7801\u63a7\u5236\u6d41\uff08\u5305\u62ec\u987a\u5e8f\u3001\u6761\u4ef6\u3001\u5faa\u73af\u3001\u5e76\u884c\u7ec4\u5408\u7b49\u903b\u8f91\u539f\u8bed\uff09\uff0c\u7136\u540e\u6309\u7167\u5168\u5c40\u8ba1\u5212\u6267\u884c\u884c\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53cd\u5e94\u5f0f\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5728FEVER\u4e0a\u5b9e\u73b0\u4e8620.93%\u7684\u7edd\u5bf9\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u5728HotpotQA\u4e0a\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "PseudoAct\u901a\u8fc7\u4f2a\u4ee3\u7801\u5408\u6210\u5b9e\u73b0\u4e86\u663e\u5f0f\u548c\u65f6\u5e8f\u8fde\u8d2f\u7684\u51b3\u7b56\u903b\u8f91\uff0c\u51cf\u5c11\u4e86\u5197\u4f59\u884c\u52a8\uff0c\u9632\u6b62\u4e86\u65e0\u9650\u5faa\u73af\uff0c\u907f\u514d\u4e86\u65e0\u4fe1\u606f\u91cf\u7684\u66ff\u4ee3\u63a2\u7d22\uff0c\u5b9e\u73b0\u4e86\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u957f\u7a0b\u51b3\u7b56\u3002"}}
{"id": "2602.23701", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23701", "abs": "https://arxiv.org/abs/2602.23701", "authors": ["Yawen Wang", "Wenjie Wu", "Junjie Wang", "Qing Wang"], "title": "From Flat Logs to Causal Graphs: Hierarchical Failure Attribution for LLM-based Multi-Agent Systems", "comment": null, "summary": "LLM-powered Multi-Agent Systems (MAS) have demonstrated remarkable capabilities in complex domains but suffer from inherent fragility and opaque failure mechanisms. Existing failure attribution methods, whether relying on direct prompting, costly replays, or supervised fine-tuning, typically treat execution logs as flat sequences. This linear perspective fails to disentangle the intricate causal links inherent to MAS, leading to weak observability and ambiguous responsibility boundaries. To address these challenges, we propose CHIEF, a novel framework that transforms chaotic trajectories into a structured hierarchical causal graph. It then employs hierarchical oracle-guided backtracking to efficiently prune the search space via sybthesized virtual oracles. Finally, it implements counterfactual attribution via a progressive causal screening strategy to rigorously distinguish true root causes from propagated symptoms. Experiments on Who&When benchmark show that CHIEF outperforms eight strong and state-of-the-art baselines on both agent- and step-level accuracy. Ablation studies further confirm the critical role of each proposed module.", "AI": {"tldr": "CHIEF\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6df7\u4e71\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8f68\u8ff9\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u5c42\u6b21\u56e0\u679c\u56fe\uff0c\u4f7f\u7528\u5c42\u6b21\u5316oracle\u5f15\u5bfc\u56de\u6eaf\u548c\u6e10\u8fdb\u56e0\u679c\u7b5b\u9009\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6545\u969c\u5f52\u56e0\u7684\u51c6\u786e\u6027\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u56fa\u6709\u7684\u8106\u5f31\u6027\u548c\u4e0d\u900f\u660e\u7684\u6545\u969c\u673a\u5236\u3002\u73b0\u6709\u7684\u6545\u969c\u5f52\u56e0\u65b9\u6cd5\u901a\u5e38\u5c06\u6267\u884c\u65e5\u5fd7\u89c6\u4e3a\u6241\u5e73\u5e8f\u5217\uff0c\u8fd9\u79cd\u7ebf\u6027\u89c6\u89d2\u65e0\u6cd5\u89e3\u8026MAS\u4e2d\u590d\u6742\u7684\u56e0\u679c\u8054\u7cfb\uff0c\u5bfc\u81f4\u5f31\u53ef\u89c2\u6d4b\u6027\u548c\u6a21\u7cca\u7684\u8d23\u4efb\u8fb9\u754c\u3002", "method": "CHIEF\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u5c06\u6df7\u4e71\u8f68\u8ff9\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u5c42\u6b21\u56e0\u679c\u56fe\uff1b2) \u4f7f\u7528\u5c42\u6b21\u5316oracle\u5f15\u5bfc\u56de\u6eaf\uff0c\u901a\u8fc7\u5408\u6210\u865a\u62dforacle\u9ad8\u6548\u526a\u679d\u641c\u7d22\u7a7a\u95f4\uff1b3) \u901a\u8fc7\u6e10\u8fdb\u56e0\u679c\u7b5b\u9009\u7b56\u7565\u5b9e\u73b0\u53cd\u4e8b\u5b9e\u5f52\u56e0\uff0c\u4e25\u683c\u533a\u5206\u771f\u6b63\u7684\u6839\u672c\u539f\u56e0\u548c\u4f20\u64ad\u7684\u75c7\u72b6\u3002", "result": "\u5728Who&When\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCHIEF\u5728\u667a\u80fd\u4f53\u7ea7\u522b\u548c\u6b65\u9aa4\u7ea7\u522b\u51c6\u786e\u6027\u4e0a\u90fd\u4f18\u4e8e\u516b\u4e2a\u5f3a\u5927\u7684\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6bcf\u4e2a\u63d0\u51fa\u6a21\u5757\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "CHIEF\u901a\u8fc7\u7ed3\u6784\u5316\u5c42\u6b21\u56e0\u679c\u8868\u793a\u548c\u9ad8\u6548\u7684\u53cd\u4e8b\u5b9e\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6545\u969c\u5f52\u56e0\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u89e3\u51b3MAS\u7684\u8106\u5f31\u6027\u548c\u4e0d\u900f\u660e\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.23716", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23716", "abs": "https://arxiv.org/abs/2602.23716", "authors": ["Jiangyuan Wang", "Kejun Xiao", "Huaipeng Zhao", "Tao Luo", "Xiaoyi Zeng"], "title": "ProductResearch: Training E-Commerce Deep Research Agents via Multi-Agent Synthetic Trajectory Distillation", "comment": null, "summary": "Large Language Model (LLM)-based agents show promise for e-commerce conversational shopping, yet existing implementations lack the interaction depth and contextual breadth required for complex product research. Meanwhile, the Deep Research paradigm, despite advancing information synthesis in web search, suffers from domain gaps when transferred to e-commerce. We propose ProductResearch, a multi-agent framework that synthesizes high-fidelity, long-horizon tool-use trajectories for training robust e-commerce shopping agents. The framework employs a User Agent to infer nuanced shopping intents from behavioral histories, and a Supervisor Agent that orchestrates iterative collaboration with a Research Agent to generate synthetic trajectories culminating in comprehensive, insightful product research reports. These trajectories are rigorously filtered and distilled through a reflective internalization process that consolidates multi-agent supervisory interactions into coherent single-role training examples, enabling effective fine-tuning of LLM agents for complex shopping inquiries. Extensive experiments show that a compact MoE model fine-tuned on our synthetic data achieves substantial improvements over its base model in response comprehensiveness, research depth, and user-perceived utility, approaching the performance of frontier proprietary deep research systems and establishing multi-agent synthetic trajectory training as an effective and scalable paradigm for enhancing LLM-based shopping assistance.", "AI": {"tldr": "\u63d0\u51faProductResearch\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\u6765\u8bad\u7ec3\u7535\u5546\u8d2d\u7269\u667a\u80fd\u4f53\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u590d\u6742\u4ea7\u54c1\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709LLM\u7535\u5546\u8d2d\u7269\u667a\u80fd\u4f53\u7f3a\u4e4f\u6df1\u5ea6\u4ea4\u4e92\u548c\u4e0a\u4e0b\u6587\u5e7f\u5ea6\uff0c\u800c\u6df1\u5ea6\u7814\u7a76\u8303\u5f0f\u5728\u7535\u5546\u9886\u57df\u5b58\u5728\u9886\u57df\u5dee\u8ddd\uff0c\u9700\u8981\u4e13\u95e8\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u7528\u6237\u667a\u80fd\u4f53\u63a8\u65ad\u8d2d\u7269\u610f\u56fe\uff0c\u76d1\u7763\u667a\u80fd\u4f53\u534f\u8c03\u7814\u7a76\u667a\u80fd\u4f53\u8fed\u4ee3\u534f\u4f5c\u751f\u6210\u5408\u6210\u8f68\u8ff9\uff0c\u901a\u8fc7\u53cd\u601d\u5185\u5316\u8fc7\u7a0b\u5c06\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8f6c\u5316\u4e3a\u5355\u89d2\u8272\u8bad\u7ec3\u6837\u672c", "result": "\u57fa\u4e8e\u5408\u6210\u6570\u636e\u5fae\u8c03\u7684\u7d27\u51d1MoE\u6a21\u578b\u5728\u54cd\u5e94\u5168\u9762\u6027\u3001\u7814\u7a76\u6df1\u5ea6\u548c\u7528\u6237\u611f\u77e5\u6548\u7528\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u63a5\u8fd1\u524d\u6cbf\u4e13\u6709\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u6027\u80fd", "conclusion": "\u591a\u667a\u80fd\u4f53\u5408\u6210\u8f68\u8ff9\u8bad\u7ec3\u662f\u589e\u5f3aLLM\u8d2d\u7269\u8f85\u52a9\u7684\u6709\u6548\u53ef\u6269\u5c55\u8303\u5f0f\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u63d0\u5347\u7535\u5546\u5bf9\u8bdd\u8d2d\u7269\u667a\u80fd\u4f53\u7684\u80fd\u529b"}}
{"id": "2602.23720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23720", "abs": "https://arxiv.org/abs/2602.23720", "authors": ["Sheng Cao", "Zhao Chang", "Chang Li", "Hannan Li", "Liyao Fu", "Ji Tang"], "title": "The Auton Agentic AI Framework", "comment": null, "summary": "The field of Artificial Intelligence is undergoing a transition from Generative AI -- probabilistic generation of text and images -- to Agentic AI, in which autonomous systems execute actions within external environments on behalf of users. This transition exposes a fundamental architectural mismatch: Large Language Models (LLMs) produce stochastic, unstructured outputs, whereas the backend infrastructure they must control -- databases, APIs, cloud services -- requires deterministic, schema-conformant inputs. The present paper describes the Auton Agentic AI Framework, a principled architecture for standardizing the creation, execution, and governance of autonomous agent systems. The framework is organized around a strict separation between the Cognitive Blueprint, a declarative, language-agnostic specification of agent identity and capabilities, and the Runtime Engine, the platform-specific execution substrate that instantiates and runs the agent. This separation enables cross-language portability, formal auditability, and modular tool integration via the Model Context Protocol (MCP). The paper formalizes the agent execution model as an augmented Partially Observable Markov Decision Process (POMDP) with a latent reasoning space, introduces a hierarchical memory consolidation architecture inspired by biological episodic memory systems, defines a constraint manifold formalism for safety enforcement via policy projection rather than post-hoc filtering, presents a three-level self-evolution framework spanning in-context adaptation through reinforcement learning, and describes runtime optimizations -- including parallel graph execution, speculative inference, and dynamic context pruning -- that reduce end-to-end latency for multi-step agent workflows.", "AI": {"tldr": "Auton Agentic AI Framework\uff1a\u4e00\u4e2a\u6807\u51c6\u5316\u81ea\u4e3b\u667a\u80fd\u4f53\u521b\u5efa\u3001\u6267\u884c\u548c\u6cbb\u7406\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u8ba4\u77e5\u84dd\u56fe\u4e0e\u8fd0\u884c\u65f6\u5f15\u64ce\u5206\u79bb\u89e3\u51b3LLM\u8f93\u51fa\u4e0e\u540e\u7aef\u57fa\u7840\u8bbe\u65bd\u9700\u6c42\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "AI\u9886\u57df\u6b63\u4ece\u751f\u6210\u5f0fAI\u5411\u667a\u80fd\u4f53AI\u8fc7\u6e21\uff0c\u4f46LLM\u7684\u968f\u673a\u975e\u7ed3\u6784\u5316\u8f93\u51fa\u4e0e\u540e\u7aef\u57fa\u7840\u8bbe\u65bd\u6240\u9700\u7684\u786e\u5b9a\u6027\u3001\u6a21\u5f0f\u4e00\u81f4\u8f93\u5165\u4e4b\u95f4\u5b58\u5728\u67b6\u6784\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51faAuton Agentic AI Framework\uff0c\u91c7\u7528\u8ba4\u77e5\u84dd\u56fe\uff08\u58f0\u660e\u5f0f\u3001\u8bed\u8a00\u65e0\u5173\u7684\u667a\u80fd\u4f53\u8eab\u4efd\u548c\u80fd\u529b\u89c4\u8303\uff09\u4e0e\u8fd0\u884c\u65f6\u5f15\u64ce\uff08\u5e73\u53f0\u7279\u5b9a\u6267\u884c\u57fa\u677f\uff09\u4e25\u683c\u5206\u79bb\u7684\u67b6\u6784\uff0c\u652f\u6301\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u7684\u6a21\u5757\u5316\u5de5\u5177\u96c6\u6210\u3002", "result": "\u6846\u67b6\u5b9e\u73b0\u4e86\u8de8\u8bed\u8a00\u53ef\u79fb\u690d\u6027\u3001\u5f62\u5f0f\u5316\u53ef\u5ba1\u8ba1\u6027\u548c\u6a21\u5757\u5316\u5de5\u5177\u96c6\u6210\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u3001\u5206\u5c42\u8bb0\u5fc6\u6574\u5408\u3001\u7ea6\u675f\u6d41\u5f62\u5f62\u5f0f\u5316\u3001\u4e09\u7ea7\u81ea\u8fdb\u5316\u6846\u67b6\u548c\u8fd0\u884c\u65f6\u4f18\u5316\u7b49\u6280\u672f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u521b\u5efa\u3001\u6267\u884c\u548c\u6cbb\u7406\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u5f0fAI\u5411\u667a\u80fd\u4f53AI\u8fc7\u6e21\u4e2d\u7684\u5173\u952e\u67b6\u6784\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u652f\u6301\u5b89\u5168\u3001\u9ad8\u6548\u7684\u591a\u6b65\u9aa4\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2602.23777", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23777", "abs": "https://arxiv.org/abs/2602.23777", "authors": ["Zhipeng Xu", "Zilong Wang", "Xinyang Jiang", "Dongsheng Li", "De Cheng", "Nannan Wang"], "title": "Reasoning-Driven Multimodal LLM for Domain Generalization", "comment": "Accepted at ICLR 2026 (Poster)", "summary": "This paper addresses the domain generalization (DG) problem in deep learning. While most DG methods focus on enforcing visual feature invariance, we leverage the reasoning capability of multimodal large language models (MLLMs) and explore the potential of constructing reasoning chains that derives image categories to achieve more robust predictions under domain shift. To this end, we systematically study the role of reasoning in DG using DomainBed-Reasoning, a newly constructed extension of DomainBed dataset, in which each sample is paired with class-relevant reasoning chains. Our analysis reveals two key challenges: (i) fine-tuning MLLMs with reasoning chains for classification is more challenging than direct label supervision, since the model must optimize complex reasoning sequences before label prediction; and (ii) mismatches in reasoning patterns between supervision signals and fine-tuned MLLMs lead to a trade-off between semantic richness (informative but harder to optimize) and optimization efficiency (easier to optimize but less informative). To address these issues, we propose RD-MLDG (Reasoning-Driven Multimodal LLM for Domain Generalization), a framework with two components: (i) MTCT (Multi-Task Cross-Training), which introduces an additional direct classification pathway to guide reasoning supervision; and (ii) SARR (Self-Aligned Reasoning Regularization), which preserves the semantic richness of reasoning chains while mitigating reasoning-pattern mismatches via iterative self-labeling. Experiments on standard DomainBed datasets (PACS, VLCS, OfficeHome, TerraInc) demonstrate that RD-MLDG achieves state-of-the-art performances, highlighting reasoning as a promising complementary signal for robust out-of-domain generalization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faRD-MLDG\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u63a8\u7406\u94fe\u6784\u5efa\u548c\u81ea\u5bf9\u9f50\u6b63\u5219\u5316\u63d0\u5347\u6a21\u578b\u5728\u57df\u5916\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u5927\u591a\u6570\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u5173\u6ce8\u89c6\u89c9\u7279\u5f81\u4e0d\u53d8\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u4f5c\u8005\u63a2\u7d22\u901a\u8fc7\u6784\u5efa\u63a8\u7406\u94fe\u6765\u63a8\u5bfc\u56fe\u50cf\u7c7b\u522b\uff0c\u4ee5\u5728\u9886\u57df\u504f\u79fb\u4e0b\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u9884\u6d4b\u3002", "method": "\u63d0\u51faRD-MLDG\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) MTCT\uff08\u591a\u4efb\u52a1\u4ea4\u53c9\u8bad\u7ec3\uff09\uff0c\u5f15\u5165\u76f4\u63a5\u5206\u7c7b\u8def\u5f84\u6765\u6307\u5bfc\u63a8\u7406\u76d1\u7763\uff1b2) SARR\uff08\u81ea\u5bf9\u9f50\u63a8\u7406\u6b63\u5219\u5316\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u6807\u6ce8\u4fdd\u6301\u63a8\u7406\u94fe\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u540c\u65f6\u7f13\u89e3\u63a8\u7406\u6a21\u5f0f\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u6807\u51c6DomainBed\u6570\u636e\u96c6\uff08PACS\u3001VLCS\u3001OfficeHome\u3001TerraInc\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRD-MLDG\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u63a8\u7406\u4f5c\u4e3a\u9c81\u68d2\u57df\u5916\u6cdb\u5316\u7684\u6709\u524d\u666f\u7684\u8865\u5145\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u9886\u57df\u6cdb\u5316\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u7684RD-MLDG\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u63a8\u7406\u94fe\u548c\u81ea\u5bf9\u9f50\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u7406\u76d1\u7763\u4e2d\u7684\u4f18\u5316\u6311\u6218\u548c\u6a21\u5f0f\u4e0d\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2602.23802", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23802", "abs": "https://arxiv.org/abs/2602.23802", "authors": ["Yiyang Fang", "Wenke Huang", "Pei Fu", "Yihao Yang", "Kehua Su", "Zhenbo Luo", "Jian Luan", "Mang Ye"], "title": "EMO-R3: Reflective Reinforcement Learning for Emotional Reasoning in Multimodal Large Language Models", "comment": "Accepted by CVPR 2026", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual reasoning and understanding tasks but still struggle to capture the complexity and subjectivity of human emotions. Existing approaches based on supervised fine-tuning often suffer from limited generalization and poor interpretability, while reinforcement learning methods such as Group Relative Policy Optimization fail to align with the intrinsic characteristics of emotional cognition. To address these challenges, we propose Reflective Reinforcement Learning for Emotional Reasoning (EMO-R3), a framework designed to enhance the emotional reasoning ability of MLLMs. Specifically, we introduce Structured Emotional Thinking to guide the model to perform step-by-step emotional reasoning in a structured and interpretable manner, and design a Reflective Emotional Reward that enables the model to re-evaluate its reasoning based on visual-text consistency and emotional coherence. Extensive experiments demonstrate that EMO-R3 significantly improves both the interpretability and emotional intelligence of MLLMs, achieving superior performance across multiple visual emotional understanding benchmarks.", "AI": {"tldr": "\u63d0\u51faEMO-R3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u60c5\u611f\u601d\u7ef4\u548c\u53cd\u601d\u5f0f\u60c5\u611f\u5956\u52b1\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6355\u6349\u4eba\u7c7b\u60c5\u611f\u7684\u590d\u6742\u6027\u548c\u4e3b\u89c2\u6027\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u53ef\u89e3\u91ca\u6027\u5dee\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5982GRPO\u672a\u80fd\u4e0e\u60c5\u611f\u8ba4\u77e5\u7684\u5185\u5728\u7279\u6027\u5bf9\u9f50\u3002", "method": "\u63d0\u51faEMO-R3\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u7ed3\u6784\u5316\u60c5\u611f\u601d\u7ef4\uff0c\u5f15\u5bfc\u6a21\u578b\u4ee5\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u8fdb\u884c\u9010\u6b65\u60c5\u611f\u63a8\u7406\uff1b2) \u53cd\u601d\u5f0f\u60c5\u611f\u5956\u52b1\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u57fa\u4e8e\u89c6\u89c9-\u6587\u672c\u4e00\u81f4\u6027\u548c\u60c5\u611f\u8fde\u8d2f\u6027\u91cd\u65b0\u8bc4\u4f30\u5176\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEMO-R3\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u60c5\u611f\u667a\u80fd\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u60c5\u611f\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "EMO-R3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u601d\u7ef4\u548c\u53cd\u601d\u5f0f\u5956\u52b1\u673a\u5236\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u60c5\u611f\u7406\u89e3\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.23974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23974", "abs": "https://arxiv.org/abs/2602.23974", "authors": ["Fan Zhang", "Baoru Huang", "Xin Zhang"], "title": "Pessimistic Auxiliary Policy for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning aims to learn an agent from pre-collected datasets, avoiding unsafe and inefficient real-time interaction. However, inevitable access to out-ofdistribution actions during the learning process introduces approximation errors, causing the error accumulation and considerable overestimation. In this paper, we construct a new pessimistic auxiliary policy for sampling reliable actions. Specifically, we develop a pessimistic auxiliary strategy by maximizing the lower confidence bound of the Q-function. The pessimistic auxiliary strategy exhibits a relatively high value and low uncertainty in the vicinity of the learned policy, avoiding the learned policy sampling high-value actions with potentially high errors during the learning process. Less approximation error introduced by sampled action from pessimistic auxiliary strategy leads to the alleviation of error accumulation. Extensive experiments on offline reinforcement learning benchmarks reveal that utilizing the pessimistic auxiliary strategy can effectively improve the efficacy of other offline RL approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u6765\u7f13\u89e3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u5916\u52a8\u4f5c\u8bef\u5dee\u79ef\u7d2f\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u5927\u5316Q\u51fd\u6570\u7684\u4e0b\u754c\u7f6e\u4fe1\u5ea6\u6765\u91c7\u6837\u53ef\u9760\u52a8\u4f5c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u73b0\u6709\u79bb\u7ebfRL\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4ece\u9884\u6536\u96c6\u7684\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u907f\u514d\u4e86\u5b9e\u65f6\u4ea4\u4e92\u7684\u4e0d\u5b89\u5168\u6027\u548c\u4f4e\u6548\u6027\u3002\u7136\u800c\uff0c\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4e0d\u53ef\u907f\u514d\u5730\u8bbf\u95ee\u5206\u5e03\u5916\u52a8\u4f5c\u4f1a\u5f15\u5165\u8fd1\u4f3c\u8bef\u5dee\uff0c\u5bfc\u81f4\u8bef\u5dee\u79ef\u7d2f\u548c\u4e25\u91cd\u7684\u8fc7\u9ad8\u4f30\u8ba1\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u6765\u91c7\u6837\u53ef\u9760\u52a8\u4f5c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u901a\u8fc7\u6700\u5927\u5316Q\u51fd\u6570\u7684\u4e0b\u754c\u7f6e\u4fe1\u5ea6\u6765\u5f00\u53d1\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u3002\u8be5\u7b56\u7565\u5728\u5b66\u4e60\u7b56\u7565\u9644\u8fd1\u8868\u73b0\u51fa\u76f8\u5bf9\u8f83\u9ad8\u7684\u4ef7\u503c\u548c\u8f83\u4f4e\u7684uncertainty\uff0c\u907f\u514d\u4e86\u5b66\u4e60\u7b56\u7565\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u91c7\u6837\u5177\u6709\u6f5c\u5728\u9ad8\u8bef\u5dee\u7684\u9ad8\u4ef7\u503c\u52a8\u4f5c\u3002", "result": "\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5229\u7528\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u5176\u4ed6\u79bb\u7ebfRL\u65b9\u6cd5\u7684\u6548\u80fd\u3002\u7531\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u91c7\u6837\u7684\u52a8\u4f5c\u5f15\u5165\u7684\u8f83\u5c11\u8fd1\u4f3c\u8bef\u5dee\u6709\u52a9\u4e8e\u7f13\u89e3\u8bef\u5dee\u79ef\u7d2f\u3002", "conclusion": "\u63d0\u51fa\u7684\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u901a\u8fc7\u91c7\u6837\u53ef\u9760\u52a8\u4f5c\u6765\u51cf\u5c11\u5206\u5e03\u5916\u52a8\u4f5c\u5f15\u8d77\u7684\u8fd1\u4f3c\u8bef\u5dee\uff0c\u4ece\u800c\u6709\u6548\u7f13\u89e3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8bef\u5dee\u79ef\u7d2f\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2602.24055", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.24055", "abs": "https://arxiv.org/abs/2602.24055", "authors": ["Reva Schwartz", "Carina Westling", "Morgan Briggs", "Marzieh Fadaee", "Isar Nejadgholi", "Matthew Holmes", "Fariza Rashid", "Maya Carlyle", "Afaf Ta\u00efk", "Kyra Wilson", "Peter Douglas", "Theodora Skeadas", "Gabriella Waters", "Rumman Chowdhury", "Thiago Lacerda"], "title": "CIRCLE: A Framework for Evaluating AI from a Real-World Lens", "comment": "Accepted at Intelligent Systems Conference (IntelliSys) 2026", "summary": "This paper proposes CIRCLE, a six-stage, lifecycle-based framework to bridge the reality gap between model-centric performance metrics and AI's materialized outcomes in deployment. While existing frameworks like MLOps focus on system stability and benchmarks measure abstract capabilities, decision-makers outside the AI stack lack systematic evidence about the behavior of AI technologies under real-world user variability and constraints. CIRCLE operationalizes the Validation phase of TEVV (Test, Evaluation, Verification, and Validation) by formalizing the translation of stakeholder concerns outside the stack into measurable signals. Unlike participatory design, which often remains localized, or algorithmic audits, which are often retrospective, CIRCLE provides a structured, prospective protocol for linking context-sensitive qualitative insights to scalable quantitative metrics. By integrating methods such as field testing, red teaming, and longitudinal studies into a coordinated pipeline, CIRCLE produces systematic knowledge: evidence that is comparable across sites yet sensitive to local context. This can enable governance based on materialized downstream effects rather than theoretical capabilities.", "AI": {"tldr": "CIRCLE\u6846\u67b6\u901a\u8fc7\u516d\u9636\u6bb5\u751f\u547d\u5468\u671f\u65b9\u6cd5\uff0c\u5c06\u6a21\u578b\u6027\u80fd\u6307\u6807\u4e0eAI\u5b9e\u9645\u90e8\u7f72\u7ed3\u679c\u8fde\u63a5\u8d77\u6765\uff0c\u586b\u8865\u73b0\u5b9e\u5dee\u8ddd", "motivation": "\u73b0\u6709MLOps\u6846\u67b6\u5173\u6ce8\u7cfb\u7edf\u7a33\u5b9a\u6027\uff0c\u57fa\u51c6\u6d4b\u8bd5\u8861\u91cf\u62bd\u8c61\u80fd\u529b\uff0c\u4f46\u51b3\u7b56\u8005\u7f3a\u4e4fAI\u5728\u771f\u5b9e\u4e16\u754c\u7528\u6237\u53d8\u5f02\u548c\u7ea6\u675f\u4e0b\u7684\u884c\u4e3a\u8bc1\u636e", "method": "CIRCLE\u662f\u4e00\u4e2a\u516d\u9636\u6bb5\u751f\u547d\u5468\u671f\u6846\u67b6\uff0c\u5c06TEVV\u4e2d\u7684\u9a8c\u8bc1\u9636\u6bb5\u64cd\u4f5c\u5316\uff0c\u5c06\u5229\u76ca\u76f8\u5173\u8005\u5173\u6ce8\u8f6c\u5316\u4e3a\u53ef\u6d4b\u91cf\u4fe1\u53f7\uff0c\u6574\u5408\u73b0\u573a\u6d4b\u8bd5\u3001\u7ea2\u961f\u6d4b\u8bd5\u548c\u7eb5\u5411\u7814\u7a76\u7b49\u65b9\u6cd5", "result": "CIRCLE\u63d0\u4f9b\u7ed3\u6784\u5316\u524d\u77bb\u6027\u534f\u8bae\uff0c\u5c06\u60c5\u5883\u654f\u611f\u7684\u5b9a\u6027\u6d1e\u5bdf\u4e0e\u53ef\u6269\u5c55\u7684\u5b9a\u91cf\u6307\u6807\u8fde\u63a5\uff0c\u4ea7\u751f\u53ef\u8de8\u7ad9\u70b9\u6bd4\u8f83\u53c8\u5bf9\u672c\u5730\u60c5\u5883\u654f\u611f\u7684\u7cfb\u7edf\u77e5\u8bc6", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u57fa\u4e8e\u5177\u4f53\u4e0b\u6e38\u6548\u5e94\u800c\u975e\u7406\u8bba\u80fd\u529b\u7684\u6cbb\u7406\uff0c\u4e3aAI\u90e8\u7f72\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5"}}
{"id": "2602.24080", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.24080", "abs": "https://arxiv.org/abs/2602.24080", "authors": ["Xiang Li", "Jiabao Gao", "Sipei Lin", "Xuan Zhou", "Chi Zhang", "Bo Cheng", "Jiale Han", "Benyou Wang"], "title": "Human or Machine? A Preliminary Turing Test for Speech-to-Speech Interaction", "comment": "Accepted by ICLR 2026 Conference", "summary": "The pursuit of human-like conversational agents has long been guided by the Turing test. For modern speech-to-speech (S2S) systems, a critical yet unanswered question is whether they can converse like humans. To tackle this, we conduct the first Turing test for S2S systems, collecting 2,968 human judgments on dialogues between 9 state-of-the-art S2S systems and 28 human participants. Our results deliver a clear finding: no existing evaluated S2S system passes the test, revealing a significant gap in human-likeness. To diagnose this failure, we develop a fine-grained taxonomy of 18 human-likeness dimensions and crowd-annotate our collected dialogues accordingly. Our analysis shows that the bottleneck is not semantic understanding but stems from paralinguistic features, emotional expressivity, and conversational persona. Furthermore, we find that off-the-shelf AI models perform unreliably as Turing test judges. In response, we propose an interpretable model that leverages the fine-grained human-likeness ratings and delivers accurate and transparent human-vs-machine discrimination, offering a powerful tool for automatic human-likeness evaluation. Our work establishes the first human-likeness evaluation for S2S systems and moves beyond binary outcomes to enable detailed diagnostic insights, paving the way for human-like improvements in conversational AI systems.", "AI": {"tldr": "\u9996\u4e2a\u9488\u5bf9\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u56fe\u7075\u6d4b\u8bd5\u663e\u793a\uff0c\u73b0\u6709\u7cfb\u7edf\u5747\u672a\u901a\u8fc7\u6d4b\u8bd5\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u526f\u8bed\u8a00\u7279\u5f81\u3001\u60c5\u611f\u8868\u8fbe\u548c\u5bf9\u8bdd\u4e2a\u6027\uff0c\u800c\u975e\u8bed\u4e49\u7406\u89e3\u3002", "motivation": "\u73b0\u4ee3\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u80fd\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u5bf9\u8bdd\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u8bc4\u4f30\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4eba\u7c7b\u76f8\u4f3c\u6027\uff0c\u7814\u7a76\u8005\u8fdb\u884c\u4e86\u9996\u4e2a\u9488\u5bf9\u8fd9\u7c7b\u7cfb\u7edf\u7684\u56fe\u7075\u6d4b\u8bd5\u3002", "method": "\u6536\u96c6\u4e862,968\u4e2a\u4eba\u7c7b\u5224\u65ad\uff0c\u8bc4\u4f309\u4e2a\u6700\u5148\u8fdb\u7684\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u4e0e28\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u5bf9\u8bdd\u3002\u5f00\u53d1\u4e86\u5305\u542b18\u4e2a\u4eba\u7c7b\u76f8\u4f3c\u6027\u7ef4\u5ea6\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6cd5\uff0c\u5e76\u5bf9\u6536\u96c6\u7684\u5bf9\u8bdd\u8fdb\u884c\u4f17\u5305\u6807\u6ce8\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u5229\u7528\u7ec6\u7c92\u5ea6\u7684\u4eba\u7c7b\u76f8\u4f3c\u6027\u8bc4\u5206\u8fdb\u884c\u51c6\u786e\u900f\u660e\u7684\u4eba\u673a\u533a\u5206\u3002", "result": "\u6240\u6709\u8bc4\u4f30\u7684\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u5747\u672a\u901a\u8fc7\u56fe\u7075\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5728\u4eba\u7c7b\u76f8\u4f3c\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u74f6\u9888\u4e0d\u5728\u4e8e\u8bed\u4e49\u7406\u89e3\uff0c\u800c\u5728\u4e8e\u526f\u8bed\u8a00\u7279\u5f81\u3001\u60c5\u611f\u8868\u8fbe\u548c\u5bf9\u8bdd\u4e2a\u6027\u3002\u73b0\u6210\u7684AI\u6a21\u578b\u4f5c\u4e3a\u56fe\u7075\u6d4b\u8bd5\u8bc4\u5224\u8005\u8868\u73b0\u4e0d\u53ef\u9760\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u9996\u4e2a\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4eba\u7c7b\u76f8\u4f3c\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u4e8c\u5143\u7ed3\u679c\uff0c\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u8bca\u65ad\u89c1\u89e3\uff0c\u4e3a\u5bf9\u8bddAI\u7cfb\u7edf\u5b9e\u73b0\u4eba\u7c7b\u76f8\u4f3c\u6027\u6539\u8fdb\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.24097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24097", "abs": "https://arxiv.org/abs/2602.24097", "authors": ["Yue Xie", "Zizhen Xu", "William Beazley", "Fumiya Iida"], "title": "Bi-level RL-Heuristic Optimization for Real-world Winter Road Maintenance", "comment": null, "summary": "Winter road maintenance is critical for ensuring public safety and reducing environmental impacts, yet existing methods struggle to manage large-scale routing problems effectively and mostly reply on human decision. This study presents a novel, scalable bi-level optimization framework, validated on real operational data on UK strategic road networks (M25, M6, A1), including interconnected local road networks in surrounding areas for vehicle traversing, as part of the highway operator's efforts to solve existing planning challenges. At the upper level, a reinforcement learning (RL) agent strategically partitions the road network into manageable clusters and optimally allocates resources from multiple depots. At the lower level, a multi-objective vehicle routing problem (VRP) is solved within each cluster, minimizing the maximum vehicle travel time and total carbon emissions. Unlike existing approaches, our method handles large-scale, real-world networks efficiently, explicitly incorporating vehicle-specific constraints, depot capacities, and road segment requirements. Results demonstrate significant improvements, including balanced workloads, reduced maximum travel times below the targeted two-hour threshold, lower emissions, and substantial cost savings. This study illustrates how advanced AI-driven bi-level optimization can directly enhance operational decision-making in real-world transportation and logistics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u76ee\u6807\u8f66\u8f86\u8def\u5f84\u89c4\u5212\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u51ac\u5b63\u9053\u8def\u7ef4\u62a4\u8def\u7ebf\u4f18\u5316\uff0c\u5728\u82f1\u56fd\u771f\u5b9e\u8def\u7f51\u4e0a\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u51ac\u5b63\u9053\u8def\u7ef4\u62a4\u5bf9\u516c\u5171\u5b89\u5168\u548c\u73af\u5883\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u8def\u7ebf\u89c4\u5212\u95ee\u9898\uff0c\u4e14\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff1a\u4e0a\u5c42\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5c06\u8def\u7f51\u5212\u5206\u4e3a\u53ef\u7ba1\u7406\u96c6\u7fa4\u5e76\u4ece\u591a\u4e2a\u4ed3\u5e93\u4f18\u5316\u5206\u914d\u8d44\u6e90\uff1b\u4e0b\u5c42\u5728\u6bcf\u4e2a\u96c6\u7fa4\u5185\u89e3\u51b3\u591a\u76ee\u6807\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff0c\u6700\u5c0f\u5316\u6700\u957f\u8f66\u8f86\u884c\u9a76\u65f6\u95f4\u548c\u603b\u78b3\u6392\u653e\u3002", "result": "\u7ed3\u679c\u663e\u793a\u663e\u8457\u6539\u8fdb\uff1a\u5e73\u8861\u4e86\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5c06\u6700\u957f\u884c\u9a76\u65f6\u95f4\u63a7\u5236\u5728\u76ee\u6807\u4e24\u5c0f\u65f6\u9608\u503c\u4ee5\u4e0b\uff0c\u964d\u4f4e\u4e86\u6392\u653e\uff0c\u5e76\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6210\u672c\u8282\u7ea6\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5148\u8fdb\u7684AI\u9a71\u52a8\u53cc\u5c42\u4f18\u5316\u53ef\u4ee5\u76f4\u63a5\u589e\u5f3a\u73b0\u5b9e\u4e16\u754c\u4ea4\u901a\u548c\u7269\u6d41\u9886\u57df\u7684\u8fd0\u8425\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2602.24110", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24110", "abs": "https://arxiv.org/abs/2602.24110", "authors": ["Yanwei Ren", "Haotian Zhang", "Likang Xiao", "Xikai Zhang", "Jiaxing Huang", "Jiayan Qiu", "Baosheng Yu", "Quan Chen", "Liu Liu"], "title": "Recycling Failures: Salvaging Exploration in RLVR via Fine-Grained Off-Policy Guidance", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the complex reasoning capabilities of Large Reasoning Models. However, standard outcome-based supervision suffers from a critical limitation that penalizes trajectories that are largely correct but fail due to several missteps as heavily as completely erroneous ones. This coarse feedback signal causes the model to discard valuable largely correct rollouts, leading to a degradation in rollout diversity that prematurely narrows the exploration space. Process Reward Models have demonstrated efficacy in providing reliable step-wise verification for test-time scaling, naively integrating these signals into RLVR as dense rewards proves ineffective.Prior methods attempt to introduce off-policy guided whole-trajectory replacement that often outside the policy model's distribution, but still fail to utilize the largely correct rollouts generated by the model itself and thus do not effectively mitigate the narrowing of the exploration space. To address these issues, we propose SCOPE (Step-wise Correction for On-Policy Exploration), a novel framework that utilizes Process Reward Models to pinpoint the first erroneous step in suboptimal rollouts and applies fine-grained, step-wise off-policy rectification. By applying precise refinement on partially correct rollout, our method effectively salvages partially correct trajectories and increases diversity score by 13.5%, thereby sustaining a broad exploration space. Extensive experiments demonstrate that our approach establishes new state-of-the-art results, achieving an average accuracy of 46.6% on math reasoning and exhibiting robust generalization with 53.4% accuracy on out-of-distribution reasoning tasks.", "AI": {"tldr": "SCOPE\u6846\u67b6\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5b9a\u4f4d\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u9996\u4e2a\u9519\u8bef\u6b65\u9aa4\uff0c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fee\u6b63\uff0c\u6709\u6548\u5229\u7528\u90e8\u5206\u6b63\u786e\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u63d0\u5347\u63a2\u7d22\u591a\u6837\u602713.5%\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u523046.6%\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u76d1\u7763\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5bf9\u90e8\u5206\u6b63\u786e\u4f46\u5305\u542b\u51e0\u4e2a\u9519\u8bef\u6b65\u9aa4\u7684\u8f68\u8ff9\u60e9\u7f5a\u4e0e\u5b8c\u5168\u9519\u8bef\u8f68\u8ff9\u76f8\u540c\uff0c\u5bfc\u81f4\u6a21\u578b\u4e22\u5f03\u6709\u4ef7\u503c\u7684\u90e8\u5206\u6b63\u786e\u63a8\u7406\u8f68\u8ff9\uff0c\u964d\u4f4e\u63a2\u7d22\u591a\u6837\u6027\uff0c\u8fc7\u65e9\u7f29\u5c0f\u63a2\u7d22\u7a7a\u95f4\u3002", "method": "\u63d0\u51faSCOPE\u6846\u67b6\uff0c\u5229\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u7cbe\u786e\u5b9a\u4f4d\u6b21\u4f18\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u9996\u4e2a\u9519\u8bef\u6b65\u9aa4\uff0c\u5e94\u7528\u7ec6\u7c92\u5ea6\u7684\u6b65\u9aa4\u7ea7\u79bb\u7b56\u7565\u4fee\u6b63\uff0c\u5bf9\u90e8\u5206\u6b63\u786e\u7684\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u7cbe\u786e\u4f18\u5316\u3002", "result": "SCOPE\u6709\u6548\u62ef\u6551\u90e8\u5206\u6b63\u786e\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5c06\u591a\u6837\u6027\u5206\u6570\u63d0\u534713.5%\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u523046.6%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5728\u5206\u5e03\u5916\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u523053.4%\u7684\u51c6\u786e\u7387\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "SCOPE\u6846\u67b6\u901a\u8fc7\u6b65\u9aa4\u7ea7\u4fee\u6b63\u6709\u6548\u5229\u7528\u90e8\u5206\u6b63\u786e\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u7ef4\u6301\u5e7f\u6cdb\u7684\u63a2\u7d22\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.24173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24173", "abs": "https://arxiv.org/abs/2602.24173", "authors": ["Antoine Peyronnet", "Fabian Gloeckle", "Amaury Hayat"], "title": "LemmaBench: A Live, Research-Level Benchmark to Evaluate LLM Capabilities in Mathematics", "comment": "15 pages, 3 figures, 5 Tables", "summary": "We present a new approach for benchmarking Large Language Model (LLM) capabilities on research-level mathematics. Existing benchmarks largely rely on static, hand-curated sets of contest or textbook-style problems as proxies for mathematical research. Instead, we establish an updatable benchmark evaluating models directly on the latest research results in mathematics. This consists of an automatic pipeline that extracts lemmas from arXiv and rewrites them into self-contained statements by making all assumptions and required definitions explicit. It results in a benchmark that can be updated regularly with new problems taken directly from human mathematical research, while previous instances can be used for training without compromising future evaluations. We benchmark current state-of-the-art LLMs, which obtain around 10-15$\\%$ accuracy in theorem proving (pass@1) depending on the model, showing that there is currently a large margin of progression for LLMs to reach human-level proving capabilities in a research context.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8earXiv\u6700\u65b0\u6570\u5b66\u7814\u7a76\u7684\u53ef\u66f4\u65b0LLM\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u9759\u6001\u7ade\u8d5b\u9898\u57fa\u51c6", "motivation": "\u73b0\u6709LLM\u6570\u5b66\u80fd\u529b\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u7684\u624b\u5de5\u6574\u7406\u7ade\u8d5b\u9898\u6216\u6559\u79d1\u4e66\u95ee\u9898\uff0c\u4e0d\u80fd\u771f\u6b63\u53cd\u6620\u7814\u7a76\u7ea7\u6570\u5b66\u80fd\u529b\uff0c\u9700\u8981\u5efa\u7acb\u80fd\u76f4\u63a5\u8bc4\u4f30\u6a21\u578b\u5728\u6700\u65b0\u6570\u5b66\u7814\u7a76\u6210\u679c\u4e0a\u8868\u73b0\u7684\u52a8\u6001\u57fa\u51c6", "method": "\u6784\u5efa\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff1a\u4ecearXiv\u63d0\u53d6\u5f15\u7406\uff0c\u901a\u8fc7\u663e\u5f0f\u5316\u6240\u6709\u5047\u8bbe\u548c\u5b9a\u4e49\u5c06\u5176\u91cd\u5199\u4e3a\u81ea\u5305\u542b\u7684\u9648\u8ff0\uff0c\u5f62\u6210\u53ef\u5b9a\u671f\u66f4\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u5728\u5b9a\u7406\u8bc1\u660e\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u7ea6\u4e3a10-15%\uff08pass@1\uff09\uff0c\u663e\u793aLLM\u8981\u8fbe\u5230\u4eba\u7c7b\u7814\u7a76\u6c34\u5e73\u7684\u8bc1\u660e\u80fd\u529b\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4", "conclusion": "\u5efa\u7acb\u4e86\u9996\u4e2a\u57fa\u4e8e\u6700\u65b0\u6570\u5b66\u7814\u7a76\u7684\u53ef\u66f4\u65b0LLM\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u4e3a\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u7814\u7a76\u73af\u5883\u4e2d\u7684\u6570\u5b66\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u4e0e\u4eba\u7c7b\u7814\u7a76\u6c34\u5e73\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd"}}
{"id": "2602.24195", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24195", "abs": "https://arxiv.org/abs/2602.24195", "authors": ["Gregory Kang Ruey Lau", "Hieu Dao", "Nicole Kan Hui Lin", "Bryan Kian Hsiang Low"], "title": "Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume", "comment": "Earlier versions presented at ICLR 2025 QUESTION workshop and ICML 2025 R2-FM workshop", "summary": "Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation.", "AI": {"tldr": "UMPIRE\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u91c7\u6837\u54cd\u5e94\u7684\u8bed\u4e49\u4f53\u79ef\u6765\u8bc4\u4f30\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u591a\u79cd\u6a21\u6001\u548c\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u8f93\u51fa\uff0c\u5f71\u54cd\u53ef\u9760\u90e8\u7f72\u3002\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u6a21\u6001\u3001\u4f9d\u8d56\u5916\u90e8\u5de5\u5177\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "UMPIRE\u6846\u67b6\u5229\u7528\u6a21\u578b\u5185\u90e8\u6a21\u6001\u7279\u5f81\uff0c\u65e0\u9700\u5916\u90e8\u5de5\u5177\uff0c\u901a\u8fc7\u8ba1\u7b97\u91c7\u6837\u54cd\u5e94\u7684\u4e0d\u8fde\u8d2f\u6027\u8c03\u6574\u8bed\u4e49\u4f53\u79ef\uff0c\u6355\u6349\u6837\u672c\u7684\u5168\u5c40\u8bed\u4e49\u591a\u6837\u6027\u548c\u57fa\u4e8e\u5185\u90e8\u7f6e\u4fe1\u5ea6\u7684\u5c40\u90e8\u4e0d\u8fde\u8d2f\u6027\u3002", "result": "\u5728\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891-\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u5bf9\u6297\u6027\u548c\u5206\u5e03\u5916\u8bbe\u7f6e\uff09\u4e2d\uff0cUMPIRE\u5728\u9519\u8bef\u68c0\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u975e\u6587\u672c\u8f93\u51fa\u4efb\u52a1\uff08\u5982\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\uff09\u3002", "conclusion": "UMPIRE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8f93\u5165\u8f93\u51fa\u6a21\u6001\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u90e8\u7f72\u3002"}}
{"id": "2602.24288", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24288", "abs": "https://arxiv.org/abs/2602.24288", "authors": ["Fan Shu", "Yite Wang", "Ruofan Wu", "Boyi Liu", "Zhewei Yao", "Yuxiong He", "Feng Yan"], "title": "DARE-bench: Evaluating Modeling and Instruction Fidelity of LLMs in Data Science", "comment": "Published as a conference paper at ICLR 2026. 10 pages plus appendix", "summary": "The fast-growing demands in using Large Language Models (LLMs) to tackle complex multi-step data science tasks create an emergent need for accurate benchmarking. There are two major gaps in existing benchmarks: (i) the lack of standardized, process-aware evaluation that captures instruction adherence and process fidelity, and (ii) the scarcity of accurately labeled training data. To bridge these gaps, we introduce DARE-bench, a benchmark designed for machine learning modeling and data science instruction following. Unlike many existing benchmarks that rely on human- or model-based judges, all tasks in DARE-bench have verifiable ground truth, ensuring objective and reproducible evaluation. To cover a broad range of tasks and support agentic tools, DARE-bench consists of 6,300 Kaggle-derived tasks and provides both large-scale training data and evaluation sets. Extensive evaluations show that even highly capable models such as gpt-o4-mini struggle to achieve good performance, especially in machine learning modeling tasks. Using DARE-bench training tasks for fine-tuning can substantially improve model performance. For example, supervised fine-tuning boosts Qwen3-32B's accuracy by 1.83x and reinforcement learning boosts Qwen3-4B's accuracy by more than 8x. These significant improvements verify the importance of DARE-bench both as an accurate evaluation benchmark and critical training data.", "AI": {"tldr": "DARE-bench\u662f\u4e00\u4e2a\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u548c\u6570\u636e\u79d1\u5b66\u6307\u4ee4\u9075\u5faa\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b6,300\u4e2aKaggle\u884d\u751f\u4efb\u52a1\uff0c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u6807\u51c6\u5316\u8fc7\u7a0b\u8bc4\u4f30\u548c\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u7f3a\u9677\uff1a\u4e00\u662f\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8fc7\u7a0b\u611f\u77e5\u8bc4\u4f30\uff0c\u65e0\u6cd5\u6355\u6349\u6307\u4ee4\u9075\u5faa\u548c\u8fc7\u7a0b\u4fdd\u771f\u5ea6\uff1b\u4e8c\u662f\u7f3a\u4e4f\u51c6\u786e\u6807\u6ce8\u7684\u8bad\u7ec3\u6570\u636e\u3002\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u591a\u6b65\u9aa4\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u7684\u9700\u6c42\u5feb\u901f\u589e\u957f\uff0c\u8feb\u5207\u9700\u8981\u51c6\u786e\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u63d0\u51faDARE-bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b6,300\u4e2a\u4eceKaggle\u5e73\u53f0\u884d\u751f\u7684\u4efb\u52a1\uff0c\u6240\u6709\u4efb\u52a1\u90fd\u6709\u53ef\u9a8c\u8bc1\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u786e\u4fdd\u5ba2\u89c2\u548c\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u3002\u8be5\u57fa\u51c6\u8986\u76d6\u5e7f\u6cdb\u7684\u4efb\u52a1\u8303\u56f4\u5e76\u652f\u6301\u667a\u80fd\u4f53\u5de5\u5177\uff0c\u540c\u65f6\u63d0\u4f9b\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u548c\u8bc4\u4f30\u96c6\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5373\u4f7f\u662fgpt-o4-mini\u7b49\u9ad8\u6027\u80fd\u6a21\u578b\u4e5f\u96be\u4ee5\u53d6\u5f97\u826f\u597d\u8868\u73b0\uff0c\u5c24\u5176\u5728\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u4efb\u52a1\u4e2d\u3002\u4f7f\u7528DARE-bench\u8bad\u7ec3\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff1a\u76d1\u7763\u5fae\u8c03\u4f7fQwen3-32B\u51c6\u786e\u7387\u63d0\u53471.83\u500d\uff0c\u5f3a\u5316\u5b66\u4e60\u4f7fQwen3-4B\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc78\u500d\u3002", "conclusion": "DARE-bench\u65e2\u662f\u4e00\u4e2a\u51c6\u786e\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e5f\u662f\u5173\u952e\u7684\u8bad\u7ec3\u6570\u636e\u6765\u6e90\uff0c\u5176\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u9a8c\u8bc1\u4e86\u5b83\u5728\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u8bc4\u4f30\u548c\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}

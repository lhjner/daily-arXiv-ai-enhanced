<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)
*Chi-Hsiang Hsiao,Yi-Cheng Wang,Tzung-Sheng Lin,Yi-Ren Yeh,Chu-Song Chen*

Main category: cs.AI

TL;DR: 本文提出了一种多模态知识图谱增强的检索生成方法，通过整合视觉线索到知识图谱构建、检索和答案生成过程中，提升对长文档和视觉文档的理解能力。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成方法在处理长文档和领域特定内容时存在局限性，主要受限于上下文窗口大小，难以进行深度推理。现有的知识图谱增强方法又局限于文本输入，无法利用视觉等多模态信息的互补优势。

Method: 提出多模态知识图谱增强的检索生成框架，将视觉线索整合到知识图谱构建、检索和答案生成三个关键阶段，支持跨模态推理。

Result: 实验结果表明，该方法在全局和细粒度问答任务上均优于现有的检索增强生成方法，在文本和多模态语料库上都取得了更好的性能。

Conclusion: 通过整合视觉信息到知识图谱增强的检索生成框架中，能够显著提升对复杂内容的理解能力，特别是在需要跨模态推理的场景下表现出色。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.

</details>


### [2] [Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)](https://arxiv.org/abs/2512.20628)
*Edited by Tessai Hayama,Takayuki Ito,Takahiro Uchiya,Motoki Miura,Takahiro Kawaji,Takaya Yuizono,Atsuo Yoshitaka,Tokuro Matsuo,Shun Okuhara,Jawad Haqbeen,Sofia Sahab,Wen Gu,Shiyao Ding*

Main category: cs.AI

TL;DR: KICSS 2025会议论文集，包含人工智能、知识工程、人机交互和创意支持系统等领域的同行评审论文


<details>
  <summary>Details</summary>
Motivation: 为人工智能、知识工程、人机交互和创意支持系统领域的研究人员提供多学科交流平台，促进相关领域的研究发展

Method: 采用双盲同行评审流程筛选论文，部分优秀论文推荐至IEICE Transactions on Information and Systems期刊发表

Result: 成功举办了第20届国际知识、信息与创意支持系统会议，收录了经过严格评审的学术论文，形成了会议论文集

Conclusion: KICSS 2025会议为相关领域研究者提供了高质量的学术交流平台，通过严格的评审机制确保了论文质量，促进了学术成果的传播

Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.

</details>


### [3] [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.AI

TL;DR: Microprobe是一种新颖的可靠性评估方法，仅需100个战略选择的探针示例就能全面评估基础模型可靠性，相比传统方法大幅降低计算成本和时间。


<details>
  <summary>Details</summary>
Motivation: 传统基础模型可靠性评估通常需要数千个评估示例，计算成本高且耗时，难以在实际部署中广泛应用。需要一种更高效的评估方法来支持负责任的AI部署。

Method: 结合五个关键可靠性维度的战略提示多样性、先进的不确定性量化方法和自适应加权策略，通过仅100个战略选择的探针示例来高效检测潜在故障模式。

Result: 在多个语言模型（GPT-2变体）和跨领域验证（医疗、金融、法律）中，microprobe相比随机采样基线实现了23.5%更高的综合可靠性分数，具有显著的统计意义（p < 0.001，Cohen's d = 1.21）。专家验证评分为4.14/5.0（vs 随机选择3.14/5.0），以99.9%统计功效完成评估，减少90%评估成本，保持95%传统方法覆盖率。

Conclusion: Microprobe填补了高效模型评估的关键空白，为负责任的AI部署提供了一种计算效率高且可靠的评估方法，显著降低了评估成本和时间要求。

Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.

</details>


### [4] [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)
*Jianbing Ma,Ao Feng,Zhenjie Gao,Xinyu Song,Li Su,Bin Chen,Wei Wang,Jiamin Wu*

Main category: cs.AI

TL;DR: Erkang-Diagnosis-1.1是基于阿里通义千问模型开发的AI医疗咨询助手，整合了500GB高质量医学知识，采用增强预训练和检索增强生成混合方法，能在3-5轮交互中提供准确的诊断建议和健康指导。


<details>
  <summary>Details</summary>
Motivation: 开发一个安全、可靠、专业的AI健康顾问，作为用户的智能健康伴侣，赋能基层医疗和健康管理，解决医疗资源分配不均和初级医疗需求问题。

Method: 基于阿里通义千问模型开发，整合约500GB高质量结构化医学知识，采用增强预训练和检索增强生成的混合方法，通过3-5轮高效交互理解用户症状并进行初步分析。

Result: Erkang-Diagnosis-1.1在综合医学考试中表现优于GPT-4，能够准确理解用户症状并提供有价值的诊断建议和健康指导。

Conclusion: Erkang-Diagnosis-1.1是一个有效的AI医疗咨询助手，在医学知识理解和诊断建议方面表现出色，有望成为用户的智能健康伴侣并赋能基层医疗。

Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.

</details>


### [5] [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)
*Leo Lu,Jonathan Zhang,Sean Chua,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: 本文探索不同大语言模型之间推理链的互换性，研究一个模型的部分推理链能否被另一个模型可靠地继续完成，并评估这种互换性对推理稳定性和答案准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然CoT提示显著提升了LLMs的推理能力，但先前研究主要关注通过内部推理策略提升模型性能，对不同模型间推理的互换性了解甚少。本文旨在探索推理链在不同模型间的可移植性，将其视为评估推理时可信度的一种手段。

Method: 使用token级对数概率阈值在早期、中期和晚期阶段截断基线模型（Gemma-3-4B-IT和LLaMA-3.1-70B-Instruct）的推理链，然后让继续模型（Gemma-3-1B-IT和LLaMA-3.1-8B-Instruct）完成剩余推理，测试同族内和跨族行为。评估流程结合截断阈值和过程奖励模型（PRM），提供可复现的推理稳定性评估框架。

Result: 评估显示，混合推理链通常能够保持甚至在某些情况下提高最终答案的准确性和逻辑结构。这表明推理互换性可能成为推理模型的一种新兴行为特性。

Conclusion: 推理互换性为协作AI系统中可靠的模块化推理提供了新的范式，展示了不同模型间推理链的可移植性潜力，为构建更可靠的协作推理系统提供了见解。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.

</details>


### [6] [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)
*Zixun Luo,Yuhang Fan,Yufei Li,Youzhi Zhang,Hengyu Lin,Ziqi Wang*

Main category: cs.AI

TL;DR: AiAuditTrack (AAT) 是一个基于区块链的框架，用于记录AI使用流量和治理，通过去中心化身份和可验证凭证建立可信AI实体，记录交互轨迹，实现跨系统监督和审计。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动应用和大语言模型的快速扩张，AI交互数据激增，带来了安全、问责和风险可追溯性的紧迫挑战，需要建立可信的AI使用记录和治理机制。

Method: AAT利用去中心化身份(DID)和可验证凭证(VC)建立可信可识别的AI实体，将AI实体建模为动态交互图中的节点，边表示时间特定的行为轨迹，并提出风险扩散算法来追踪风险行为源头并在相关实体间传播预警。

Result: 系统性能通过区块链TPS指标评估，证明了AAT在大规模交互记录下的可行性和稳定性，能够为复杂多智能体环境提供可扩展、可验证的AI审计、风险管理和责任归属解决方案。

Conclusion: AAT提供了一个可扩展且可验证的解决方案，用于AI审计、风险管理和复杂多智能体环境中的责任归属，通过区块链技术实现可信的AI交互记录和治理。

Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.

</details>


### [7] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

Main category: cs.AI

TL;DR: 提出MoAS架构，通过学习的路由器为每个token动态选择最优注意力方案（MHA、GQA或MQA），在保持性能的同时提升推理效率


<details>
  <summary>Details</summary>
Motivation: Transformer模型中的注意力机制选择需要在建模质量和推理效率之间权衡。MHA提供最佳质量但推理时KV缓存内存需求大，MQA和GQA减少内存使用但往往以模型性能为代价

Method: 提出混合注意力方案（MoAS），通过学习的路由器为每个token动态选择最优注意力方案（MHA、GQA或MQA），而不是静态平均方案

Result: 在WikiText-2上的实验结果显示，动态路由（验证损失2.3074）优于静态混合（2.3093），性能与MHA基线竞争，同时提供条件计算效率潜力

Conclusion: MoAS通过动态路由机制有效平衡了注意力机制的质量与效率权衡，为Transformer模型提供了更灵活的架构选择

Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [8] [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)
*Deliang Wen,Ke Sun*

Main category: cs.AI

TL;DR: Memory Bear系统基于认知科学原理构建类人记忆架构，解决LLM在记忆方面的固有局限，通过多模态感知、动态记忆维护和自适应认知服务实现LLM记忆机制的全链重构。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临内存限制，包括受限的上下文窗口、长期知识遗忘、冗余信息积累和幻觉生成，这些问题严重限制了持续对话和个性化服务。

Method: 提出Memory Bear系统，基于认知科学原理构建类人记忆架构，整合多模态信息感知、动态记忆维护和自适应认知服务，实现LLM记忆机制的全链重构。

Result: 在医疗、企业运营和教育等领域展示了显著的工程创新和性能突破，显著提高长期对话中的知识保真度和检索效率，降低幻觉率，通过记忆-认知集成增强上下文适应性和推理能力。

Conclusion: 相比现有解决方案（如Mem0、MemGPT、Graphiti），Memory Bear在准确性、token效率和响应延迟等关键指标上表现更优，标志着AI从"记忆"向"认知"迈进的关键一步。

Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from "memory" to "cognition".

</details>


### [9] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

Main category: cs.AI

TL;DR: 本文提出了一种对抗性反馈注意力训练机制，通过动态掩码策略和策略梯度优化，让模型自动重新分配注意力权重到任务相关的关键词，解决了传统Transformer模型在情感分析中过度关注常见词而忽略重要低频词的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的情感分析模型虽然能捕捉上下文信息，但在某些场景下准确率不理想。研究发现这些模型倾向于将注意力主要分配给常见词汇，而忽略了那些不常见但对任务高度相关的术语，这严重影响了整体性能。

Method: 提出对抗性反馈注意力训练机制：1）采用动态掩码策略，尝试掩码不同词汇来欺骗判别器；2）判别器努力检测这些掩码引起的重要差异；3）利用Transformer对token级扰动的敏感性，使用策略梯度方法优化注意力分布，实现高效快速收敛。

Result: 在三个公开数据集上的实验表明，该方法取得了最先进的结果。此外，将该训练机制应用于增强大型语言模型的注意力，带来了12.6%的额外性能提升。

Conclusion: 提出的对抗性反馈注意力训练机制能有效解决Transformer模型在情感分析中的注意力分配问题，通过自动重新分配注意力到适当焦点，无需人工标注，显著提升了模型性能，并可推广应用到大型语言模型中。

Abstract: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [10] [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)
*Yiqing Ma,Jung-Hua Liu*

Main category: cs.AI

TL;DR: 该研究通过三个实验量化了LLMs的行为缺陷：懒惰性（复杂指令执行不完整）、解码次优性（短视解码）和上下文退化（长对话中遗忘指令）。研究发现LLMs普遍存在懒惰问题，但解码次优性证据有限，且在长对话中表现出意外的上下文保持能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常表现出行为缺陷，如懒惰（过早截断响应或不完全遵守多部分请求）、解码次优性（由于短视解码而未能选择更高质量的序列）和上下文退化（在长对话中遗忘或忽略核心指令）。本研究旨在通过受控实验量化这些现象。

Method: 进行了三个受控实验（A、B、C），在多个先进LLMs（OpenAI GPT-4变体、DeepSeek）上测试：实验A评估懒惰性（复杂多部分指令的遵守情况）；实验B测试解码次优性（简单推理任务中的贪婪解码表现）；实验C检验上下文退化（200轮混乱对话中的指令保持能力）。

Result: 1. 普遍存在懒惰性：模型经常省略必要部分或未能满足长度要求；2. 解码次优性证据有限：在简单推理任务中，模型的贪婪答案与其最高置信度解决方案一致；3. 上下文保持能力意外强：在200轮混乱对话测试中，模型保持关键事实和指令的能力远超预期。

Conclusion: 现代LLMs在遵守详细指令方面仍面临挑战，但可能内部缓解了一些假设的失败模式（如上下文遗忘）。研究讨论了可靠性影响，建议采用自我优化和动态提示等策略来减少懒惰性并增强多指令遵守能力。

Abstract: Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.

</details>


### [11] [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)
*Daan Di Scala,Sophie Lathouwers,Michael van Bekkum*

Main category: cs.AI

TL;DR: 该立场论文指出可信人工智能（TAI）中功能性TAI（FTAI）与规范性TAI（NTAI）之间存在鸿沟，提出需要建立语义语言作为桥梁来评估AI系统的可信度。


<details>
  <summary>Details</summary>
Motivation: 随着法规和功能优势的推动，可信人工智能（TAI）日益受到关注。然而，功能性TAI（关注如何实现可信系统）与规范性TAI（关注需要执行的法规）之间存在差距，这使得评估AI系统的可信度变得困难。

Method: 作者提出引入概念性语义语言作为桥梁，匹配FTAI和NTAI。这种语义语言可以作为框架帮助开发者评估AI系统的可信度，并帮助利益相关者将规范和法规转化为具体的实施步骤。

Result: 论文描述了当前最先进的现状，识别了FTAI和NTAI之间的差距，讨论了开发语义语言的起点及其预期效果，并提供了关键考虑因素。

Conclusion: 需要建立语义语言作为桥梁来弥合FTAI和NTAI之间的鸿沟，以促进可信人工智能的评估。论文提供了未来行动的关键考虑因素和方向。

Abstract: Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.

</details>


### [12] [From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education](https://arxiv.org/abs/2512.20714)
*Iman Reihanian,Yunfei Hou,Qingquan Sun*

Main category: cs.AI

TL;DR: 这篇综述分析了2023-2025年间32项研究，探讨生成式AI在高等教育计算机科学教育中的个性化应用效果，识别了五种应用领域和成功设计模式，提出了探索优先的采用框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI能够实现大规模个性化计算机科学教育，但需要研究这种个性化是支持还是削弱学习效果，以及如何有效设计和实施。

Method: 采用范围综述方法，从259条记录中有目的地抽样32项研究（2023-2025年），分析个性化机制和有效性信号，识别应用领域和设计选择对学习结果的影响。

Result: 识别了五个应用领域：智能辅导、个性化材料、形成性反馈、AI增强评估和代码审查；发现包含解释优先指导、解决方案保留、分级提示阶梯和工件基础的设计比无约束聊天界面效果更好；成功实施具有四种模式：基于学生工件的上下文感知辅导、需要反思的多级提示结构、与传统CS基础设施结合、人工参与的质量保证。

Conclusion: 生成式AI可以作为精确支架机制，但需要嵌入可审计的工作流程中，保持生产性挣扎的同时扩展个性化支持；提出了探索优先的采用框架，强调试点、工具化、学习保护默认设置和基于证据的扩展；需要应对学术诚信、隐私、偏见和过度依赖等风险。

Abstract: Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

</details>


### [13] [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)
*Haipeng Luo,Huawen Feng,Qingfeng Sun,Can Xu,Kai Zheng,Yufei Wang,Tao Yang,Han Hu,Yansong Tang,Di Wang*

Main category: cs.AI

TL;DR: AgentMath是一个将语言模型推理能力与代码解释器计算精度相结合的智能体框架，用于高效解决复杂数学问题，在多个数学竞赛基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（如o3和DeepSeek-R1）在自然语言推理方面取得了显著进展，但在解决需要复杂数学运算的问题时仍然计算效率低下且准确性不足。需要一种能够结合语言模型推理能力和代码解释器计算精度的解决方案。

Method: 提出了三个关键创新：1）将自然语言思维链自动转换为结构化工具增强轨迹的方法，生成高质量监督微调数据；2）新颖的智能体强化学习范式，动态交错自然语言生成与实时代码执行；3）高效的训练系统，包含请求级异步rollout调度、智能体部分rollout和前缀感知加权负载均衡等技术。

Result: AgentMath在AIME24、AIME25和HMMT25等具有挑战性的数学竞赛基准测试中达到最先进性能。AgentMath-30B-A3B分别获得90.6%、86.4%和73.8%的准确率，实现了4-5倍的训练加速。

Conclusion: 该方法验证了将语言模型推理与代码执行相结合的有效性，为构建更复杂和可扩展的数学推理智能体铺平了道路。

Abstract: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.

</details>


### [14] [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)
*Rashmeet Kaur Nayyar,Naman Shah,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 该论文提出了一种新的强化学习方法，用于处理具有参数化动作空间的顺序决策问题，通过在线学习状态和动作抽象来提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的顺序决策通常涉及参数化动作空间，需要同时处理离散动作选择和连续动作参数决策。现有方法存在严重限制：规划方法需要手工制作动作模型，标准强化学习算法要么针对离散动作要么针对连续动作，而少数处理参数化动作的强化学习方法通常依赖领域特定工程，未能充分利用这些空间的潜在结构。

Method: 提出了一种抽象驱动的强化学习方法，使智能体能够在线自主学习状态和动作抽象。引入的算法在学习过程中逐步细化这些抽象，在状态-动作空间的关键区域增加细粒度细节，从而提高性能。

Result: 在多个连续状态、参数化动作的领域中，该抽象驱动方法使TD(λ)算法实现了比最先进基线方法显著更高的样本效率。

Conclusion: 该方法成功扩展了强化学习算法的适用范围，使其能够处理具有参数化动作空间的长时程、稀疏奖励设置，通过自主学习抽象表示提高了学习效率和性能。

Abstract: Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.

</details>


### [15] [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)
*Onat Ozer,Grace Wu,Yuchen Wang,Daniel Dosti,Honghao Zhang,Vivi De La Rue*

Main category: cs.AI

TL;DR: 论文提出使用多智能体多角色辩论方法替代单一LLM自我反思，以解决LLM在推理任务中重复相同错误的退化问题，在HotPot QA和HumanEval上取得更好性能


<details>
  <summary>Details</summary>
Motivation: LLM在推理任务中通过反思错误可以提升性能，但单一LLM的持续自我反思会导致思维退化，重复相同错误，即使知道自己是错的

Method: 引入多智能体多角色辩论作为生成反思的方法，通过不同角色和视角的辩论来产生更多样化的反思

Result: 在HotPot QA上达到47% EM准确率，在HumanEval上达到82.7%准确率，均超越了单一LLM的反思方法

Conclusion: 多智能体多角色辩论方法能产生更多样化的反思，有效解决单一LLM自我反思的退化问题，在复杂推理和编程任务上表现更优

Abstract: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.

</details>


### [16] [The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents](https://arxiv.org/abs/2512.20884)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 提出概率框架解决LLM智能体知识交换中的认知不对称问题，通过Beta-Bernoulli分布建模信念，引入遗忘因子和认知缓存，实现基于不确定性的双向知识共享


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM和RAG的自主智能体存在认知不对称问题，只能单向消费数字内容，导致冗余推理和集体智能停滞。现有自反思框架缺乏概率基础来量化确定性或证明外部交互的合理性。

Method: 提出形式化概率框架：1) 使用带遗忘因子γ的Beta-Bernoulli分布建模智能体对命题的信念；2) 将认知不确定性量化为信念方差；3) 建立双向交互的双重驱动：稳态动机（维持确定性）和最优学习策略（针对最大模糊点）；4) 引入认知缓存动态优先处理非平稳知识分布；5) 将积累的信念状态用作RLHF的可验证奖励信号和SFT的高质量数据过滤器。

Result: 模拟结果表明，这种不确定性驱动策略在异构（Zipfian）环境中显著优于随机基线，保持对概念漂移的高适应性。公共贡献被重新定义为最优主动学习：分享解决方案以获取反馈是智能体减少自身不确定性的最有效方法。

Conclusion: 该概率框架为智能体提供了非利他性的双向知识交换动机，通过量化认知不确定性和动态资源分配，解决了认知不对称问题，促进了集体智能的发展，并为RLHF和SFT提供了可验证的奖励信号和数据过滤机制。

Abstract: Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.

</details>


### [17] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: 本文提出了一种结合LangChain多智能体系统和许可区块链的架构，用于确保自主AI系统的监控、策略执行和不可篡改审计，在智能库存管理、交通信号控制和医疗监控等场景中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统在医疗、智慧城市、数字取证和供应链管理等领域的应用日益增长，虽然这些系统具有灵活性和实时推理能力，但也引发了信任、监督以及信息与活动完整性方面的担忧。

Method: 提出单一架构模型，结合LangChain多智能体系统和许可区块链，实现持续监控、策略执行和智能体行为的不可篡改审计。该框架将感知-概念化-行动周期与区块链治理层关联，验证输入、评估推荐行动并记录执行结果。具体实现包括基于Hyperledger Fabric的系统、MCP集成的行动执行器和LangChain智能体。

Result: 实验在智能库存管理、交通信号控制和医疗监控等场景中进行。结果表明，区块链安全验证能有效防止未经授权的操作，提供整个决策过程的可追溯性，并将操作延迟保持在合理范围内。

Conclusion: 该框架为实施高影响力的自主AI应用提供了一个通用系统，既能保持自主性又能确保责任性，实现了自主与责任的平衡。

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


### [18] [FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning](https://arxiv.org/abs/2512.20991)
*Toqeer Ali Syed,Abdulaziz Alshahrani,Ali Ullah,Ali Akarma,Sohail Khan,Muhammad Nauman,Salman Jan*

Main category: cs.AI

TL;DR: 本文提出了一种价格感知的智能AI系统，结合个人财务管理与饮食优化，为中等收入家庭提供营养充足且价格合理的膳食计划，能根据市场价格波动自动调整。


<details>
  <summary>Details</summary>
Motivation: 中等收入环境下，有限的家庭预算与营养需求之间的矛盾日益突出，食品价格波动加剧了这一挑战。需要一种能够平衡经济可负担性与营养充足性的解决方案。

Method: 采用模块化多智能体架构，包含预算、营养、价格监控和健康个性化等特定智能体。这些智能体共享知识库，使用替代图来确保在最低成本下维持营养质量。

Result: 在沙特代表性家庭案例研究中，系统相比静态周菜单实现了12-18%的成本降低，营养充足率超过95%，在20-30%的价格变化下表现出高性能。

Conclusion: 该框架能够在本地结合经济可负担性与营养充足性，为实现可持续和公平的饮食规划提供可行途径，符合可持续发展目标中的零饥饿和良好健康目标。

Abstract: The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.

</details>


### [19] [TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control](https://arxiv.org/abs/2512.20996)
*Yuwei Du,Jun Zhang,Jie Feng,Zhicheng Liu,Jian Yuan,Yong Li*

Main category: cs.AI

TL;DR: TrafficSimAgent是一个基于LLM的智能体框架，通过专家级协作简化交通仿真实验，让非专业用户也能轻松进行仿真任务


<details>
  <summary>Details</summary>
Motivation: 现有交通仿真平台（如SUMO、MATSim）功能全面但使用门槛高，非专业用户难以从零开始进行实验并将其应用于日常工作

Method: 提出基于LLM的专家智能体框架，采用跨层级协作：高层专家智能体理解自然语言指令、规划实验流程、按需调用MCP兼容工具；低层专家智能体基于实时交通状况为基本元素选择最优行动方案

Result: 多场景实验表明，TrafficSimAgent能在各种条件下有效执行仿真，即使在用户指令模糊时也能产生合理结果；其专家级自主决策优化优于其他系统和SOTA LLM方法

Conclusion: TrafficSimAgent通过LLM驱动的专家智能体协作，显著降低了交通仿真的使用门槛，为非专业用户提供了灵活高效的仿真解决方案

Abstract: Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.

</details>


### [20] [LLM Personas as a Substitute for Field Experiments in Method Benchmarking](https://arxiv.org/abs/2512.21080)
*Enoch Hyunwook Kang*

Main category: cs.AI

TL;DR: 论文证明：在聚合观察和算法盲评估条件下，用LLM模拟人物替代真实人类进行A/B测试是有效的基准替代方案，且区分不同方法所需的人物评估次数可通过信息论方法量化。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的A/B测试成本高、延迟大，限制了迭代方法开发。LLM人物模拟提供了廉价替代方案，但需要验证这种替代是否保持基准接口的有效性。

Method: 提出充要条件特征：当(1)方法仅观察聚合结果，(2)评估仅依赖提交的成果而非算法身份时，人物替换只是面板变更。通过信息论方法定义聚合通道的可区分性，推导出可靠区分不同方法所需的人物评估次数界限。

Result: 证明在特定条件下，LLM人物模拟可以替代真实人类进行基准测试，且通过计算所需样本量可以确保模拟基准与实地实验具有相同的决策相关性。

Conclusion: LLM人物模拟可以作为有效的基准测试替代方案，特别是在聚合观察和算法盲评估条件下。区分不同方法所需的人物评估次数可通过理论界限确定，为廉价、快速的迭代方法开发提供了理论基础。

Abstract: Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.

</details>


### [21] [Beyond Context: Large Language Models Failure to Grasp Users Intent](https://arxiv.org/abs/2512.21110)
*Ahmed M. Hussain,Salahuddin Salahuddin,Panos Papadimitratos*

Main category: cs.AI

TL;DR: 当前大语言模型安全机制存在重大漏洞：无法理解上下文和识别用户意图，导致恶意用户可通过情感框架、渐进揭示和学术论证等系统方法绕过安全防护


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全方法主要关注显性有害内容，但忽视了关键漏洞：无法理解上下文和识别用户意图。这为恶意用户创造了可系统性利用的漏洞来绕过安全机制

Method: 对多个最先进的LLM（包括ChatGPT、Claude、Gemini和DeepSeek）进行实证评估，分析通过情感框架、渐进揭示和学术论证技术绕过可靠安全机制的方法

Result: 分析表明，推理增强配置不仅没有减轻反而放大了利用效果，提高了事实精确性但未能质疑潜在意图。Claude Opus 4.1是例外，在某些用例中优先考虑意图检测而非信息提供

Conclusion: 当前架构设计存在系统性漏洞，需要范式转变：将上下文理解和意图识别作为核心安全能力，而非事后保护机制

Abstract: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.

</details>


### [22] [A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care](https://arxiv.org/abs/2512.21127)
*Oliver Normand,Esther Borsi,Mitch Fruin,Lauren E Walker,Jamie Heagerty,Chris C. Holmes,Anthony J Avery,Iain E Buchan,Harry Coppock*

Main category: cs.AI

TL;DR: 该研究首次在真实NHS初级保健数据上评估LLM药物安全审查系统，发现虽然系统在识别临床问题方面表现良好（灵敏度100%），但仅在46.9%的患者中正确识别所有问题和干预措施，主要失败机制是情境推理而非药物知识缺失。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在医学基准测试中常达到或超过临床医生水平，但很少在真实临床数据上进行评估或超越表面指标进行深入分析。研究旨在评估LLM在真实NHS初级保健数据中的表现，并详细描述不同临床复杂性下的关键失败行为。

Method: 回顾性研究使用覆盖NHS Cheshire和Merseyside地区2,125,549名成人的大规模电子健康记录，战略抽样捕获广泛的临床复杂性和药物安全风险，经数据质量排除后得到277名患者。专家临床医生审查这些患者，对系统识别的问题和提出的干预措施进行分级。

Result: 主要LLM系统在识别临床问题存在方面表现强劲（灵敏度100%，特异性83.1%），但仅在46.9%的患者中正确识别所有问题和干预措施。失败分析揭示主要失败机制是情境推理而非药物知识缺失，包括五大模式：对不确定性的过度自信、未根据患者情境调整标准指南、误解医疗实践方式、事实错误和流程盲点。

Conclusion: 该研究突出了LLM临床AI安全部署前必须解决的缺陷，表明需要进行更大规模的前瞻性评估和更深入的LLM临床行为研究。失败模式在不同患者复杂性和人口统计学层面以及各种最先进模型和配置中持续存在。

Abstract: Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.

</details>


### [23] [RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic](https://arxiv.org/abs/2512.21220)
*Le Wang,Zonghao Ying,Xiao Yang,Quanchen Zou,Zhenfei Yin,Tianlin Li,Jian Yang,Yaodong Yang,Aishan Liu,Xianglong Liu*

Main category: cs.AI

TL;DR: RoboSafe：一种用于具身智能体的混合推理运行时安全防护系统，通过可执行的基于谓词的安全逻辑，结合后向反思推理和前向预测推理，显著减少危险行为


<details>
  <summary>Details</summary>
Motivation: 基于视觉语言模型的具身智能体在执行复杂现实任务时容易受到危险指令的影响，而现有的运行时安全防护方法（如静态规则过滤器或提示级控制）难以应对动态、时间依赖和上下文丰富的环境中出现的隐式风险

Method: 提出RoboSafe系统，包含两个互补的推理模块：1）后向反思推理模块，持续回顾短期记忆中的近期轨迹以推断时间安全谓词，并在检测到违规时主动触发重新规划；2）前向预测推理模块，通过从长期安全记忆和智能体的多模态观察中生成上下文感知的安全谓词来预测即将到来的风险。这些组件在混合长短安全记忆上运行，形成自适应、可验证的安全逻辑

Result: 在多个智能体上的广泛实验表明，与领先的基线方法相比，RoboSafe显著减少了危险行为（风险发生率降低36.8%），同时保持了接近原始的任务性能。物理机械臂上的真实世界评估进一步证实了其实用性

Conclusion: RoboSafe通过混合推理方法为具身智能体提供了有效、自适应且可验证的运行时安全防护，能够处理动态环境中的隐式风险，同时保持任务性能，具有实际应用价值

Abstract: Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.

</details>

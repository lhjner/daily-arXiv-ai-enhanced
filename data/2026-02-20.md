<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 43]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE本体论为法医牙科年龄评估提供了一个标准化的语义框架，整合人工和AI辅助工作流程，旨在提高透明度和可重复性。


<details>
  <summary>Details</summary>
Motivation: 年龄评估在法医和司法决策中至关重要，特别是在涉及无证件个人和无人陪伴未成年人的案件中。当前牙科年龄评估实践面临方法异质性、数据表示碎片化以及临床、法医和法律信息系统之间互操作性有限等挑战，这些问题阻碍了透明度和可重复性，而AI方法的采用进一步放大了这些限制。

Method: 开发AIdentifyAGE本体论，这是一个领域特定的标准化语义框架。该本体论建模完整的法医法律工作流程，整合司法背景、个体信息、法医检查数据、牙齿发育评估方法、放射影像、统计参考研究和基于AI的估计方法。它基于上层和已建立的生物医学、牙科和机器学习本体论构建，确保与FAIR原则的互操作性、可扩展性和合规性。

Result: AIdentifyAGE本体论提供了一个标准化的语义框架，能够追踪观察、方法、参考数据和报告结果之间的可追溯链接。它整合了人工和AI辅助的法医牙科年龄评估工作流程，为法医法律和司法背景中的本体驱动决策支持系统建立了坚实基础。

Conclusion: AIdentifyAGE本体论是提高法医牙科年龄评估一致性、透明度和可解释性的重要步骤，为法医法律和司法背景中的本体驱动决策支持系统建立了坚实基础，有助于增强透明度和可重复性。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [2] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 单状态重用在经典概率表示中必然导致语境性，这是自适应智能的普遍表示约束


<details>
  <summary>Details</summary>
Motivation: 自适应系统常在多个上下文中运行，但受内存、表示或物理资源限制而重用固定内部状态空间。这种单状态重用普遍存在于自然和人工智能中，但其基本表示后果尚不清楚。

Method: 将上下文建模为作用于共享内部状态的干预，证明任何再现语境性结果统计的经典模型都必须承担不可约的信息论代价：对上下文的依赖不能仅通过内部状态来中介。提供最小构造性示例来明确实现这一代价并澄清其操作意义。

Result: 证明语境性不是量子力学的特性，而是经典概率表示中单状态重用的必然结果。非经典概率框架通过放宽单一全局联合概率空间的假设来避免这种障碍，无需引用量子动力学或希尔伯特空间结构。

Conclusion: 语境性是自适应智能的普遍表示约束，与物理实现无关。单状态重用在经典表示中必然导致语境性，而非经典框架可以避免这种限制。

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [3] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache是一个用于大规模人类移动模拟的缓存框架，通过可重构缓存和轻量级解码器显著提升效率，同时保持与最先进LLM方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的人类移动模拟方法虽然能生成真实行为，但计算成本高昂，限制了大规模应用。需要一种既能保持模拟保真度又能显著提升效率的解决方案。

Method: MobCache包含两个核心组件：1) 推理组件将推理步骤编码为潜在空间嵌入，使用潜在空间评估器实现推理步骤的重用和重组；2) 解码组件采用轻量级解码器，通过移动规律约束的蒸馏训练，将潜在空间推理链转换为自然语言。

Result: 实验表明，MobCache在多个维度上显著提升了效率，同时保持了与最先进的基于LLM方法相当的性能表现。

Conclusion: MobCache通过创新的缓存框架设计，成功解决了大规模人类移动模拟中的效率瓶颈问题，为城市规划、流行病学和交通分析等应用提供了可行的解决方案。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [4] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 研究分析了60个大型语言模型基准测试的饱和现象，发现近半数基准已饱和，且饱和率随基准年龄增长而增加。专家策划的基准比众包基准更抗饱和，而隐藏测试数据并无保护效果。


<details>
  <summary>Details</summary>
Motivation: AI基准测试在衡量模型进展和指导部署决策中起核心作用，但许多基准测试很快饱和，无法区分最佳性能模型，降低了其长期价值。需要了解基准饱和现象及其驱动因素。

Method: 从主要模型开发商的技术报告中选取60个LLM基准测试，从任务设计、数据构建和评估格式三个维度定义14个属性特征，测试5个假设来检验每个属性对饱和率的影响。

Result: 近半数基准测试表现出饱和现象，饱和率随基准年龄增长而增加。隐藏测试数据（公开vs私有）没有保护效果，而专家策划的基准比众包基准更能抵抗饱和。

Conclusion: 研究结果揭示了哪些设计选择能延长基准测试的寿命，为构建更持久的评估策略提供了信息，有助于指导未来基准测试的设计。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [5] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 代码进化技术虽然表现优异，但往往缺乏与简单基线的比较。研究发现，在数学界限优化、智能体框架设计和机器学习竞赛三个领域中，简单基线方法都能匹配甚至超越复杂方法。


<details>
  <summary>Details</summary>
Motivation: 许多代码进化技术展示了令人印象深刻的性能，但缺乏与简单基线的系统比较。研究者希望验证简单基线方法在不同领域的实际效果，并分析代码进化技术开发和使用中的缺陷。

Method: 研究在两个简单基线上测试了三个领域：1）寻找更好的数学界限；2）设计智能体框架；3）机器学习竞赛。通过分析结果，识别代码进化技术的问题，并提出改进的评估方法。

Result: 简单基线方法在所有三个领域都匹配或超越了更复杂的方法。在数学界限优化中，搜索空间和领域知识比进化管道更重要；在智能体框架设计中，高方差和小数据集导致次优选择；提出了减少评估随机性的方法。

Conclusion: 代码进化技术的主要挑战在于设计好的搜索空间而非搜索过程本身。需要更好的评估方法和最佳实践来提高代码进化研究的严谨性，减少评估随机性，确保经济可行性。

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [6] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 该论文研究了n维超立方体边被超平面切割的最小超平面数问题，改进了已有上界，并利用AI工具发现了新的构造。


<details>
  <summary>Details</summary>
Motivation: 研究n维超立方体Q_n的边被超平面切割的最小超平面数S(n)问题。这个问题在组合几何和离散几何中有重要意义，自1971年Paterson给出S(n) ≤ ⌈5n/6⌉的上界以来，一直未有显著改进。

Method: 1. 构造了8个超平面切割Q_10的实例
2. 利用新开发的AI工具CPro1：结合推理大语言模型和自动超参数调优，创建搜索算法来发现数学构造
3. 基于Q_10的构造推导出一般n维情况的上界

Result: 1. 证明了S(n) ≤ ⌈4n/5⌉（当n不是5的奇数倍时）
2. 当n是5的奇数倍时，S(n) ≤ 4n/5 + 1
3. 改进了Paterson 1971年的上界S(n) ≤ ⌈5n/6⌉
4. 获得了k<n个超平面能切割的最大边数的新下界

Conclusion: 该研究显著改进了超立方体边切割问题的最优上界，并展示了AI辅助数学发现工具CPro1在构造性组合问题中的有效性，为未来类似问题的研究提供了新方法。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [7] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: 本文提出"节点学习"这一去中心化学习范式，将智能置于边缘节点，通过选择性对等交互扩展，而非依赖集中式数据中心。


<details>
  <summary>Details</summary>
Motivation: AI向边缘扩展暴露了集中式智能的成本和脆弱性：数据传输、延迟、能耗以及对大型数据中心的依赖在异构、移动和资源受限环境中扩展性差。

Method: 节点学习是一种去中心化学习范式，节点从本地数据持续学习，维护自己的模型状态，在有益时通过机会性对等交互交换知识。学习通过重叠和扩散传播，而非全局同步或集中聚合。

Result: 该概念论文建立了这一范式的概念基础，对比了现有去中心化方法，并探讨了对通信、硬件、信任和治理的影响。

Conclusion: 节点学习不抛弃现有范式，而是将其置于更广泛的去中心化视角中，统一了自主和协作行为，适应数据、硬件、目标和连接性的异构性。

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [8] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文提出了一种基于序理论的犹豫模糊集评分统一框架，证明了经典序不构成格结构，但对称序满足评分函数的关键规范准则，并引入了优势函数用于犹豫模糊元素排序。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏序理论的形式化基础，需要建立更灵活、一致的评分机制框架。

Method: 1. 提出基于给定序的统一评分框架；2. 分析犹豫模糊元素上的经典序结构；3. 证明对称序满足强单调性和Gärdenfors条件等规范准则；4. 引入优势函数用于相对比较；5. 提供离散优势函数和相对优势函数两个具体实例。

Result: 1. 经典序不诱导格结构；2. 对称序定义的评分满足评分函数的关键规范准则；3. 优势函数可用于构建犹豫模糊集上的模糊偏好关系并支持群体决策。

Conclusion: 基于序理论的统一框架为犹豫模糊集评分提供了形式化基础，优势函数方法增强了决策支持能力，为犹豫模糊环境下的决策分析提供了新工具。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [9] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage是首个能让LLM自动创建具有自生成拓扑结构和工具集的智能体开发套件，提供结构化内存支持，在多个基准测试中优于现有ADK


<details>
  <summary>Details</summary>
Motivation: 当前智能体开发套件要么功能支持不足，要么依赖人工设计拓扑、工具和内存组件，限制了智能体的泛化能力和整体性能

Method: OpenSage让LLM自动创建和管理子智能体及工具集，采用分层图结构内存系统，并提供专门针对软件工程任务的工具包

Result: 在三个最先进的基准测试中，使用不同骨干模型的广泛实验表明OpenSage优于现有ADK，消融研究验证了各组件设计的有效性

Conclusion: OpenSage为下一代智能体开发铺平道路，将焦点从以人为中心转向以AI为中心的模式

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [10] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: AgentLAB是首个专门评估LLM智能体对自适应、长视野攻击脆弱性的基准测试，包含5种新型攻击类型、28个真实环境、644个安全测试用例，发现现有防御措施无法有效缓解长视野威胁。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在复杂长视野环境中部署增加，它们面临利用多轮交互的长视野攻击风险，而现有评估主要关注单轮攻击，缺乏针对多轮交互攻击的系统性评估基准。

Method: 开发AgentLAB基准测试，支持意图劫持、工具链攻击、任务注入、目标漂移和内存中毒等5种新型攻击类型，覆盖28个真实智能体环境，包含644个安全测试用例，用于评估LLM智能体在长视野攻击下的脆弱性。

Result: 评估发现代表性LLM智能体对长视野攻击高度脆弱，为单轮交互设计的防御措施无法可靠缓解长视野威胁，智能体在多轮交互中容易受到系统性攻击。

Conclusion: AgentLAB为跟踪LLM智能体安全进展提供了有价值的基准测试，揭示了长视野攻击的严重威胁，需要开发专门针对多轮交互安全的新防御机制。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [11] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace是一个评估大语言模型规划、推理和世界知识的基准测试，要求模型通过维基百科超链接从源页面逐步导航到目标页面。前沿模型在简单任务上表现超人类水平，但在困难任务上性能大幅下降，最佳模型成功率仅23%，揭示了当前推理系统的明显局限性。


<details>
  <summary>Details</summary>
Motivation: 需要评估大语言模型在规划、推理和世界知识方面的能力，特别是它们在多步导航任务中的表现，以揭示当前推理系统的局限性和需要改进的方向。

Method: 创建LLM-Wikirace基准测试，要求模型通过维基百科超链接逐步导航从源页面到目标页面。评估了包括Gemini-3、GPT-5和Claude Opus 4.5在内的广泛开源和闭源模型，分析不同难度级别的表现，并进行轨迹级分析。

Result: 前沿模型在简单任务上表现超人类水平，但在困难任务上性能大幅下降：最佳模型Gemini-3在困难游戏中的成功率仅为23%。分析显示世界知识是成功的必要条件，但超过一定阈值后，规划和长视野推理能力成为主导因素。轨迹分析显示即使最强模型在失败后也难以重新规划，经常陷入循环。

Conclusion: LLM-Wikirace是一个简单但有效的基准测试，揭示了当前推理系统在规划、长视野推理和失败恢复方面的明显局限性。该基准为规划能力强的LLMs提供了一个开放的竞技场，表明它们仍有很大的改进空间。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [12] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: 研究发现，在多模态视觉语言模型上进行窄域有害数据的微调会导致严重的泛化性错位，即使在训练数据中仅含10%有害内容也会引发显著的对齐退化，且多模态评估比文本评估更能揭示错位程度。


<details>
  <summary>Details</summary>
Motivation: 终身多模态智能体需要通过后训练持续适应新任务，但这在获取能力与保持安全对齐之间产生了根本性矛盾。研究者希望探究对齐的视觉语言模型在窄域有害数据集上微调后，是否会产生广泛泛化的错位行为。

Method: 使用Gemma3-4B模型进行实验，通过LoRA（低秩适应）在不同秩上进行微调，评估微调后模型在无关任务和模态上的错位程度。采用几何分析方法探究有害行为的低维子空间特性，并评估两种缓解策略：良性窄域微调和基于激活的引导。

Result: 1）错位程度随LoRA秩单调增加；2）多模态评估显示错位程度（70.71±1.22，r=128）远高于文本评估（41.19±2.51）；3）即使训练混合中仅含10%有害数据也会导致显著对齐退化；4）几何分析显示有害行为占据极低维子空间，10个主成分即可捕获大部分错位信息；5）两种缓解策略虽能大幅减少错位，但无法完全消除已学习的有害行为。

Conclusion: 当前的后训练范式可能无法在部署后环境中充分保持对齐，凸显了需要开发更鲁棒的持续学习框架，以平衡能力获取与安全对齐的冲突。

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [13] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 论文挑战了AI系统黑盒安全评估的基本假设，证明对于依赖未观测内部变量的模型，任何黑盒评估器都无法可靠估计部署风险，并建立了统计和计算上的根本限制。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全评估假设模型在测试分布上的行为能可靠预测部署性能，但该假设忽略了模型可能依赖未观测的内部变量，这些变量在评估中罕见但在部署中普遍存在，导致评估失效。

Method: 通过潜在上下文条件策略的形式化分析，使用Le Cam方法证明被动评估的最小最大下界，基于哈希的触发构造和Yao最小最大原理分析自适应评估，在陷门单向函数假设下展示计算分离，并提供白盒探测的样本复杂度分析。

Result: 被动评估的期望绝对误差≥0.208δL；自适应评估在最坏情况下误差≥δL/16；计算上部署环境可激活不安全行为而多项式时间评估器无法区分；白盒探测需要O(1/(γ²ε_R²))样本。

Conclusion: 黑盒测试在统计上可能无法确定安全性，需要额外的保障措施如架构约束、训练时保证、可解释性和部署监控来确保最坏情况下的安全保证。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [14] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe是一个用于股票推荐的对话式纵向基准测试，它超越了传统的行为模仿评估，通过多视角参考区分描述性行为和基于投资者风险偏好的规范性效用。


<details>
  <summary>Details</summary>
Motivation: 传统推荐基准主要评估模型模仿用户行为的能力，但在金融咨询领域，观察到的用户行为可能在市场波动下存在噪音或短视，与用户的长期目标相冲突。将用户选择作为唯一真实标准会混淆行为模仿和决策质量。

Method: 构建Conv-FinRe基准：基于真实市场数据和人类决策轨迹，创建包含入职访谈、逐步市场情境和咨询对话的对话式纵向评估框架。模型需要在固定投资期限内生成股票排名。基准提供多视角参考，区分描述性行为（用户实际选择）和规范性效用（基于投资者特定风险偏好）。

Result: 评估多个最先进的LLM发现，理性决策质量和行为对齐之间存在持续紧张关系：在基于效用的排名上表现良好的模型往往无法匹配用户选择，而行为对齐的模型可能过度拟合短期噪音。

Conclusion: Conv-FinRe基准为金融推荐系统提供了更全面的评估框架，能够诊断LLM是遵循理性分析、模仿用户噪音还是受市场动量驱动。数据集已在Hugging Face公开，代码库在GitHub可用。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [15] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS是一个神经符号框架，通过"搜索-验证"管道解决时间序列数据库的自然语言查询问题，使用SQL搜索候选窗口，然后用Python程序验证原始信号。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法无法处理连续形态意图（如形状或异常），而时间序列模型难以处理超长历史记录，需要专门解决时间序列数据库自然语言查询的框架。

Method: 提出Sonar-TS神经符号框架，采用类似主动声纳的"搜索-验证"管道：首先使用特征索引通过SQL搜索候选窗口，然后生成Python程序锁定并验证候选窗口与原始信号的匹配。

Result: 创建了首个大规模基准测试NLQTSBench，实验表明Sonar-TS能有效处理复杂时间查询，而传统方法在此领域表现不佳。

Conclusion: 这是对时间序列数据库自然语言查询的首次系统研究，提供了一个通用框架和评估标准，为未来研究奠定了基础。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [16] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder是一个两阶段匹配系统，通过快速初步筛选和精确公平度评估，解决异质技能水平团队间的公平匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现代多人游戏中，基于平均团队技能的匹配方法在技能分布广泛或偏斜时会导致不平衡的对局，影响玩家留存和满意度，需要更公平的匹配系统。

Method: 采用两阶段方法：第一阶段使用Ruzicka相似度指数快速筛选"非异常值"技能范围；第二阶段将玩家等级映射到基于反正态分布的非线性技能桶，使用Kantorovich距离计算"制裁分数"评估匹配公平性。

Result: 通过分析1.4亿个模拟团队配对中制裁分数的分布，验证了系统的可行性，为公平匹配阈值提供了坚实基础。

Conclusion: Cinder系统能够提供快速且公平的匹配，解决了异质技能水平团队间的匹配挑战，为多人游戏匹配系统提供了有效解决方案。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [17] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA是一个多智能体计算机使用框架，通过意图对齐的计划记忆来稳定长时程执行，在桌面自动化任务中实现了74.83%的成功率和0.91的步骤效率比。


<details>
  <summary>Details</summary>
Motivation: 现有方法（从基于RL的规划器到轨迹检索）在长时程、噪声感知、多窗口上下文和动态环境状态下，经常偏离用户意图并重复解决常规子问题，导致错误累积和效率低下。

Method: 提出IntentCUA多智能体框架，包含规划器、计划优化器和批评器，通过共享记忆将原始交互轨迹抽象为多视图意图表示和可重用技能。意图原型检索子组对齐的技能并将其注入部分计划中。

Result: 在端到端评估中，IntentCUA实现了74.83%的任务成功率和0.91的步骤效率比，优于基于RL和轨迹中心的基线方法。消融实验显示多视图意图抽象和共享计划记忆共同提高了执行稳定性。

Conclusion: 系统级意图抽象和基于记忆的协调是实现大型动态环境中可靠高效桌面自动化的关键，合作式多智能体循环在长时程任务中提供了最大收益。

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [18] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: S2Q通过连续子价值Q学习，学习多个子价值函数来保留替代高价值动作，使用基于Softmax的行为策略促进持续探索，在合作多智能体强化学习中表现出更好的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有价值分解方法依赖于单一最优动作，当基础价值函数在训练中发生变化时难以适应，容易收敛到次优策略，需要一种能保留替代高价值动作的方法来提高适应性。

Method: 提出连续子价值Q学习(S2Q)，学习多个子价值函数来保留替代高价值动作，将这些子价值函数整合到基于Softmax的行为策略中，促进持续探索并使总价值函数能快速适应变化的最优解。

Result: 在具有挑战性的MARL基准测试中，S2Q持续优于多种MARL算法，展示了改进的适应性和整体性能。

Conclusion: S2Q通过保留替代高价值动作和促进持续探索，有效解决了现有价值分解方法在动态环境中适应性不足的问题，为合作多智能体强化学习提供了更鲁棒的解决方案。

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [19] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: PBS是一种通过动态优先处理高损失样本来加速语言模型收敛的训练优化技术，使用轻量级线性预测器从静态特征估计样本难度。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习方法需要预定义难度指标，而硬样本挖掘方法需要昂贵的逐样本损失跟踪。PBS旨在开发一种轻量级、高效的方法来识别困难样本，从而加速模型收敛。

Method: PBS使用在线训练的轻量级线性预测器，仅基于四个简单的词元级静态特征（词频、序列长度、词汇多样性、稀有词元比例）来估计样本难度，动态构建优先处理高损失样本的训练批次。

Result: 预测器与实际损失的相关性达到0.44，在130M参数transformer上的实验显示，PBS实现了6-13%的收敛加速，预测器相关性在10,000训练步中从0.14提升到0.44。

Conclusion: 词频统计编码了样本难度的有意义信息，使得PBS能够以可忽略的计算开销实现有效的课程学习，加速语言模型训练收敛。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [20] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 提出一个人类-AI协作框架来生成可信的可持续发展评级基准数据集，以解决不同机构评级结果差异大的问题


<details>
  <summary>Details</summary>
Motivation: 不同可持续发展评级机构对同一公司的评分差异很大，限制了评级的可比性、可信度和决策相关性，需要统一标准

Method: 提出包含两部分的通用人类-AI协作框架：STRIDE提供原则性标准和评分系统指导构建公司级基准数据集，SR-Delta进行差异分析

Result: 该框架能够实现可持续发展评级方法的可扩展和可比评估

Conclusion: 呼吁AI社区采用AI驱动方法来加强和推进支持可持续发展议程的评级方法

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [21] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 本文提出了一种基于Owen值的改进SHAP方法，通过满足T性质的分割策略解决特征依赖问题，提升视觉任务中的特征归因精度和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 传统SHAP方法在视觉任务中假设特征独立，但像素之间存在空间和语义依赖关系，导致归因不准确。现有基于Owen值的实现依赖于分割策略，而常用分割方法（如轴对齐或SLIC）违反关键一致性属性。

Method: 提出新的分割方法，满足T性质以确保层次结构中不同层级的语义对齐。该方法支持计算剪枝，同时提高归因准确性和可解释性。基于Owen值的层次化归因框架（O-Shap）。

Result: 在图像和表格数据集上的实验表明，O-Shap在归因精度、语义一致性和运行效率方面优于基线SHAP变体，特别是在结构重要的场景中表现更佳。

Conclusion: 通过满足T性质的分割策略，O-Shap有效解决了特征依赖问题，提供了更准确、语义一致且高效的特征归因方法，为视觉任务中的可解释AI提供了改进方案。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [22] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: InstructKG框架利用课程讲义材料自动构建与教师教学意图一致的知识图谱，捕捉概念间的学习依赖关系，以支持个性化学习


<details>
  <summary>Details</summary>
Motivation: 掌握教育概念需要理解其先决条件和子概念关系，这对于识别学生知识缺口和实现个性化学习至关重要。在大规模课程中，教师难以诊断个体误解或确定需要强化的概念。现有知识图谱方法要么停留在表层概念层面，要么忽略了教学材料中丰富的教学信号。

Method: InstructKG框架从课程讲义材料（幻灯片、笔记等）中提取重要概念作为节点，并推断学习依赖关系作为有向边（如"部分-整体"或"依赖"关系）。该框架将教育材料特有的丰富时间和语义信号（如"递归"在"归并排序"之前教授；"递归"在"归并排序"定义中被提及）与大语言模型的泛化能力相结合。

Result: 通过在多个课程的真实世界多样化讲义材料上进行实验和基于人工的评估，证明InstructKG能够捕捉丰富且与教师教学意图一致的学习进展关系。

Conclusion: InstructKG为自动构建教师对齐的知识图谱提供了一个有效框架，能够捕捉课程预期的学习进展，有助于大规模个性化教育。

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [23] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 提出一种用于分解困难CircuitSAT实例的新型并行算法，通过专门约束将原始SAT实例划分为弱化公式族，参数化设计可高效识别高质量分解。


<details>
  <summary>Details</summary>
Motivation: 解决困难CircuitSAT实例（如布尔电路逻辑等价性检查和密码哈希函数原像攻击）的计算挑战，需要更高效的分解方法。

Method: 使用专门约束将原始SAT实例划分为弱化公式族，设计参数化并行算法，通过并行计算的硬度估计指导高质量分解的识别。

Result: 在具有挑战性的CircuitSAT实例上展示了算法的实际有效性，包括布尔电路逻辑等价性检查和密码哈希函数原像攻击的编码实例。

Conclusion: 提出的并行分解算法能有效处理困难CircuitSAT问题，参数化设计提供了灵活性和效率，在密码分析和电路验证等应用中具有实用价值。

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [24] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA是一个新的基因组基础模型预训练框架，结合联合嵌入预测架构和传统生成目标，通过潜在空间预测提升基因组功能上下文理解


<details>
  <summary>Details</summary>
Motivation: 现有基因组基础模型主要依赖掩码语言建模或下一标记预测，这些方法擅长捕捉局部基因组语法和精细基序模式，但往往无法捕获更广泛的功能上下文，导致表示缺乏全局生物学视角

Method: 引入JEPA-DNA框架，整合联合嵌入预测架构与生成目标，通过潜在接地将标记级恢复与潜在空间预测目标结合，监督CLS标记预测掩码基因组片段的高级功能嵌入

Result: 在多样化基因组基准测试中，JEPA-DNA在监督和零样本任务中始终优于仅生成基线，提供更稳健和生物学接地的表示

Conclusion: JEPA-DNA为理解基因组字母和序列底层功能逻辑的基础模型提供了可扩展路径

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [25] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo是一个仅含2000万参数的轻量级公式识别模型，通过精心设计、知识蒸馏和词汇表迁移，性能媲美SOTA模型，但模型大小减少了65-80%，支持实时推理和浏览器部署。


<details>
  <summary>Details</summary>
Motivation: 现有公式识别模型通常参数量大、计算成本高，难以在消费级硬件或浏览器中实时运行。需要开发一个既保持高性能又足够轻量的模型，以实现实际应用中的实时推理和便捷部署。

Method: 采用最小化设计思路，结合注意力机制优化、知识蒸馏技术以及词汇表和分词器的迁移策略，构建仅含2000万参数的紧凑模型架构。

Result: Texo在性能上与UniMERNet-T和PPFormulaNet-S等SOTA模型相当，同时模型大小分别减少了80%和65%，能够在消费级硬件上实现实时推理，并支持浏览器内部署。

Conclusion: Texo证明了通过精心设计和优化，可以在大幅减小模型规模的同时保持高性能，为公式识别任务的实际应用提供了可行的轻量级解决方案，并开发了演示应用促进用户使用。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [26] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: 提出大型行为模型(LBM)，通过行为嵌入而非临时提示来预测个体战略决策，利用结构化心理特征档案提升预测准确性


<details>
  <summary>Details</summary>
Motivation: 大语言模型在预测人类高风险决策时存在局限性，难以生成一致、个体特定的行为，特别是当准确预测依赖于心理特质与情境约束的复杂交互时。基于提示的方法在此场景下脆弱，存在身份漂移问题且无法充分利用详细人物描述。

Method: 引入大型行为模型(LBM)，这是一种行为基础模型，通过微调来高保真预测个体战略选择。LBM从临时人物提示转向行为嵌入，基于从全面心理测量工具包推导的结构化高维特质档案进行条件化。在连接稳定倾向、动机状态和情境约束与观察选择的专有数据集上进行训练。

Result: 在保留场景评估中，LBM微调相对于未适应的Llama-3.1-8B-Instruct骨干模型改善了行为预测，当基于大五人格特质条件化时，性能与前沿基线相当。研究发现基于提示的基线存在复杂性上限，而LBM继续受益于越来越密集的特质档案，随着提供额外特质维度，性能持续提升。

Conclusion: LBM为高保真行为模拟提供了可扩展方法，在战略预见、谈判分析、认知安全和决策支持等应用中具有潜力，通过行为嵌入方法克服了传统提示方法的局限性。

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [27] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 该研究使用布鲁姆分类法分析大语言模型的内部神经表征，发现认知复杂度在线性可分离的子空间中被编码，线性分类器在识别不同认知层级时达到约95%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的黑盒特性需要超越表面性能指标的新评估框架。本研究旨在探究认知复杂度是否在模型的内部神经表征中被编码，以及不同认知层级是否在线性可分离的子空间中表示。

Method: 使用布鲁姆分类法作为分层框架，分析不同LLM的高维激活向量。通过检查模型残差流中从基础回忆（Remember）到抽象综合（Create）的不同认知层级是否线性可分，使用线性分类器进行探测。

Result: 线性分类器在所有布鲁姆层级上达到约95%的平均准确率，表明认知层级在模型表征的线性可访问子空间中被编码。模型在正向传播早期就解析了提示的认知难度，且表征在不同层中变得越来越可分。

Conclusion: 研究提供了强有力的证据，表明认知复杂度在大语言模型的内部表征中以线性可访问的方式被编码，这为理解LLM如何处理不同认知难度的任务提供了新的视角，并为模型评估框架的发展做出了贡献。

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [28] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 提出Shapley-DCLR指标量化LLMs在回溯测试中的时间知识泄露，并开发TimeSPEC方法通过声明验证减少泄露


<details>
  <summary>Details</summary>
Motivation: 评估LLMs预测未来事件能力需要进行回溯测试，但LLMs可能在训练中编码了截止日期后的知识，导致时间知识泄露，影响回溯评估的有效性

Method: 1) 提出声明级框架检测时间知识泄露，将模型推理分解为原子声明并按时间可验证性分类；2) 使用Shapley值测量每个声明对预测的贡献，得到Shapley-DCLR指标；3) 开发TimeSPEC方法，通过声明验证和再生主动过滤时间污染

Result: 在350个实例（美国最高法院案件预测、NBA薪资估计、股票回报排名）的实验中，标准提示基线存在显著泄露，TimeSPEC在保持任务性能的同时显著降低了Shapley-DCLR

Conclusion: 显式的、可解释的声明级验证方法优于基于提示的时间约束，能够更可靠地进行回溯测试，TimeSPEC有效减少了时间知识泄露

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [29] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 本文详细记录了从arXiv LaTeX源文件训练1.36B参数科学语言模型的完整流程，重点分析预处理、分词和训练稳定性等工程实践，为中等计算预算下的领域专用模型训练提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 虽然前沿大语言模型展现出强大的推理和数学能力，但从原始科学文献训练领域专用语言模型的实际过程仍然缺乏详细文档。本研究旨在填补这一空白，为中等计算预算的研究者提供可复制的训练指南。

Method: 开发端到端训练流程：1) 元数据筛选和存档验证；2) LaTeX提取和文本规范化；3) 领域感知分词；4) 在2xA100 GPU约束下的密集Transformer训练。通过24次实验运行分析训练稳定性、扩展行为、数据损失和基础设施瓶颈。

Result: 研究发现：预处理决策显著影响可用token数量；分词策略影响符号稳定性；存储和I/O约束可能成为与计算同等重要的限制因素。在数据丰富（52B预训练token）的情况下训练稳定收敛。

Conclusion: 本研究提供了从零开始训练小型科学语言模型的工程化、透明化记录，而非提出新架构。这些见解有助于支持在中等计算预算下构建领域专用模型的研究者。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [30] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 提出一种无需数据的正则化方法，通过曲率矩阵近似解决任务向量组合中的表示漂移问题，实现任务算术的模块化扩展


<details>
  <summary>Details</summary>
Motivation: 任务算术为基础模型提供模块化、可扩展的适应方式，但多个任务向量组合会导致跨任务干扰，引起表示漂移和性能下降。现有表示漂移正则化方法通常需要外部任务数据，这与模块化和数据可用性约束（如隐私要求）相冲突。

Method: 提出无数据方法，将表示漂移的正则化框架化为曲率矩阵近似问题。采用Kronecker分解近似曲率技术，获得实用正则化器，在任务添加和否定任务中实现最先进结果。该方法任务数量复杂度恒定，且对任务向量缩放具有鲁棒性，无需保留调优。

Result: 在任务添加和否定任务中达到最先进性能，具有恒定任务数量复杂度，对任务向量缩放具有鲁棒性，无需保留调优。

Conclusion: 通过曲率矩阵近似框架，提出了一种无需数据的表示漂移正则化方法，解决了任务算术中的跨任务干扰问题，在保持模块化和数据隐私约束的同时提升了性能。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [31] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 提出将形式化验证与深度学习结合的图像检索新框架，通过图验证和神经代码生成支持开放词汇自然语言查询，确保结果可信可验证


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入模型的自然语言搜索在处理复杂关系、对象组合、精确约束（如身份、数量、比例）等查询时仍存在不可靠问题，需要更透明可信的检索框架

Method: 结合图验证方法和神经代码生成，将形式化验证集成到基于深度学习的图像检索中，通过形式推理系统验证用户查询中的每个原子事实

Result: 框架不仅返回匹配结果，还能识别标记哪些具体约束被满足或未满足，提供更透明可靠的检索过程，同时提升最流行嵌入方法的效果

Conclusion: 通过形式化验证与深度学习的协同结合，超越了向量表示的模糊性和近似性，实现了开放词汇自然语言查询的可信可验证检索

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [32] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 提出MCVAE模型，通过模态特定变分编码器、融合瓶颈门控机制和多任务目标，解决NSCLC生存预测中多模态数据严重缺失的问题。


<details>
  <summary>Details</summary>
Motivation: NSCLC患者生存预测具有挑战性，多模态数据（全切片图像、转录组、DNA甲基化）提供互补信息，但临床数据常存在模态缺失，现有方法在严重缺失情况下缺乏鲁棒性。

Method: 提出多模态对比变分自编码器（MCVAE）：模态特定变分编码器捕获各数据源不确定性；融合瓶颈门控机制归一化现有模态贡献；多任务目标结合生存损失、重建损失和跨模态对比损失；训练时应用随机模态掩蔽提高鲁棒性。

Result: 在TCGA-LUAD（475例）和TCGA-LUSC（446例）数据集上验证，模型在疾病特异性生存预测和严重缺失场景鲁棒性方面优于两种最先进模型。测试所有模态子集发现，多模态整合并非总是有益。

Conclusion: MCVAE能有效处理多模态数据严重缺失问题，提高NSCLC生存预测性能，同时揭示了多模态整合的局限性：并非所有模态组合都能改善预测效果。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [33] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 本文提出一个基于隐私设计（PbD）的框架，指导AI应用开发者为儿童设计隐私保护的LLM应用，结合GDPR、PIPEDA、COPPA等法规原则，并通过教育辅导案例验证可行性。


<details>
  <summary>Details</summary>
Motivation: 儿童越来越多地使用AI技术，但面临隐私风险。现有隐私法规要求企业实施保护措施，但在实践中存在挑战，需要为设计师和开发者提供具体指导框架。

Method: 提出基于隐私设计（PbD）的框架，整合GDPR、PIPEDA、COPPA等隐私法规原则，映射到LLM应用的数据收集、模型训练、运营监控和持续验证等阶段，并纳入UNCRC、AADC等儿童设计指南。

Result: 框架提供了各阶段的操作控制措施和设计指南，通过13岁以下儿童LLM教育辅导案例研究，展示了如何通过技术和组织控制以及适龄设计决策实现隐私保护并符合法律要求。

Conclusion: 通过在LLM生命周期中采用数据保护策略和适龄设计决策，可以支持开发既提供隐私保护又符合法律要求的儿童AI应用，该框架为实践提供了具体指导。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [34] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: WarpRec是一个高性能推荐系统框架，通过后端无关架构消除学术实验与工业部署间的鸿沟，包含50+算法、40+指标和19种策略，支持从本地到分布式无缝过渡，集成能耗追踪，并面向Agentic AI演进。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统生态系统存在分裂问题：研究人员必须在易于内存实验和需要重写以适应分布式工业引擎的复杂高成本之间做出选择。这种分裂阻碍了推荐系统的创新。

Method: 提出WarpRec框架，采用新颖的后端无关架构，包含50多种最先进算法、40个评估指标和19种过滤与分割策略，支持从本地执行到分布式训练和优化的无缝过渡。框架集成CodeCarbon进行实时能耗追踪。

Result: WarpRec消除了学术实验与工业部署之间的权衡，展示了可扩展性不必以科学完整性或可持续性为代价。框架为下一代可持续、面向智能体的推荐系统提供了架构基础。

Conclusion: WarpRec不仅弥合了学术界与工业界之间的差距，还可以作为下一代可持续、面向智能体的推荐系统的架构基础，推动推荐系统从静态排名引擎向生成式AI生态系统中的交互工具演进。

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [35] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 该研究提出了一个针对ARM Cortex处理器（M0+, M4, M7）的AI模型优化基准测试框架，通过自动化测试平台系统评估能效、准确性和资源利用率，发现浮点运算与推理时间呈近线性关系，并利用帕累托分析平衡能耗与精度。


<details>
  <summary>Details</summary>
Motivation: 在嵌入式系统中优化AI模型时，需要在能效、准确性和资源利用率之间取得平衡。目前缺乏针对ARM Cortex处理器的系统化基准测试框架，难以指导开发者选择最优的处理器与AI模型组合。

Method: 设计自动化测试平台，系统评估ARM Cortex M0+、M4、M7处理器上的AI模型性能。分析浮点运算与推理时间的相关性，使用帕累托分析平衡能耗与模型精度之间的权衡。

Result: 发现浮点运算与推理时间呈近线性相关，可作为计算需求估计的可靠指标。M7处理器适合短推理周期任务，M4处理器在长推理任务中能效更高，M0+处理器适合简单AI任务。通过帕累托分析可有效平衡能耗与精度。

Conclusion: 该基准测试框架为开发者提供了设计高能效AI系统的实用指导，帮助在嵌入式应用中实现高性能与可持续性的平衡。不同ARM Cortex处理器各有适用场景，需根据具体任务需求选择。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [36] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: KG-RAG框架将知识图谱与检索增强生成相结合，专门用于提升大语言模型在电信领域的性能，显著减少幻觉并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在电信领域应用困难，因为该领域具有复杂性、标准不断演进和专业术语等特点，导致模型输出不准确、幻觉增多，实用性降低。

Method: 提出KG-RAG框架，将知识图谱（提供电信标准和文档的结构化领域知识表示）与检索增强生成（动态检索相关事实以支撑模型输出）相结合。

Result: 在基准数据集上的实验表明，KG-RAG优于纯LLM和标准RAG基线，平均准确率分别比RAG提高14.3%，比纯LLM模型提高21.6%。

Conclusion: KG-RAG能有效在复杂电信场景中产生准确、可靠且可解释的输出，解决了电信领域大语言模型应用的关键挑战。

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [37] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 本文提出两个新指标（可重用性和可验证性）来评估多智能体IR管道中思维链的质量，发现这些指标与传统准确率不相关，揭示了当前基于准确率的推理能力排行榜的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前思维链评估主要关注目标任务准确率，但这一指标无法评估推理过程本身的质量或效用。需要新的评估指标来更全面地衡量思维链的质量。

Method: 采用Thinker-Executor框架将思维链生成与执行解耦，引入可重用性（衡量Executor重用Thinker思维链的容易程度）和可验证性（衡量Executor使用思维链匹配Thinker答案的频率）两个新指标。在五个基准测试中评估了四个Thinker模型与十个Executor模型组成的委员会。

Result: 研究发现可重用性和可验证性与标准准确率不相关，揭示了当前基于准确率的推理能力排行榜的盲点。令人惊讶的是，专门推理模型生成的思维链并不比Llama和Gemma等通用LLM生成的思维链更可重用或可验证。

Conclusion: 需要超越准确率的新评估指标来全面评估思维链的质量，专门推理模型在思维链质量方面不一定优于通用LLM，这对多智能体IR系统中的思维链评估提出了重要启示。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [38] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLong是一个开源LLM智能体，通过轨迹分割SFT和渐进式RL训练来解决极长视野任务，在PaperBench等基准上超越了Kimi K2 Thinking等模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在处理极长视野任务时存在能力不足的问题，需要开发能够有效解决这类复杂、多步骤任务的智能体系统。

Method: 1. 通过轨迹分割SFT冷启动模型：保留早期上下文，渐进截断后期上下文，保持子轨迹间重叠；2. 使用Research-Factory自动生成高质量训练数据；3. 提出渐进式RL训练：分多个阶段进行，逐步延长超时时间。

Result: KLong（106B）在PaperBench上超越了Kimi K2 Thinking（1T）11.28%，性能提升泛化到SWE-bench Verified和MLE-bench等其他编码基准。

Conclusion: 提出的轨迹分割SFT和渐进式RL训练方法有效提升了LLM智能体解决极长视野任务的能力，KLong在多个基准上表现出优越性能和泛化能力。

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [39] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 提出基于常微分方程的统一理论框架ODESteer，用于大语言模型激活导向对齐，通过屏障函数和多步自适应导向提升性能


<details>
  <summary>Details</summary>
Motivation: 当前激活导向方法存在两个关键限制：缺乏统一理论框架指导导向方向设计，以及过度依赖单步导向无法捕捉激活分布的复杂模式

Method: 提出基于常微分方程的理论框架，将传统激活加法解释为ODE的一阶近似，通过屏障函数定义导向方向，引入ODESteer方法进行多步自适应导向

Result: ODESteer在多个LLM对齐基准测试中取得一致改进：TruthfulQA提升5.7%，UltraFeedback提升2.5%，RealToxicityPrompts提升2.4%

Conclusion: 通过ODE统一激活导向的理论基础，提出的ODESteer方法在理论和实证上都取得了进展，为大语言模型对齐提供了新的原则性视角

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [40] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 该研究提出了一种基于联邦学习的混合AI模型，结合SWIN Transformer和CNN技术，用于通过X光片诊断COVID-19和肺炎，旨在提高诊断准确性并保护医疗数据隐私。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力的显著提升，人工智能在医疗健康领域应用的机会大大增加。医疗专家和医院需要共享数据空间，但医疗数据具有敏感性。研究者希望通过结合人工智能和联邦学习技术，建立一个安全、分布式的医疗数据处理系统，为医生提供可靠辅助诊断工具。

Method: 采用混合联邦学习框架，结合最新的CNN模型（DenseNet201、Inception V3、VGG 19）和微软开发的SWIN Transformer视觉Transformer模型。使用TensorFlow和Keras框架实现，通过联邦学习确保数据隐私安全，采用实时持续学习方法提升模型性能。

Result: 论文提出了一种能够检测COVID-19和肺炎的混合模型，通过联邦学习技术确保医疗数据的安全性和真实性，同时利用实时持续学习提高疾病诊断和严重程度预测的准确性。

Conclusion: 该研究展示了联邦学习与混合AI模型结合在医疗诊断中的潜力，能够在不泄露敏感医疗数据的前提下，利用分布式数据训练更准确的诊断模型，为全球抗击疫情提供技术支持。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [41] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 论文提出使用"人类游戏宇宙"作为评估AI通用智能的新方法，并开发了AI GameStore平台，通过LLM生成代表性人类游戏来测试AI性能


<details>
  <summary>Details</summary>
Motivation: 传统AI基准测试通常只评估狭窄能力，且容易饱和。需要一种更全面的方法来评估AI的人类级通用智能

Method: 提出通过"所有可想象的人类游戏"来评估AI通用智能，开发AI GameStore平台，使用LLM和人类参与循环从热门游戏平台自动生成标准化游戏环境

Result: 生成了100个基于App Store和Steam热门游戏的测试环境，测试7个前沿视觉语言模型。最佳模型在多数游戏中得分不到人类平均分的10%，在需要世界模型学习、记忆和规划的游戏上表现尤其差

Conclusion: AI GameStore是衡量和推动机器实现人类级通用智能的实用方法，未来需要进一步扩展和完善该平台

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [42] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT是一个基于分层离散扩散模型的分子图生成框架，通过引入化学先验编码和解耦原子编码，在MOSES数据集上实现了接近完美的化学有效性，超越了现有图扩散模型和1D基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型在分子生成中存在化学有效性低、难以满足目标属性要求的问题，相比1D建模方法表现不佳。需要开发能够克服这些性能限制的新框架。

Method: MolHIT基于分层离散扩散模型，将离散扩散推广到编码化学先验的额外类别，并采用解耦原子编码技术，根据原子的化学角色分离原子类型。

Result: 在MOSES数据集上实现了新的最先进性能，首次在图扩散中达到接近完美的化学有效性，在多个指标上超越了强大的1D基线方法。在下游任务如多属性引导生成和骨架扩展中也表现出色。

Conclusion: MolHIT成功克服了现有图扩散模型的长期性能限制，为AI驱动的药物发现和材料科学提供了强大的分子生成框架。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [43] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是CLEF评估实验室，专注于从多语言历史文本中提取人物-地点关系，扩展了之前的HIPE系列，增加了语义关系提取任务，要求系统分类两种关系类型，并引入三重评估框架。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决历史文本中人物与地点关系提取的挑战，这些文本通常噪声大、多语言且跨越不同时期。通过建立评估框架，旨在支持知识图谱构建、历史传记重建和数字人文空间分析等下游应用。

Method: 方法包括：1）构建多语言历史文本数据集；2）定义两种关系类型（at和isAt）；3）要求系统基于时间和地理线索进行推理；4）引入三重评估框架，同时评估准确性、计算效率和领域泛化能力。

Result: HIPE-2026建立了评估实验室框架，扩展了HIPE系列到语义关系提取领域，为多语言历史文本中人物-地点关系提取提供了标准化评估方法，并连接了大规模历史数据处理。

Conclusion: HIPE-2026通过专注于人物-地点关系提取，为历史文本处理提供了重要评估框架，支持数字人文领域的知识图谱构建、历史传记重建和空间分析等应用，推动了历史文本语义分析的发展。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>

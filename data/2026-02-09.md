<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning](https://arxiv.org/abs/2602.06107)
*Zhuoming Chen,Hongyi Liu,Yang Zhou,Haizhong Zheng,Beidi Chen*

Main category: cs.AI

TL;DR: Jackpot框架通过最优预算拒绝采样(OBRS)减少rollout模型与策略之间的分布不匹配，实现更高效的LLM强化学习训练


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的强化学习训练成本高昂，特别是rollout阶段。将rollout生成与策略优化解耦（如使用更高效的模型进行rollout）可显著提升效率，但这会引入严重的分布不匹配问题，导致学习不稳定。

Method: 提出Jackpot框架，采用最优预算拒绝采样(OBRS)直接减少rollout模型与演化策略之间的差异。包括：原则性的OBRS过程、联合更新策略和rollout模型的统一训练目标、基于top-k概率估计和批次级偏差校正的高效系统实现。

Result: 理论分析表明OBRS在可控接受预算下持续使rollout分布更接近目标分布。实证显示相比重要性采样基线，Jackpot显著提升训练稳定性，在Qwen3-8B-Base上训练300个更新步骤（批次大小64）时，性能可与在线策略RL相媲美。

Conclusion: 基于OBRS的对齐方法使我们更接近将rollout生成与策略优化实际有效解耦的目标，为LLM的强化学习提供了更实用和高效的解决方案。

Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.

</details>


### [2] [Large Language Model Reasoning Failures](https://arxiv.org/abs/2602.06176)
*Peiyang Song,Pengrui Han,Noah Goodman*

Main category: cs.AI

TL;DR: 该论文首次对大型语言模型（LLMs）的推理失败进行了全面综述，提出了新的分类框架，将推理分为具身和非具身类型，并将推理失败分为基础性、应用特定和鲁棒性问题三类，为理解LLM推理系统性弱点提供了结构化视角。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在推理能力方面取得了显著进展，但在看似简单的场景中仍然存在显著的推理失败。为了系统性地理解和解决这些缺陷，需要对这些推理失败进行全面的调查和分类。

Method: 提出了一个新颖的分类框架：将推理分为具身推理和非具身推理，非具身推理进一步细分为非正式（直觉）推理和正式（逻辑）推理。同时，将推理失败分为三类：影响下游任务的基础性失败、特定领域表现的应用特定限制、以及跨微小变化表现不一致的鲁棒性问题。

Result: 为每种推理失败提供了清晰的定义，分析了现有研究，探索了根本原因，并提出了缓解策略。通过整合分散的研究工作，提供了对LLM推理系统性弱点的结构化视角。

Conclusion: 该综述为理解LLM推理失败提供了有价值的见解，并指导未来研究朝着构建更强、更可靠、更鲁棒的推理能力方向发展。同时发布了GitHub资源库，为该领域研究提供了便捷入口。

Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

</details>


### [3] [Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)](https://arxiv.org/abs/2602.06227)
*Pierriccardo Olivieri,Fausto Lasca,Alessandro Gianola,Matteo Papini*

Main category: cs.AI

TL;DR: 提出基于LTLfMT逻辑框架的非马尔可夫奖励规范方法，用于大规模状态空间的MDP，通过奖励机器和HER解决奖励稀疏问题


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大规模状态空间MDP中的复杂任务时表达能力有限，需要手动编码谓词，缺乏统一可重用的框架

Method: 使用LTLfMT（基于理论的线性时序逻辑）增强表达能力，识别可处理的逻辑片段，结合奖励机器和Hindsight Experience Replay（HER）方法

Result: 在连续控制环境中使用非线性算术理论验证了方法有效性，实验表明定制的HER实现对解决复杂目标任务至关重要

Conclusion: 提出的LTLfMT框架能够自然地规范复杂任务，解决了大规模状态空间中非马尔可夫奖励规范的理论和计算挑战

Abstract: In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal logic in which predicates are first-order formulas of arbitrary first-order theories rather than simple Boolean variables. This enhanced expressiveness enables the specification of complex tasks over unstructured and heterogeneous data domains, promoting a unified and reusable framework that eliminates the need for manual predicate encoding. However, the increased expressive power of LTLfMT introduces additional theoretical and computational challenges compared to standard LTLf specifications. We address these challenges from a theoretical standpoint, identifying a fragment of LTLfMT that is tractable but sufficiently expressive for reward specification in an infinite-state-space context. From a practical perspective, we introduce a method based on reward machines and Hindsight Experience Replay (HER) to translate first-order logic specifications and address reward sparsity. We evaluate this approach to a continuous-control setting using Non-Linear Arithmetic Theory, showing that it enables natural specification of complex tasks. Experimental results show how a tailored implementation of HER is fundamental in solving tasks with complex goals.

</details>


### [4] [Do LLMs Act Like Rational Agents? Measuring Belief Coherence in Probabilistic Decision Making](https://arxiv.org/abs/2602.06286)
*Khurram Yamin,Jingjing Tang,Santiago Cortes-Gomez,Amit Sharma,Eric Horvitz,Bryan Wilder*

Main category: cs.AI

TL;DR: 该研究评估大型语言模型在医疗诊断等高风险决策中是否作为理性效用最大化者，通过贝叶斯推理框架检验其信念一致性和偏好稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地部署在医疗诊断等高风险领域，其决策逻辑难以解释，需要评估它们是否具有一致的信念和稳定的偏好，以确定是否适合指导高风险决策。

Method: 采用诊断挑战问题，通过贝叶斯效用最大化框架分析LLM行为，建立可证伪条件来检验报告概率是否与任何理性智能体的真实信念一致，并在多个医疗诊断领域评估多个LLM。

Result: 研究提供了关于LLM推理与理想贝叶斯效用最大化之间关系的见解，建立了可证伪条件来识别报告概率与理性智能体真实信念不一致的情况。

Conclusion: 研究结果对LLM在高风险决策中的应用具有重要启示，指出了当前模型的局限性，并为未来改进方向提供了指导。

Abstract: Large language models (LLMs) are increasingly deployed as agents in high-stakes domains where optimal actions depend on both uncertainty about the world and consideration of utilities of different outcomes, yet their decision logic remains difficult to interpret. We study whether LLMs are rational utility maximizers with coherent beliefs and stable preferences. We consider behaviors of models for diagnosis challenge problems. The results provide insights about the relationship of LLM inferences to ideal Bayesian utility maximization for elicited probabilities and observed actions. Our approach provides falsifiable conditions under which the reported probabilities \emph{cannot} correspond to the true beliefs of any rational agent. We apply this methodology to multiple medical diagnostic domains with evaluations across several LLMs. We discuss implications of the results and directions forward for uses of LLMs in guiding high-stakes decisions.

</details>


### [5] [Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems](https://arxiv.org/abs/2602.06319)
*Qifan Zhang,Jianhao Ruan,Aochuan Chen,Kang Zeng,Nuo Chen,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: GrAlgoBench是一个新的基准测试，专门评估大型推理模型在图算法问题上的表现，揭示了当前模型在长上下文推理和过度思考方面的主要缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有数学、代码和常识推理基准存在局限：缺乏长上下文评估、挑战性不足、答案难以程序化验证。需要更严谨的测试平台来评估大型推理模型的真实能力。

Method: 引入GrAlgoBench基准，通过图算法问题评估大型推理模型。图算法问题需要长上下文推理、难度可精细控制、支持标准化程序化评估。包含9个任务，进行系统实验。

Result: 实验揭示两大主要弱点：1) 随着上下文长度增加，准确率急剧下降，当图超过120个节点时准确率低于50%；2) 存在过度思考现象，大量自我验证无效，推理轨迹膨胀但正确性未改善。

Conclusion: GrAlgoBench将图算法问题确立为严谨、多维且实用的测试平台，有助于推进大型推理模型的研究。代码已开源。

Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.

</details>


### [6] [Difficulty-Estimated Policy Optimization](https://arxiv.org/abs/2602.06375)
*Yu Zhao,Fan Jiang,Tianle Liu,Bo Zeng,Yu Liu,Longyue Wang,Weihua Luo*

Main category: cs.AI

TL;DR: DEPO框架通过在线难度估计器动态筛选训练数据，在保持模型性能的同时将推理成本降低2倍，为高效训练高性能推理模型提供了新路径。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在遇到过于简单或复杂的问题时会出现梯度信号衰减问题，而DAPO等变体虽然尝试解决梯度消失问题，但无法缓解在低效用样本上进行全面展开所带来的巨大计算开销。

Method: 提出Difficulty-Estimated Policy Optimization (DEPO)框架，集成在线难度估计器，在展开阶段前动态评估和筛选训练数据，优先将计算资源分配给具有高学习潜力的样本。

Result: 实验结果表明，DEPO在不影响模型性能的情况下，将展开成本降低了高达2倍，显著降低了训练高性能推理模型的计算门槛。

Conclusion: DEPO为推理对齐的效率和鲁棒性优化提供了创新框架，通过智能数据筛选机制实现了计算资源的高效利用，为推理模型的规模化训练提供了更可持续的路径。

Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.

</details>


### [7] [Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization](https://arxiv.org/abs/2602.06394)
*Arvid E. Gollwitzer,Paridhi Latawa,David de Gruijl,Deepak A. Subramanian,Adrián Noriega de la Colina*

Main category: cs.AI

TL;DR: QA-Token是一种质量感知的分词方法，通过考虑数据可靠性来改进词汇表构建，在基因组学和金融领域显著提升性能，同时减少15%的token数量。


<details>
  <summary>Details</summary>
Motivation: 当前的分词方法在处理序列数据时没有考虑信号质量，限制了它们在嘈杂真实世界语料库上的有效性。需要一种能够将数据可靠性直接纳入词汇表构建的方法。

Method: 提出了QA-Token（质量感知分词），包含三个关键贡献：(1)双层优化公式，联合优化词汇表构建和下游性能；(2)强化学习方法，通过质量感知奖励学习合并策略，具有收敛保证；(3)通过Gumbel-Softmax松弛的自适应参数学习机制，实现端到端优化。

Result: 实验评估显示一致的改进：基因组学（变异检测F1分数比BPE提高6.7个百分点）、金融（夏普比率提高30%）。在基础模型规模上，对包含1.7万亿碱基对的预训练语料进行分词，实现了最先进的病原体检测（94.53 MCC），同时减少15%的token数量。

Conclusion: QA-Token解锁了嘈杂的真实世界语料库（包括petabase规模的基因组序列和terabyte规模的金融时间序列），用于基础模型训练，且推理时无额外开销。

Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.

</details>


### [8] [Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution](https://arxiv.org/abs/2602.06413)
*Hsien-Jyh Liao*

Main category: cs.AI

TL;DR: 本文认为自回归模型在长程推理任务中的性能下降主要源于过程级不稳定性而非任务复杂性，提出了自回归推理中决策优势随执行长度指数衰减的理论，指出稳定长程推理需要离散分段和类图结构。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出卓越的推理能力，但在长程任务中性能会急剧下降。传统解释主要归因于任务复杂性，但作者认为这些解释不完整，即使在无分支的线性任务中，自回归执行也存在内在稳定性限制。

Method: 提出自回归推理存在过程级不稳定性的理论框架，推导出定理A证明单路径自回归推理中决策优势随执行长度指数衰减。通过合成环境和真实TextWorld任务的实证研究验证理论预测。

Result: 实证研究显示性能断崖与理论预测一致，揭示了自回归架构在维持长期连贯性方面的新限制。短程评估协议可能掩盖结构不稳定性。

Conclusion: 长程推理失败应从动力学角度理解，未来推理系统可能需要从单纯扩展转向结构化治理。稳定长程推理需要离散分段，自然诱导出有向无环图等图状执行结构。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit.
  We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs).
  Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoretical predictions. Our findings provide a dynamical perspective on long-horizon reasoning failure and suggest new limitations on maintaining long-term coherence under purely autoregressive architectures. Furthermore, we highlight that short-horizon evaluation protocols may obscure structural instability, indicating a potential shift from scaling toward structured governance in future reasoning systems.

</details>


### [9] [AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents](https://arxiv.org/abs/2602.06485)
*Haotian Chen,Xin Cong,Shengda Fan,Yuyang Fu,Ziqin Gong,Yaxi Lu,Yishan Li,Boye Niu,Chengjun Pan,Zijun Song,Huadong Wang,Yesai Wu,Yueying Wu,Zihao Xie,Yukun Yan,Zhong Zhang,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文首次系统研究了4B参数规模智能体模型的训练，提出了AgentCPM-Explore模型，通过参数空间模型融合、奖励信号去噪和上下文信息精炼等技术，在多个基准测试中超越了8B级模型甚至更大规模模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体系统严重依赖大规模模型，而边缘规模模型（4B参数级别）的能力尚未得到充分探索。本文旨在系统研究如何训练高性能的边缘规模智能体模型。

Method: 提出了AgentCPM-Explore模型和整体训练框架，包括：1）参数空间模型融合解决SFT中的灾难性遗忘问题；2）奖励信号去噪处理RL中的噪声敏感性；3）上下文信息精炼应对长上下文场景中的推理退化问题。

Result: AgentCPM-Explore在4B级模型中达到SOTA性能，在四个基准测试中匹配或超越了8B级SOTA模型，在五个基准测试中甚至超越了Claude-4.5-Sonnet或DeepSeek-v3.2等更大规模模型，在GAIA文本任务中达到97.09%准确率（pass@64）。

Conclusion: 边缘规模模型的瓶颈并非其固有能力上限，而是推理稳定性问题。通过提出的训练框架，AgentCPM-Explore有效释放了边缘规模模型被低估的潜力，证明了小规模模型也能实现高性能智能体能力。

Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.

</details>


### [10] [HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction](https://arxiv.org/abs/2602.06527)
*Shengxuan Qiu,Haochen Huang,Shuzhang Zhong,Pengfei Zuo,Meng Li*

Main category: cs.AI

TL;DR: HyPER是一种训练免费的在线控制策略，用于专家混合模型中的多路径解码，通过动态扩展-缩减控制重新分配固定计算预算，在推理任务中实现更好的准确率-计算权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索-利用权衡上存在刚性限制：树结构搜索通过脆弱的扩展规则硬编码探索，干扰后训练推理；并行推理则过度探索冗余假设路径且依赖弱答案选择。最优平衡是阶段依赖的，正确与错误推理路径往往只在后期才分叉。

Method: 将测试时扩展重新表述为假设池上的动态扩展-缩减控制问题。提出HyPER：包含在线控制器（随假设池演化从探索转向利用）、令牌级精炼机制（无需完整路径重采样即可实现高效生成时利用）、长度和置信度感知聚合策略（可靠答案时利用）。

Result: 在四个专家混合语言模型和多样化推理基准上的实验表明，HyPER始终实现更优的准确率-计算权衡，准确率提高8-10%，同时令牌使用减少25-40%。

Conclusion: 通过将测试时扩展重新表述为动态控制问题，HyPER在专家混合模型中实现了更高效的多路径解码，显著改善了推理准确率与计算效率的平衡。

Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consistently achieves a superior accuracy-compute trade-off, improving accuracy by 8 to 10 percent while reducing token usage by 25 to 40 percent.

</details>


### [11] [SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees](https://arxiv.org/abs/2602.06554)
*Tianyi Hu,Qingxu Fu,Yanxi Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: 本文提出SeeUPO算法，解决现有RL算法在多轮交互场景中缺乏收敛保证的问题，通过序列级顺序更新策略优化实现无critic且收敛的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的AI智能体主要使用强化学习训练，但现有骨干RL算法在多轮场景中缺乏经过验证的收敛保证，导致训练不稳定和无法收敛到最优策略。

Method: 提出SeeUPO（Sequence-level Sequential Update Policy Optimization）算法，将多轮交互建模为顺序执行的多智能体赌博机问题，通过反向执行顺序的逐轮顺序策略更新，确保单调改进并通过反向归纳收敛到全局最优解。

Result: 在AppWorld和BFCL v4基准测试中，SeeUPO相比现有骨干算法取得显著改进：在Qwen3-14B上相对增益43.3%-54.6%，在Qwen2.5-14B上相对增益24.1%-41.9%（跨基准平均），同时表现出优越的训练稳定性。

Conclusion: SeeUPO是一种具有收敛保证的无critic方法，能够有效解决多轮交互场景中RL算法的收敛问题，为基于LLM的AI智能体训练提供了更稳定可靠的解决方案。

Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.
  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.
  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.
  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.

</details>


### [12] [Same Answer, Different Representations: Hidden instability in VLMs](https://arxiv.org/abs/2602.06652)
*Farooq Ahmad Wani,Alessandro Suglia,Rohit Saxena,Aryo Pradipta Gema,Wai-Chung Kwan,Fazl Barez,Maria Sofia Bucarelli,Fabrizio Silvestri,Pasquale Minervini*

Main category: cs.AI

TL;DR: 研究发现视觉语言模型的输出稳定性不能完全反映其内部表示稳定性，提出新的评估框架揭示三种失败模式：内部表示漂移、规模不改善鲁棒性、扰动对不同任务有不同影响。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的鲁棒性评估主要关注输出层面的不变性，隐含假设稳定的预测反映了稳定的多模态处理过程。然而这种假设可能不足够，需要更深入地评估模型内部表示的变化。

Method: 提出了一个表示感知和频率感知的评估框架，测量内部嵌入漂移、频谱敏感性和结构平滑性（视觉标记的空间一致性），同时结合标准的基于标签的指标。在SEEDBench、MMMU和POPE数据集上对现代视觉语言模型进行评估。

Result: 揭示了三种不同的失败模式：1）模型经常在保持预测答案的同时经历显著的内部表示漂移；2）鲁棒性不随模型规模提升而改善；3）扰动对不同任务有不同影响：破坏推理任务但减少幻觉任务的误报。

Conclusion: 仅依赖输出层面的鲁棒性评估是不够的，需要更全面的内部表示评估框架。视觉语言模型在保持预测稳定性的同时可能经历严重的内部表示变化，这挑战了当前鲁棒性评估的基本假设。

Abstract: The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.

</details>


### [13] [Autoregressive Models for Knowledge Graph Generation](https://arxiv.org/abs/2602.06707)
*Thiviyan Thanapalasingam,Antonis Vozikis,Peter Bloem,Paul Groth*

Main category: cs.AI

TL;DR: ARK是一种自回归知识图谱生成模型，将图谱视为三元组序列进行生成，无需显式规则监督即可学习语义约束，在IntelliGraphs基准测试中达到89.2%-100%语义有效性


<details>
  <summary>Details</summary>
Motivation: 知识图谱生成需要模型学习三元组间的复杂语义依赖关系，同时保持领域有效性约束。与独立评分三元组的链接预测不同，生成模型必须捕获整个子图的相互依赖关系以产生语义连贯的结构

Method: 提出ARK（自回归知识图谱生成）模型家族，将图谱视为(head, relation, tail)三元组序列进行自回归生成。模型直接从数据中学习隐式语义约束（类型一致性、时间有效性、关系模式），无需显式规则监督。还提出SAIL作为ARK的变分扩展，通过学习的潜在表示实现可控生成

Result: 在IntelliGraphs基准测试中，模型在多样化数据集上达到89.2%到100.0%的语义有效性，同时生成训练中未见的新图谱。分析表明模型容量（隐藏维度≥64）比架构深度对KG生成更重要，循环架构在保持可比有效性的同时提供显著计算效率

Conclusion: 自回归模型为知识图谱生成提供了有效框架，在知识库补全和查询回答中具有实际应用价值。模型容量比架构深度更关键，循环架构在保持性能的同时提供计算效率优势

Abstract: Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantically coherent structures. We present ARK (Auto-Regressive Knowledge Graph Generation), a family of autoregressive models that generate KGs by treating graphs as sequences of (head, relation, tail) triples. ARK learns implicit semantic constraints directly from data, including type consistency, temporal validity, and relational patterns, without explicit rule supervision. On the IntelliGraphs benchmark, our models achieve 89.2% to 100.0% semantic validity across diverse datasets while generating novel graphs not seen during training. We also introduce SAIL, a variational extension of ARK that enables controlled generation through learned latent representations, supporting both unconditional sampling and conditional completion from partial graphs. Our analysis reveals that model capacity (hidden dimensionality >= 64) is more critical than architectural depth for KG generation, with recurrent architectures achieving comparable validity to transformer-based alternatives while offering substantial computational efficiency. These results demonstrate that autoregressive models provide an effective framework for KG generation, with practical applications in knowledge base completion and query answering.

</details>


### [14] [Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions](https://arxiv.org/abs/2602.06746)
*Alessandro Abate,Giuseppe De Giacomo,Mathias Jackermeier,Jan Kretínský,Maximilian Prokop,Christoph Weinhuber*

Main category: cs.AI

TL;DR: 该论文提出了一种基于语义LTL到自动机转换的多任务强化学习方法，利用结构化任务嵌入实现通用策略学习


<details>
  <summary>Details</summary>
Motivation: 研究多任务强化学习，旨在学习一个能够泛化到任意（可能未见过的）任务的通用策略。使用线性时序逻辑（LTL）公式作为任务规范，这是形式化方法中常用的系统属性规范方式，最近在强化学习中成功应用。

Method: 提出了一种新颖的任务嵌入技术，利用新一代语义LTL到自动机的转换方法（最初为时序综合开发）。生成的语义标记自动机在每个状态包含丰富的结构化信息，能够：(i) 高效地在线计算自动机，(ii) 提取用于条件化策略的表达性任务嵌入，(iii) 自然支持完整的LTL。

Result: 在各种领域的实验结果表明，该方法实现了最先进的性能，并且能够扩展到现有方法失败的复杂规范。

Conclusion: 通过结合语义LTL到自动机转换和任务嵌入技术，提出了一种有效的多任务强化学习方法，能够处理复杂的LTL规范并实现良好的泛化性能。

Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.

</details>


### [15] [Wild Guesses and Mild Guesses in Active Concept Learning](https://arxiv.org/abs/2602.06818)
*Anirudh Chari,Neil Pattanaik*

Main category: cs.AI

TL;DR: 本文研究了主动概念学习中的查询策略平衡问题，比较了最大化期望信息增益的理性主动学习策略与人类常用的积极测试策略在不同概念学习任务中的表现差异。


<details>
  <summary>Details</summary>
Motivation: 人类概念学习通常是主动的：学习者选择查询哪些实例来减少对底层规则或类别的不确定性。主动概念学习需要在查询的信息量与生成和评分假设的学习者稳定性之间取得平衡。本文旨在研究这种权衡，特别是在神经符号贝叶斯学习器中，其假设是由大型语言模型提出的可执行程序，并通过贝叶斯更新重新加权。

Method: 采用神经符号贝叶斯学习器，假设由大型语言模型生成的可执行程序表示，通过贝叶斯更新重新加权。比较两种查询策略：1）理性主动学习策略（EIG），选择查询以最大化近似期望信息增益；2）人类常用的积极测试策略（PTS），查询当前最佳假设预测为积极的实例。在经典数字游戏的概念学习任务中进行实验。

Result: 在经典数字游戏的概念学习任务中，EIG策略在需要证伪的复杂概念（如复合规则或包含例外的规则）上表现有效，但在简单概念上表现不佳。研究发现EIG失败的原因是策略与LLM提案分布之间的支持不匹配：高度诊断性的边界查询将后验推向生成器产生无效或过于具体程序的区域，导致粒子近似中的支持不匹配陷阱。PTS虽然在信息获取上不是最优的，但倾向于通过选择"安全"查询来保持提案有效性，从而在简单规则上实现更快的收敛。

Conclusion: 研究结果表明，"确认偏误"可能不是认知错误，而是在人类思维特有的稀疏、开放式假设空间中，为保持可处理推理而做出的理性适应。积极测试策略通过选择更安全的查询来维持假设生成器的有效性，在某些情况下比最大化信息增益的策略更有效。

Abstract: Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting "safe" queries, leading to faster convergence on simple rules. Our results suggest that "confirmation bias" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.

</details>


### [16] [ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training](https://arxiv.org/abs/2602.06820)
*Dunwei Tu,Hongyan Hao,Hansi Yang,Yihao Chen,Yi-Kai Zhang,Zhikang Xia,Yu Yang,Yueqing Sun,Xingchen Liu,Furao Shen,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.AI

TL;DR: ScaleEnv是一个从零开始构建完全交互式环境和可验证任务的框架，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性，显著提升智能体在未见多轮工具使用基准测试上的性能。


<details>
  <summary>Details</summary>
Motivation: 训练能够适应多样化场景的通用智能体需要交互式环境进行自我探索，但现有交互环境严重不足，现有合成方法在环境多样性和可扩展性方面存在显著限制。

Method: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性。

Result: 在未见的多轮工具使用基准测试（如τ²-Bench和VitaBench）上表现出显著性能提升，展示了强大的泛化能力；研究还发现增加领域数量与模型泛化性能之间存在正相关关系。

Conclusion: 扩展环境多样性对于鲁棒的智能体学习至关重要，ScaleEnv通过构建多样化的交互环境为智能体训练提供了有效解决方案。

Abstract: Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $τ^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.

</details>


### [17] [POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models](https://arxiv.org/abs/2602.06822)
*Yi Chen,Wonjin Shin,Shuhong Liu,Tho Mai,Jeongmo Lee,Chuanbo Hua,Kun Wang,Jun Liu,Joo-Young Kim*

Main category: cs.AI

TL;DR: POP是一种轻量级在线结构化剪枝框架，通过分区引导的动态剪枝实现上下文条件化的模型压缩，在推理过程中根据生成内容动态调整剪枝模式。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型通过规模化获得强大性能，但现有结构化剪枝方法在推理时采用固定的剪枝决策，忽略了自回归token生成过程中出现的稀疏模式变化。

Method: POP将模型通道划分为保留区、候选区和剪枝区三个区域：预填充阶段定义粗粒度剪枝分区，解码阶段在候选区内生成细粒度掩码，避免全通道重新评估。该方法无需预处理、离线校准、重新训练或学习预测器。

Result: 在大型语言模型、专家混合模型和视觉语言模型等多种大型基础模型上的广泛评估表明，POP相比现有剪枝方法能提供更高的准确性，同时计算开销更小，推理延迟更低。

Conclusion: POP是一种高效、轻量级的即插即用在线结构化剪枝框架，能够实现上下文条件化的动态剪枝，在保持模型性能的同时显著减少计算开销和推理延迟。

Abstract: Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.

</details>


### [18] [LLM Active Alignment: A Nash Equilibrium Perspective](https://arxiv.org/abs/2602.06836)
*Tonghan Wang,Yuqi Pan,Xinyi Yang,Yanchen Jiang,Milind Tambe,David C. Parkes*

Main category: cs.AI

TL;DR: 提出一个基于博弈论的框架，通过纳什均衡分析来预测和引导大语言模型群体的行为，避免在开放文本空间中计算均衡的复杂性。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型群体行为预测和引导的问题，特别是在开放文本空间中纳什均衡计算不可行的情况下，需要开发一个可解释且行为实质性的策略类别。

Method: 将每个智能体的行动建模为对人类子群体的混合选择，智能体主动战略性地选择与哪些群体对齐，采用标准凹效用假设推导封闭形式的纳什均衡特征。

Result: 该方法可作为现有对齐流程（如RLHF）之上的主动对齐层，在社交媒体场景中展示了大语言模型群体（特别是基于推理的模型）可能出现政治排斥现象，而该方法可以避免这种病理现象。

Conclusion: 该博弈论框架为跨领域调节多智能体大语言模型动态提供了有前景的方法，能够通过明确、可操作的指导将对齐目标转向社会期望的结果。

Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human subpopulations. Agents choose actively and strategically which groups to align with, yielding an interpretable and behaviorally substantive policy class. We derive closed-form NE characterizations, adopting standard concave-utility assumptions to enable analytical system-level predictions and give explicit, actionable guidance for shifting alignment targets toward socially desirable outcomes. The method functions as an active alignment layer on top of existing alignment pipelines such as RLHF. In a social-media setting, we show that a population of LLMs, especially reasoning-based models, may exhibit political exclusion, pathologies where some subpopulations are ignored by all LLM agents, which can be avoided by our method, illustrating the promise of applying the method to regulate multi-agent LLM dynamics across domains.

</details>


### [19] [An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization](https://arxiv.org/abs/2602.06838)
*Jin Wang,Hui Ma,Fei Xing,Ming Yan*

Main category: cs.AI

TL;DR: 提出自适应差分隐私联邦学习框架，通过客户端轻量级压缩模块、服务器端自适应梯度裁剪和约束感知聚合机制，解决异构数据和隐私约束下的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在实际部署中面临设备异构性、非独立同分布数据导致的梯度更新不稳定和偏差问题，而差分隐私的固定梯度裁剪和高斯噪声注入会进一步放大梯度扰动，导致训练震荡和性能下降。

Method: 1) 客户端：引入轻量级本地压缩模块，正则化中间表示并约束梯度变异性，减轻本地优化过程中的噪声放大；2) 服务器端：基于历史更新统计的自适应梯度裁剪策略，动态调整裁剪阈值；3) 约束感知聚合机制，抑制不可靠或噪声主导的客户端更新。

Result: 在CIFAR-10和SVHN数据集上的大量实验表明，该方法提高了收敛稳定性和分类准确率。

Conclusion: 提出的自适应差分隐私联邦学习框架能有效解决异构和隐私约束环境下的训练挑战，通过客户端压缩、自适应裁剪和智能聚合机制实现了更稳定高效的模型训练。

Abstract: Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.

</details>


### [20] [From Features to Actions: Explainability in Traditional and Agentic AI Systems](https://arxiv.org/abs/2602.06841)
*Sindhuja Chaduvula,Jessee Ho,Kina Kim,Aravind Narayanan,Mahshid Alinoori,Muskan Garg,Dhanesh Ramachandram,Shaina Raza*

Main category: cs.AI

TL;DR: 该研究比较了静态分类任务中的属性解释方法与智能体基准中的轨迹诊断方法，发现属性方法在静态设置中表现稳定，但无法可靠诊断智能体轨迹中的执行级故障，而基于轨迹的诊断方法能有效定位行为故障。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，AI系统已从静态预测转向多步决策的智能体系统。传统的可解释AI主要关注静态模型预测的解释，而智能体系统的成功与失败由决策序列决定。目前尚不清楚为静态预测设计的解释方法如何适用于行为随时间演化的智能体环境。

Method: 通过比较静态分类任务中的属性解释方法与智能体基准（TAU-bench Airline和AssistantBench）中的轨迹诊断方法。在静态设置中使用属性方法，在智能体设置中使用基于轨迹的评估方法，分析两者在解释能力上的差异。

Result: 属性方法在静态设置中实现稳定的特征排名（Spearman ρ=0.86），但无法可靠诊断智能体轨迹中的执行级故障。基于轨迹的诊断方法能一致地定位行为故障，发现状态跟踪不一致在失败运行中普遍2.7倍，并将成功概率降低49%。

Conclusion: 研究结果表明，在评估和诊断自主AI行为时，需要从静态解释转向轨迹级可解释性方法，以更好地理解和诊断智能体系统的行为故障。

Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\times$ more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.
  Resources:
  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework

</details>


### [21] [AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents](https://arxiv.org/abs/2602.06855)
*Alisia Lupidi,Bhavul Gauri,Thomas Simon Foster,Bassel Al Omari,Despoina Magka,Alberto Pepe,Alexis Audran-Reiss,Muna Aghamelu,Nicolas Baldwin,Lucia Cipolina-Kun,Jean-Christophe Gagnon-Audet,Chee Hau Leow,Sandra Lefdal,Hossam Mossalam,Abhinav Moudgil,Saba Nazir,Emanuel Tewolde,Isabel Urrego,Jordi Armengol Estape,Amar Budhiraja,Gaurav Chaurasia,Abhishek Charnalia,Derek Dunfield,Karen Hambardzumyan,Daniel Izcovich,Martin Josifoski,Ishita Mediratta,Kelvin Niu,Parth Pathak,Michael Shvartsman,Edan Toledo,Anton Protopopov,Roberta Raileanu,Alexander Miller,Tatiana Shavrina,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AIRS-Bench是一个包含20个任务的AI研究科学基准测试套件，用于评估LLM智能体在整个科研生命周期中的能力，包括想法生成、实验分析和迭代改进。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在推动科学研究方面具有巨大潜力，但目前缺乏一个全面的基准测试来评估智能体在整个科研生命周期中的能力，包括从想法生成到实验分析和迭代改进的全过程。

Method: 研究人员从顶级机器学习论文中选取了20个任务，涵盖语言建模、数学、生物信息学和时间序列预测等多个领域。任务格式灵活，便于集成新任务和比较不同智能体框架。使用前沿模型配合顺序和并行框架建立了基线。

Result: 智能体在4个任务中超过了人类的最佳水平，但在16个任务中未能达到人类水平。即使智能体超越了人类基准，也未能达到底层任务的理论性能上限，表明基准远未饱和，有巨大改进空间。

Conclusion: AIRS-Bench是一个尚未饱和的基准测试，为自主科学研究的发展提供了重要工具和评估标准。研究人员开源了任务定义和评估代码，以促进该领域的进一步发展。

Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.

</details>

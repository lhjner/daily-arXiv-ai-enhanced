<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 12]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory](https://arxiv.org/abs/2602.12316)
*Pepijn Cobben,Xuanqiang Angelo Huang,Thao Amelia Pham,Isabel Dahlgren,Terry Jingchen Zhang,Zhijing Jin*

Main category: cs.AI

TL;DR: GT-HarmBench是一个包含2009个高风险场景的多智能体安全基准测试，涵盖囚徒困境、猎鹿博弈等博弈论结构，用于评估前沿AI系统在多智能体环境中的安全性和协调能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全基准主要评估单智能体，而忽略了多智能体环境中的风险，如协调失败和冲突。随着前沿AI系统在高风险多智能体环境中的部署增加，需要专门的基准来理解和评估这些风险。

Method: 从MIT AI风险库中提取真实AI风险场景，构建包含2009个高风险场景的基准测试，涵盖囚徒困境、猎鹿博弈、斗鸡博弈等博弈论结构。评估15个前沿模型，测量对博弈论提示框架和顺序的敏感性，并分析导致失败的推理模式。

Result: 在15个前沿模型中，智能体仅在62%的情况下选择对社会有益的行动，经常导致有害结果。博弈论干预可以将社会有益结果提高18%。研究揭示了显著的可靠性差距。

Conclusion: GT-HarmBench为研究多智能体环境中的对齐问题提供了广泛的标准测试平台，揭示了前沿AI系统在多智能体协调方面的重大安全缺陷，并展示了博弈论干预的改进潜力。

Abstract: Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.

</details>


### [2] [Evolving Beyond Snapshots: Harmonizing Structure and Sequence via Entity State Tuning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2602.12389)
*Siyuan Li,Yunjia Wu,Yiyong Xiao,Pingyang Huang,Peize Li,Ruitong Liu,Yan Wen,Te Sun,Fangyi Pei*

Main category: cs.AI

TL;DR: EST框架通过维护持续演化的实体状态来解决TKG预测中的长期依赖问题，显著提升了多种骨干模型的性能


<details>
  <summary>Details</summary>
Motivation: 现有TKG预测方法大多是无状态的，每次时间戳都从有限查询窗口重新计算实体表示，导致"情景性遗忘"和长期依赖快速衰减

Method: 提出Entity State Tuning（EST）框架：1）拓扑感知状态感知器注入实体状态先验；2）统一时序上下文模块聚合状态增强事件；3）双轨演化机制更新全局实体状态内存，平衡可塑性与稳定性

Result: 在多个基准测试中，EST持续改进不同骨干模型并达到最先进性能，证明了状态持久性对长期TKG预测的重要性

Conclusion: EST框架通过赋予TKG预测器持久且持续演化的实体状态，有效解决了长期依赖问题，为TKG预测提供了新的解决方案

Abstract: Temporal knowledge graph (TKG) forecasting requires predicting future facts by jointly modeling structural dependencies within each snapshot and temporal evolution across snapshots. However, most existing methods are stateless: they recompute entity representations at each timestamp from a limited query window, leading to episodic amnesia and rapid decay of long-term dependencies. To address this limitation, we propose Entity State Tuning (EST), an encoder-agnostic framework that endows TKG forecasters with persistent and continuously evolving entity states. EST maintains a global state buffer and progressively aligns structural evidence with sequential signals via a closed-loop design. Specifically, a topology-aware state perceiver first injects entity-state priors into structural encoding. Then, a unified temporal context module aggregates the state-enhanced events with a pluggable sequence backbone. Subsequently, a dual-track evolution mechanism writes the updated context back to the global entity state memory, balancing plasticity against stability. Experiments on multiple benchmarks show that EST consistently improves diverse backbones and achieves state-of-the-art performance, highlighting the importance of state persistence for long-horizon TKG forecasting. The code is published at https://github.com/yuanwuyuan9/Evolving-Beyond-Snapshots

</details>


### [3] [To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.12566)
*Haoqing Wang,Xiang Long,Ziheng Li,Yilong Xu,Tingguang Li,Yehui Tang*

Main category: cs.AI

TL;DR: 该论文研究了在多领域强化学习与可验证奖励（RLVR）中不同训练范式的比较，发现跨领域RLVR存在相互增益效应，特别是推理密集型领域之间表现出协同作用。


<details>
  <summary>Details</summary>
Motivation: 当前多领域专家级模型需要跨领域RLVR协作，但现有两种主要训练范式（混合多任务RLVR和单独训练后模型合并）缺乏详细的比较分析，需要系统研究不同范式在多领域RLVR中的效果。

Method: 选择数学、编程、科学和指令跟随等多个常用高级任务作为目标领域，使用开源数据集设计广泛的定性和定量实验，分析权重空间几何、模型预测行为和信息约束等内部机制。

Result: 发现跨领域RLVR存在较少相互干扰，推理密集型领域之间表现出相互协同效应，从权重空间几何、模型预测行为和信息约束角度分析了相互增益的内部机制。

Conclusion: 该研究为多领域RLVR训练范式选择提供了实证依据，揭示了跨领域协同效应的存在，特别是推理密集型任务之间的相互增益，对构建通用多领域专家模型具有重要意义。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, and the homepage is at https://github.com/mosAI25/M2RL

</details>


### [4] [Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models](https://arxiv.org/abs/2602.12586)
*Joshua Ong Jun Leang,Yu Zhao,Mihaela Cătălina Stoian,Wenda Li,Shay B. Cohen,Eleonora Giunchiglia*

Main category: cs.AI

TL;DR: McDiffuSE使用蒙特卡洛树搜索优化掩码扩散模型中的槽填充顺序，通过前瞻模拟评估部分完成情况，在数学和代码推理任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 虽然基于掩码扩散模型的计划-填充解码在数学和代码推理中显示出潜力，但性能对槽填充顺序高度敏感，导致输出方差较大。需要一种系统方法来优化填充顺序以提高生成质量。

Method: 提出McDiffuSE框架，将槽选择建模为决策过程，使用蒙特卡洛树搜索优化填充顺序。通过前瞻模拟评估部分完成情况，在承诺前系统探索生成顺序的组合空间。

Result: 实验显示平均比自回归基线提升3.2%，比基线计划-填充方法提升8.0%。在MBPP上获得19.5%的显著提升，在MATH500上提升4.9%。分析发现McDiffuSE主要遵循顺序生成，但结合非顺序生成对最大化性能至关重要。

Conclusion: 蒙特卡洛树搜索规划是提升掩码扩散模型生成质量的有效方法。研究发现较大的探索常数（而非更多模拟）对于克服模型置信度偏差和发现有效顺序是必要的。

Abstract: While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.

</details>


### [5] [Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents](https://arxiv.org/abs/2602.12662)
*Ruihan Yang,Fanghua Ye,Xiang We,Ruoqing Zhao,Kang Luo,Xinbo Xu,Bo Zhao,Ruotian Ma,Shanyi Wang,Zhaopeng Tu,Xiaolong Li,Deqing Yang,Linus*

Main category: cs.AI

TL;DR: CogRouter是一个让LLM智能体动态调整认知深度的框架，通过分层认知级别和两阶段训练，在长视野任务中实现高效决策，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在决策任务中使用固定的认知模式：非思考模型直接生成响应，思考模型则统一进行深度推理。这种刚性对于长视野任务效率低下，因为不同步骤的认知需求差异很大，有些需要战略规划，有些只需常规执行。

Method: 基于ACT-R理论设计了四个分层认知级别（从本能反应到战略规划）。采用两阶段训练：认知感知监督微调（CoSFT）建立稳定的级别特定模式，认知感知策略优化（CoPO）通过置信度感知优势重加权进行步骤级信用分配。核心洞察是适当的认知深度应最大化最终行动的置信度。

Result: 在ALFWorld和ScienceWorld实验中，CogRouter达到最先进性能且效率优越。使用Qwen2.5-7B模型，达到82.3%的成功率，优于GPT-4o（+40.3%）、OpenAI-o3（+18.3%）和GRPO（+14.0%），同时使用62%更少的token。

Conclusion: CogRouter框架通过动态调整认知深度，显著提高了LLM智能体在长视野决策任务中的效率和性能，验证了自适应认知策略的有效性。

Abstract: Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.

</details>


### [6] [Evaluating Robustness of Reasoning Models on Parameterized Logical Problems](https://arxiv.org/abs/2602.12665)
*Naïm Es-sebbani,Esteban Marquer,Yakoub Salhi,Zied Bouraoui*

Main category: cs.AI

TL;DR: 该论文提出了一个用于评估LLM推理器的诊断性2-SAT基准，通过参数化公式家族分离表面难度与结构现象，揭示LLM在特定结构干预下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 传统SAT基准常将表面难度（长度、措辞、子句顺序）与决定可满足性的结构现象混为一谈，无法准确评估LLM推理器的真实能力。需要构建能够分离这些因素的诊断性基准。

Method: 设计了基于参数化2-CNF公式家族的诊断基准，通过五种生成器隔离不同能力：矛盾循环UNSAT核心、控制解多样性的SAT实例、预设骨干变量、延迟桥接子句、对称/重复变体。评估LLM在决策准确性和赋值有效性上的表现，并测试语义保持扰动下的鲁棒性。

Result: 在表面统计特征固定的情况下，LLM在特定结构干预下表现出急剧的性能转变，揭示了在聚合SAT准确率中不可见的脆弱性区域。不同模型都显示出对结构变化的敏感性。

Conclusion: 该诊断基准能够有效揭示LLM推理器的结构脆弱性，为评估和改进LLM的逻辑推理能力提供了更精细的工具。结构干预比表面特征更能暴露LLM的推理局限性。

Abstract: Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.

</details>


### [7] [SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks](https://arxiv.org/abs/2602.12670)
*Xiangyi Li,Wenbo Chen,Yimin Liu,Shenghan Zheng,Xiaokun Chen,Yifeng He,Yubo Li,Bingran You,Haotian Shen,Jiankai Sun,Shuyi Wang,Qunhong Zeng,Di Wang,Xuandong Zhao,Yuanli Wang,Roey Ben Chaim,Zonglin Di,Yipeng Gao,Junwei He,Yizhuo He,Liqiang Jing,Luyang Kong,Xin Lan,Jiachen Li,Songlin Li,Yijiang Li,Yueqian Lin,Xinyi Liu,Xuanqing Liu,Haoran Lyu,Ze Ma,Bowei Wang,Runhui Wang,Tianyu Wang,Wengao Ye,Yue Zhang,Hanwen Xing,Yiqi Xue,Steven Dillmann,Han-chung Lee*

Main category: cs.AI

TL;DR: SkillsBench基准测试评估Agent Skills对LLM智能体的影响，发现精心设计的技能能平均提升16.2个百分点，但效果因领域差异大，自生成技能无益，小而精的技能优于全面文档，小模型配技能可媲美大模型。


<details>
  <summary>Details</summary>
Motivation: 尽管Agent Skills（结构化程序知识包）在推理时增强LLM智能体方面被迅速采用，但目前缺乏标准方法来衡量它们是否真正有效。需要系统评估技能对智能体性能的实际影响。

Method: 提出SkillsBench基准测试，包含11个领域的86个任务，每个任务配备精心设计的技能和确定性验证器。在三种条件下评估：无技能、精心设计技能、自生成技能。测试7种智能体-模型配置，共7,308条轨迹。

Result: 精心设计的技能平均通过率提升16.2个百分点，但效果差异显著：软件工程领域仅提升4.5个百分点，医疗领域提升51.9个百分点，84个任务中有16个显示负增长。自生成技能平均无益。包含2-3个模块的聚焦技能优于全面文档。配备技能的小模型性能可媲美无技能的大模型。

Conclusion: 精心设计的Agent Skills能显著提升LLM智能体性能，但效果高度依赖领域和任务特性。模型无法可靠地生成它们能从中受益的程序知识。小而精的技能设计策略更有效，技能配置可使小模型达到大模型水平。

Abstract: Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.

</details>


### [8] [X-SYS: A Reference Architecture for Interactive Explanation Systems](https://arxiv.org/abs/2602.12748)
*Tobias Labarta,Nhi Hoang,Maximilian Dreyer,Jim Berend,Oleg Hein,Jackie Ma,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.AI

TL;DR: 论文提出X-SYS参考架构，将可解释AI从算法问题转变为信息系统问题，通过STAR质量属性和五组件分解，为交互式解释系统提供可重用蓝图，并在SemanticLens系统中实现验证。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI研究虽然提出了众多技术方法，但将可解释性部署为实际系统仍面临挑战。交互式解释系统需要合适的算法和系统能力，以在重复查询、模型数据演进和治理约束下保持解释可用性。作者认为需要将可解释AI视为信息系统问题，用户交互需求会引发特定的系统要求。

Method: 提出X-SYS参考架构，围绕STAR四个质量属性（可扩展性、可追溯性、响应性、适应性）组织，指定五组件分解（XUI服务、解释服务、模型服务、数据服务、编排与治理）。通过将交互模式映射到系统能力，实现用户界面演进与后端计算的解耦。在SemanticLens系统中实现该架构，展示基于契约的服务边界、离线/在线分离和持久状态管理等机制。

Result: X-SYS为交互式解释系统提供了可重用的蓝图，SemanticLens系统作为具体实例化，展示了如何通过契约式服务边界支持独立演进、离线/在线分离确保响应性、持久状态管理支持可追溯性。该工作为在操作约束下支持端到端设计的交互式解释系统提供了架构指导。

Conclusion: 将可解释AI操作化需要将其视为信息系统问题，X-SYS参考架构通过STAR质量属性和五组件分解，为(X)AI研究者、开发者和从业者提供了连接交互式解释用户界面与系统能力的指导框架，支持在操作约束下的端到端系统设计。

Abstract: The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.

</details>


### [9] [Information-theoretic analysis of world models in optimal reward maximizers](https://arxiv.org/abs/2602.12963)
*Alfred Harwood,Jose Faustino,Alex Altair*

Main category: cs.AI

TL;DR: 论文通过信息论方法量化了最优策略对环境信息的揭示程度，证明了确定性最优策略恰好传递n log m比特的环境信息


<details>
  <summary>Details</summary>
Motivation: 研究AI领域中一个重要问题：成功行为在多大程度上需要内部世界表示。量化最优策略提供的关于底层环境的信息量，为理解"隐式世界模型"提供理论基础

Method: 采用受控马尔可夫过程模型，假设n个状态和m个动作，在可能的转移动态空间上采用均匀先验。通过信息论分析，证明观察确定性最优策略与环境之间的互信息

Result: 证明了对于任何非常数奖励函数的最优确定性策略，恰好传递n log m比特的环境信息。该界限适用于多种目标函数，包括有限时域、无限时域折扣和时间平均奖励最大化

Conclusion: 研究结果为最优性所需的"隐式世界模型"提供了精确的信息论下界，表明最优策略必然编码了特定量的环境结构信息

Abstract: An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the "implicit world model'' necessary for optimality.

</details>


### [10] [Consistency of Large Reasoning Models Under Multi-Turn Attacks](https://arxiv.org/abs/2602.13093)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.AI

TL;DR: 该研究评估了9个前沿推理模型在对抗攻击下的表现，发现推理能力能提供一定但非完全的鲁棒性，所有模型都有独特的脆弱性特征，并识别出5种主要失败模式。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型在复杂任务上表现出色，但它们在多轮对抗压力下的鲁棒性尚未得到充分探索。研究者希望评估推理模型在面对对抗攻击时的表现，了解推理能力是否能自动带来对抗鲁棒性。

Method: 研究评估了9个前沿推理模型在对抗攻击下的表现，通过轨迹分析识别失败模式，并测试了置信度感知响应生成（CARG）防御方法的有效性。

Result: 研究发现：1）推理提供有意义但不完全的鲁棒性，推理模型显著优于指令调优基线；2）所有模型都有独特的脆弱性特征，误导性建议普遍有效，社会压力具有模型特异性；3）识别出5种失败模式（自我怀疑、社会从众、建议劫持、情绪易感性、推理疲劳），前两种占50%失败；4）CARG防御对推理模型失效，因为扩展推理轨迹导致过度自信，随机置信度嵌入反而优于针对性提取。

Conclusion: 推理能力不会自动带来对抗鲁棒性，基于置信度的防御方法需要为推理模型进行根本性重新设计。研究强调了推理模型在对抗环境中的脆弱性，为未来鲁棒推理模型的发展提供了重要见解。

Abstract: Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.

</details>


### [11] [Constrained Assumption-Based Argumentation Frameworks](https://arxiv.org/abs/2602.13135)
*Emanuele De Angelis,Fabio Fioravanti,Maria Chiara Meo,Alberto Pettorossi,Maurizio Proietti,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一种新的约束ABA（CABA）框架，通过引入约束变量来扩展传统ABA的表达能力，使其能够处理非地面（包含变量）的论证和攻击。


<details>
  <summary>Details</summary>
Motivation: 传统基于假设的论证（ABA）框架通常局限于原子语言和地面（无变量）论证，这限制了其表示能力。为了克服这一限制，需要扩展ABA框架以支持包含约束变量的非地面论证。

Method: 提出约束ABA（CABA）框架，允许组件和论证中包含约束变量，这些变量可以在可能无限的域上取值。定义了基于各种非地面攻击概念的CABA非地面语义。

Result: 新的CABA语义能够保守地推广标准ABA语义，证明新框架在表达能力增强的同时保持了与传统ABA的兼容性。

Conclusion: CABA框架成功扩展了传统ABA的表达能力，通过引入约束变量支持非地面论证，为结构化论证提供了更强大的表示工具，同时保持了与现有理论的向后兼容性。

Abstract: Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.

</details>


### [12] [Optimal Take-off under Fuzzy Clearances](https://arxiv.org/abs/2602.13166)
*Hugo Henry,Arthur Tsai,Kelly Cohen*

Main category: cs.AI

TL;DR: 本文提出了一种混合障碍物避让架构，将最优控制与模糊规则系统结合，为无人机提供自适应约束处理，但发现软件兼容性问题导致约束无法正常执行。


<details>
  <summary>Details</summary>
Motivation: 传统最优控制在不确定性下的局限性，以及航空安全关键系统需要可解释的决策制定，促使开发这种混合架构。

Method: 设计了三阶段Takagi-Sugeno-Kang模糊层，基于FAA和EASA的监管分离最小值和适航指南，调制约束半径、紧急级别和激活决策，然后将模糊推导的净空作为软约束纳入最优控制问题，使用FALCON工具箱和IPOPT求解。

Result: 概念验证显示每次迭代计算时间为2-3秒，具有近实时应用潜力，但发现FALCON和IPOPT最新版本存在软件不兼容问题，拉格朗日惩罚项恒为零，导致约束无法正常执行。

Conclusion: 该方法在计算效率上表现良好，但软件兼容性问题需要解决。未来工作包括验证软件回归问题、优化模糊隶属函数，以及扩展到更高保真度飞机模型和随机障碍环境。

Abstract: This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.

</details>

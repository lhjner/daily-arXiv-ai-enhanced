<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge](https://arxiv.org/abs/2602.17826)
*Marcelo Labre*

Main category: cs.AI

TL;DR: 该研究探讨了使用形式化领域本体（OpenMath）通过检索增强生成来提升语言模型在数学推理任务中的可靠性，发现本体引导的上下文在检索质量高时能提升性能，但无关上下文会降低性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型存在幻觉、脆弱性和缺乏形式化基础等根本限制，这在需要可验证推理的高风险专业领域（如数学）中尤为成问题。研究者希望探索形式化领域本体是否能通过检索增强生成来增强语言模型的可靠性。

Method: 使用数学作为概念验证，实现了一个神经符号管道，利用OpenMath本体，结合混合检索和交叉编码器重排序技术，将相关定义注入模型提示中。在MATH基准测试中评估了三个开源模型。

Result: 评估结果显示，本体引导的上下文在检索质量高时能提高模型性能，但无关上下文会主动降低性能。这突显了神经符号方法的潜力和挑战。

Conclusion: 形式化领域本体可以增强语言模型的可靠性，但检索质量至关重要。无关上下文会损害性能，表明神经符号方法需要精确的检索机制来发挥其潜力。

Abstract: Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.

</details>


### [2] [The Token Games: Evaluating Language Model Reasoning with Puzzle Duels](https://arxiv.org/abs/2602.17831)
*Simon Henniger,Gabriel Poesia*

Main category: cs.AI

TL;DR: TTG是一个基于16世纪数学决斗启发的评估框架，让大语言模型通过创建编程谜题相互挑战，自动评估推理能力，无需人工出题。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型推理能力的挑战：人工出题成本高（特别是需要博士级领域知识的难题），且难以确定模型是否真正推理还是见过类似训练数据。需要一种无法通过设计饱和的评估范式。

Method: 采用编程谜题格式：给定返回布尔值的Python函数，寻找使其返回True的输入。模型相互创建谜题挑战对方，通过两两对决计算Elo评分来比较模型相对能力。

Result: 评估了10个前沿模型，TTG的排名与现有基准（如Humanity's Last Exam）高度匹配，且无需人工创建谜题。发现创建优质谜题对当前模型仍是极具挑战的任务，这是先前基准未测量的能力。

Conclusion: TTG提出了一种新的评估范式，既无法通过设计饱和，又能同时测试模型的创造力、任务创建能力和问题解决能力，为评估推理能力提供了新方向。

Abstract: Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.

</details>


### [3] [WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics](https://arxiv.org/abs/2602.17990)
*Madhav Kanda,Pedro Las-Casas,Alok Gautam Kumbhare,Rodrigo Fonseca,Sharad Agarwal*

Main category: cs.AI

TL;DR: WorkflowPerturb是一个用于评估工作流评估指标的基准测试，通过对黄金工作流施加受控扰动来研究指标的性能


<details>
  <summary>Details</summary>
Motivation: LLM生成的结构化工作流评估困难，因为指标分数通常未校准，且分数变化不能直接反映工作流退化的严重程度

Method: 通过向黄金工作流施加三种类型的受控扰动（缺失步骤、压缩步骤、描述变化），每种类型在10%、30%、50%的严重级别上应用，创建包含4,973个黄金工作流和44,757个扰动变体的基准数据集

Result: 基准测试了多个指标家族，使用预期分数轨迹和残差分析其敏感性和校准性，揭示了不同指标家族之间的系统性差异

Conclusion: 研究结果支持基于严重程度的工作流评估分数解释，数据集将在接受后发布，有助于改进工作流评估指标的设计和使用

Abstract: LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.

</details>


### [4] [Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets](https://arxiv.org/abs/2602.18025)
*Haruki Abe,Takayuki Osa,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.AI

TL;DR: 该研究结合离线强化学习和跨具身学习来解决机器人策略预训练中高质量演示数据稀缺的问题，通过构建包含16个机器人平台的运动数据集进行系统分析，并提出基于形态相似性的分组策略来缓解跨机器人梯度冲突。


<details>
  <summary>Details</summary>
Motivation: 机器人策略预训练面临高质量演示数据收集成本高的问题，需要找到既能利用专家数据又能利用大量次优数据的方法，同时还要处理不同机器人形态的异质性。

Method: 结合离线强化学习和跨具身学习，构建包含16个不同机器人平台的运动数据集进行系统分析。为缓解跨机器人形态的梯度冲突，提出基于形态相似性的分组策略，将机器人按形态相似性聚类，使用组梯度更新模型。

Result: 实验证实，结合离线RL和跨具身学习的方法在次优轨迹丰富的数据集上预训练表现优异，优于纯行为克隆。但随着次优数据比例和机器人类型增加，跨形态的梯度冲突会阻碍学习。提出的分组策略能显著减少机器人间冲突，优于现有冲突解决方法。

Conclusion: 离线RL与跨具身学习的结合为机器人策略预训练提供了有效途径，特别是在次优数据丰富的场景下。基于形态相似性的分组策略是缓解跨机器人梯度冲突的简单有效方法，为大规模机器人学习提供了实用解决方案。

Abstract: Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.

</details>


### [5] [SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps](https://arxiv.org/abs/2602.18201)
*Joseph Bingham,Netanel Arussy,Dvir Aran*

Main category: cs.AI

TL;DR: 研究发现，即使训练时排除敏感属性，无监督表示仍会编码这些属性，公平性通过无知在表示层面失效


<details>
  <summary>Details</summary>
Motivation: 挑战无监督表示对敏感属性保持中性的普遍假设，揭示公平性通过无知在表示层面的局限性

Method: 使用SOMtime（基于高容量自组织映射的拓扑保持表示方法），在两个大规模真实数据集（世界价值观调查和人口普查收入数据集）上测试，与PCA、UMAP、t-SNE和自编码器对比

Result: SOMtime恢复了与被排除敏感属性对齐的单调排序，Spearman相关性高达0.85，而其他方法通常低于0.23；无监督分割产生人口统计学偏斜的聚类

Conclusion: 公平性通过无知在表示层面对序数敏感属性失效，公平性审计必须扩展到机器学习管道的无监督组件

Abstract: Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime

</details>


### [6] [Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies](https://arxiv.org/abs/2602.18291)
*Zhuoran Li,Hai Zhong,Xun Wang,Qingxin Xia,Lihua Zhang,Longbo Huang*

Main category: cs.AI

TL;DR: OMAD：首个在线多智能体强化学习框架，使用扩散策略协调智能体，通过放松策略目标最大化联合熵实现有效探索，在MPE和MAMuJoCo任务上实现2.5-5倍样本效率提升


<details>
  <summary>Details</summary>
Motivation: 在线多智能体强化学习需要增强策略表达能力以获得更好性能。扩散模型在图像生成和离线设置中表现出卓越的表达能力和多模态表示能力，但在在线MARL中应用不足，主要障碍是扩散模型的不可处理似然性阻碍了基于熵的探索和协调。

Method: 提出OMAD框架：1）放松策略目标最大化缩放联合熵，实现有效探索而不依赖可处理似然；2）在CTDE范式下使用联合分布值函数优化分散扩散策略；3）利用可处理的熵增强目标指导扩散策略同步更新，确保稳定协调。

Result: 在MPE和MAMuJoCo的10个多样化任务上建立新的最先进水平，展示了2.5倍到5倍的样本效率显著提升。

Conclusion: OMAD是首个使用扩散策略的在线多智能体强化学习框架，通过放松策略目标和联合分布值函数有效解决了扩散模型在在线MARL中的应用障碍，实现了卓越的协调性能和样本效率。

Abstract: Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.

</details>

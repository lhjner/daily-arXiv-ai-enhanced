<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [HumanMCP: A Human-Like Query Dataset for Evaluating MCP Tool Retrieval Performance](https://arxiv.org/abs/2602.23367)
*Shubh Laddha,Lucas Changbencharoen,Win Kuptivej,Surya Shringla,Archana Vaidheeswaran,Yash Bhaskar*

Main category: cs.AI

TL;DR: 本文介绍了首个大规模MCP数据集，包含针对308个MCP服务器中2800个工具的多样化高质量用户查询，解决了现有数据集缺乏真实用户查询模式的问题。


<details>
  <summary>Details</summary>
Motivation: 现有MCP服务器工具使用数据集和基准测试缺乏真实、人性化的用户查询，无法反映不同用户如何表达请求，导致泛化能力差和基准测试可靠性被夸大。

Method: 基于MCP Zero数据集，为308个MCP服务器中的2800个工具生成多样化高质量用户查询，每个工具配对多个独特的用户角色，涵盖从精确任务请求到模糊探索性命令的不同用户意图层次。

Result: 创建了首个大规模MCP数据集，包含针对2800个工具的多样化用户查询，反映了真实世界交互模式的复杂性。

Conclusion: 该数据集填补了MCP服务器工具使用评估的关键空白，能够更准确地评估工具使用和生态系统，为未来研究提供了更真实的基准。

Abstract: Model Context Protocol (MCP) servers contain a collection of thousands of open-source standardized tools, linking LLMs to external systems; however, existing datasets and benchmarks lack realistic, human-like user queries, remaining a critical gap in evaluating the tool usage and ecosystems of MCP servers. Existing datasets often do contain tool descriptions but fail to represent how different users portray their requests, leading to poor generalization and inflated reliability of certain benchmarks. This paper introduces the first large-scale MCP dataset featuring diverse, high-quality diverse user queries generated specifically to match 2800 tools across 308 MCP servers, developing on the MCP Zero dataset. Each tool is paired with multiple unique user personas that we have generated, to capture varying levels of user intent ranging from precise task requests, and ambiguous, exploratory commands, reflecting the complexity of real-world interaction patterns.

</details>


### [2] [Causal Identification from Counterfactual Data: Completeness and Bounding Results](https://arxiv.org/abs/2602.23541)
*Arvind Raghavan,Elias Bareinboim*

Main category: cs.AI

TL;DR: 本文提出了CTFIDU+算法，用于从可实现的第3层反事实分布中识别反事实查询，证明了该算法的完备性，并建立了非参数因果推理的极限。


<details>
  <summary>Details</summary>
Motivation: 先前关于反事实识别完备性的研究局限于观测或干预分布（因果层级的第1、2层），因为一般认为无法获得第3层的反事实分布数据。然而，最近研究发现某些反事实分布可以通过实验方法直接估计（称为反事实可实现性），这引出了新的问题：在获得部分第3层数据的情况下，哪些额外的反事实量变得可识别？

Method: 开发了CTFIDU+算法，用于从任意第3层分布集合中识别反事实查询。该算法基于反事实可实现性的概念，能够处理通过实验方法直接估计的反事实分布数据。

Result: 证明了CTFIDU+算法对于从可实现的反事实分布中识别反事实查询是完备的。建立了从物理可实现分布中识别反事实的理论极限，这暗示了非参数设置中精确因果推理的基本极限。对于某些不可识别的关键反事实类型，推导了使用可实现反事实数据的新解析边界。

Conclusion: 本文通过CTFIDU+算法和理论分析，明确了在可获得部分反事实数据的情况下因果推理的能力边界。对于不可识别的反事实量，反事实数据在实践中有助于收紧边界，这通过仿真得到了验证。研究揭示了非参数因果推理的基本极限。

Abstract: Previous work establishing completeness results for $\textit{counterfactual identification}$ has been circumscribed to the setting where the input data belongs to observational or interventional distributions (Layers 1 and 2 of Pearl's Causal Hierarchy), since it was generally presumed impossible to obtain data from counterfactual distributions, which belong to Layer 3. However, recent work (Raghavan & Bareinboim, 2025) has formally characterized a family of counterfactual distributions which can be directly estimated via experimental methods - a notion they call $\textit{counterfactual realizabilty}$. This leaves open the question of what $\textit{additional}$ counterfactual quantities now become identifiable, given this new access to (some) Layer 3 data. To answer this question, we develop the CTFIDU+ algorithm for identifying counterfactual queries from an arbitrary set of Layer 3 distributions, and prove that it is complete for this task. Building on this, we establish the theoretical limit of which counterfactuals can be identified from physically realizable distributions, thus implying the $\textit{fundamental limit to exact causal inference in the non-parametric setting}$. Finally, given the impossibility of identifying certain critical types of counterfactuals, we derive novel analytic bounds for such quantities using realizable counterfactual data, and corroborate using simulations that counterfactual data helps tighten the bounds for non-identifiable quantities in practice.

</details>


### [3] [Planning under Distribution Shifts with Causal POMDPs](https://arxiv.org/abs/2602.23545)
*Matteo Ceriscioli,Karthika Mohan*

Main category: cs.AI

TL;DR: 提出一个基于因果知识的POMDP理论框架，用于处理部分可观测环境下的分布偏移问题，通过将环境变化表示为对因果POMDP的干预，保持值函数的PWLC性质以确保规划可处理性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的规划经常面临分布偏移的挑战，在一个条件下获得的环境模型在状态分布或环境动态变化时可能失效，导致先前学习的策略失败。需要处理部分可观测性和环境变化同时存在的复杂情况。

Method: 使用基于因果知识的部分可观测马尔可夫决策过程（POMDP）理论框架，将环境变化表示为对因果POMDP的干预。维护和更新关于潜在状态和底层领域的信念，证明在增强的信念空间中值函数保持分段线性凸（PWLC）性质。

Result: 证明了值函数在增强的信念空间中保持分段线性凸（PWLC）性质，这一性质保留了基于α向量的POMDP方法的可处理性，使得在分布偏移下仍然可以进行有效的规划。

Conclusion: 提出的因果POMDP框架为处理部分可观测环境下的分布偏移问题提供了理论基础，通过保持值函数的PWLC性质确保了规划的可处理性，能够评估假设变化下的计划并主动识别环境中的变化组件。

Abstract: In the real world, planning is often challenged by distribution shifts. As such, a model of the environment obtained under one set of conditions may no longer remain valid as the distribution of states or the environment dynamics change, which in turn causes previously learned strategies to fail. In this work, we propose a theoretical framework for planning under partial observability using Partially Observable Markov Decision Processes (POMDPs) formulated using causal knowledge. By representing shifts in the environment as interventions on this causal POMDP, the framework enables evaluating plans under hypothesized changes and actively identifying which components of the environment have been altered. We show how to maintain and update a belief over both the latent state and the underlying domain, and we prove that the value function remains piecewise linear and convex (PWLC) in this augmented belief space. Preservation of PWLC under distribution shifts has the advantage of maintaining the tractability of planning via $α$-vector-based POMDP methods.

</details>


### [4] [Construct, Merge, Solve & Adapt with Reinforcement Learning for the min-max Multiple Traveling Salesman Problem](https://arxiv.org/abs/2602.23579)
*Guillem Rodríguez-Corominas,Maria J. Blesa,Christian Blum*

Main category: cs.AI

TL;DR: 提出RL-CMSA混合方法解决对称单仓库最小最大mTSP问题，结合强化学习引导的构造、精确优化和自适应池管理，在平衡工作负载方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多旅行商问题中的最小最大变体，旨在最小化最长路径以实现工作负载平衡，需要有效方法处理大规模实例和多个销售员的情况。

Method: 提出RL-CMSA混合方法：1) 使用基于学习到的成对q值进行概率聚类构造多样化解；2) 合并路径到紧凑池；3) 求解受限集合覆盖MILP；4) 通过跨路径移除、转移和交换操作精化解；5) 通过强化高质量解中的城市对共现更新q值；6) 通过老化和剪枝自适应调整池。

Result: 在随机和TSPLIB实例上的计算结果表明，RL-CMSA能一致找到（接近）最优解，在可比时间限制下优于最先进的混合遗传算法，特别是在实例规模和销售员数量增加时表现更优。

Conclusion: RL-CMSA结合精确优化和强化学习引导的构造，有效平衡探索与利用，为对称单仓库最小最大mTSP提供了高效解决方案，在处理大规模问题和多销售员场景时表现出色。

Abstract: The Multiple Traveling Salesman Problem (mTSP) extends the Traveling Salesman Problem to m tours that start and end at a common depot and jointly visit all customers exactly once. In the min-max variant, the objective is to minimize the longest tour, reflecting workload balance. We propose a hybrid approach, Construct, Merge, Solve & Adapt with Reinforcement Learning (RL-CMSA), for the symmetric single-depot min-max mTSP. The method iteratively constructs diverse solutions using probabilistic clustering guided by learned pairwise q-values, merges routes into a compact pool, solves a restricted set-covering MILP, and refines solutions via inter-route remove, shift, and swap moves. The q-values are updated by reinforcing city-pair co-occurrences in high-quality solutions, while the pool is adapted through ageing and pruning. This combination of exact optimization and reinforcement-guided construction balances exploration and exploitation. Computational results on random and TSPLIB instances show that RL-CMSA consistently finds (near-)best solutions and outperforms a state-of-the-art hybrid genetic algorithm under comparable time limits, especially as instance size and the number of salesmen increase.

</details>


### [5] [SleepLM: Natural-Language Intelligence for Human Sleep](https://arxiv.org/abs/2602.23605)
*Zongzhe Xu,Zitao Shuai,Eideen Mozaffari,Ravi S. Aysola,Rajesh Kumar,Yuzhe Yang*

Main category: cs.AI

TL;DR: SleepLM是一个睡眠-语言基础模型，通过自然语言和多模态多导睡眠图的对齐，实现了人类睡眠的解读和交互，在零样本和少样本学习等任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的睡眠分析系统工作在封闭的标签空间（如预定义的睡眠阶段或事件），无法描述、查询或泛化到新的睡眠现象，需要一种能够连接自然语言和睡眠生理数据的模型。

Method: 提出了多级睡眠描述生成流程，创建了首个大规模睡眠-文本数据集（超过10万小时数据，来自1万多个个体）；设计了统一的预训练目标，结合对比对齐、描述生成和信号重建，以更好地捕捉生理保真度和跨模态交互。

Result: 在真实世界的睡眠理解任务中，SleepLM在零样本和少样本学习、跨模态检索和睡眠描述生成方面优于现有技术；模型还展现出语言引导的事件定位、针对性洞察生成和对未见任务的零样本泛化等有趣能力。

Conclusion: SleepLM通过连接自然语言和多模态多导睡眠图，实现了睡眠生理的语言基础表示，为睡眠分析开辟了新途径，所有代码和数据将开源。

Abstract: We present SleepLM, a family of sleep-language foundation models that enable human sleep alignment, interpretation, and interaction with natural language. Despite the critical role of sleep, learning-based sleep analysis systems operate in closed label spaces (e.g., predefined stages or events) and fail to describe, query, or generalize to novel sleep phenomena. SleepLM bridges natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology. To support this alignment, we introduce a multilevel sleep caption generation pipeline that enables the curation of the first large-scale sleep-text dataset, comprising over 100K hours of data from more than 10,000 individuals. Furthermore, we present a unified pretraining objective that combines contrastive alignment, caption generation, and signal reconstruction to better capture physiological fidelity and cross-modal interactions. Extensive experiments on real-world sleep understanding tasks verify that SleepLM outperforms state-of-the-art in zero-shot and few-shot learning, cross-modal retrieval, and sleep captioning. Importantly, SleepLM also exhibits intriguing capabilities including language-guided event localization, targeted insight generation, and zero-shot generalization to unseen tasks. All code and data will be open-sourced.

</details>


### [6] [MMKG-RDS: Reasoning Data Synthesis via Deep Mining of Multimodal Knowledge Graphs](https://arxiv.org/abs/2602.23632)
*Lun Zhan,Feng Xiong,Huanyong Liu,Feng Zhang,Yuhui Yin*

Main category: cs.AI

TL;DR: MMKG-RDS是一个基于多模态知识图谱的推理数据合成框架，通过细粒度知识提取、可定制路径采样和多维质量评分，提升领域模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 现有方法在长尾知识覆盖、效果验证和可解释性方面存在局限，基于知识图谱的方法在功能性、粒度、可定制性和评估方面仍有不足

Method: 提出MMKG-RDS框架，利用多模态知识图谱支持细粒度知识提取、可定制路径采样和多维数据质量评分

Result: 使用MMKG-RDS-Bench数据集验证，涵盖5个领域、17种任务类型和14,950个样本，微调Qwen3模型（0.6B/8B/32B）后推理准确率提升9.2%

Conclusion: MMKG-RDS能够生成高质量的训练数据，有效提升模型推理能力，并为涉及表格和公式的复杂基准测试构建提供支持

Abstract: Synthesizing high-quality training data is crucial for enhancing domain models' reasoning abilities. Existing methods face limitations in long-tail knowledge coverage, effectiveness verification, and interpretability. Knowledge-graph-based approaches still fall short in functionality, granularity, customizability, and evaluation. To address these issues, we propose MMKG-RDS, a flexible framework for reasoning data synthesis that leverages multimodal knowledge graphs. It supports fine-grained knowledge extraction, customizable path sampling, and multidimensional data quality scoring. We validate MMKG-RDS with the MMKG-RDS-Bench dataset, covering five domains, 17 task types, and 14,950 samples. Experimental results show fine-tuning Qwen3 models (0.6B/8B/32B) on a small number of synthesized samples improves reasoning accuracy by 9.2%. The framework also generates distinct data, challenging existing models on tasks involving tables and formulas, useful for complex benchmark construction. The dataset and code are available at https://github.com/360AILAB-NLP/MMKG-RDS

</details>


### [7] [AI Must Embrace Specialization via Superhuman Adaptable Intelligence](https://arxiv.org/abs/2602.23643)
*Judah Goldfeder,Philippe Wyder,Yann LeCun,Ravid Shwartz Ziv*

Main category: cs.AI

TL;DR: 论文批判当前对通用人工智能（AGI）的流行定义，认为人类本身并非"通用"，提出应放弃追求通用性，转而专注于发展"超人适应智能"（SAI），即能在重要任务上超越人类并填补人类能力空白的专业化AI。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域对AGI的定义混乱且不一致，许多讨论基于"AI能做人类能做的一切"这一有问题的假设。作者认为这种通用性概念本身存在缺陷，需要重新思考如何更好地描述AI的未来发展方向。

Method: 通过分析现有AGI定义的逻辑问题，论证人类本身并非真正通用，提出用"超人适应智能"（SAI）这一新概念替代AGI。SAI强调专业化而非通用性，追求在特定重要任务上超越人类表现并填补人类能力空白。

Result: 成功识别了当前AGI概念的缺陷，提出了更清晰、更有用的SAI框架。这一新概念能够帮助澄清因AGI定义过载而模糊的AI讨论，为AI未来发展提供更明确的指导方向。

Conclusion: AGI是一个有缺陷的概念，不应作为AI发展的目标。相反，AI应该追求专业化而非通用性，发展能够在重要任务上超越人类并填补人类能力空白的超人适应智能（SAI），这为AI的未来发展提供了更清晰、更实用的框架。

Abstract: Everyone from AI executives and researchers to doomsayers, politicians, and activists is talking about Artificial General Intelligence (AGI). Yet, they often don't seem to agree on its exact definition. One common definition of AGI is an AI that can do everything a human can do, but are humans truly general? In this paper, we address what's wrong with our conception of AGI, and why, even in its most coherent formulation, it is a flawed concept to describe the future of AI. We explore whether the most widely accepted definitions are plausible, useful, and truly general. We argue that AI must embrace specialization, rather than strive for generality, and in its specialization strive for superhuman performance, and introduce Superhuman Adaptable Intelligence (SAI). SAI is defined as intelligence that can learn to exceed humans at anything important that we can do, and that can fill in the skill gaps where humans are incapable. We then lay out how SAI can help hone a discussion around AI that was blurred by an overloaded definition of AGI, and extrapolate the implications of using it as a guide for the future.

</details>


### [8] [PseudoAct: Leveraging Pseudocode Synthesis for Flexible Planning and Action Control in Large Language Model Agents](https://arxiv.org/abs/2602.23668)
*Yihan,Wen,Xin Chen*

Main category: cs.AI

TL;DR: PseudoAct框架通过伪代码合成实现LLM智能体的灵活规划和行动控制，解决传统反应式决策在复杂长程任务中的冗余工具使用、不稳定推理和高token消耗问题。


<details>
  <summary>Details</summary>
Motivation: 传统LLM智能体依赖反应式决策范式（如ReAct），在涉及分支、迭代或多工具协调的复杂长程任务中，会导致冗余工具使用、不稳定推理和高token消耗，需要更有效的规划和控制方法。

Method: PseudoAct通过伪代码合成框架，利用LLM将任务解决策略表达为代码的能力，合成结构化伪代码计划，将任务分解为子任务并显式编码控制流（包括顺序、条件、循环、并行组合等逻辑原语），然后按照全局计划执行行动。

Result: 实验表明，该方法在基准数据集上显著优于现有反应式智能体方法，在FEVER上实现了20.93%的绝对成功率提升，并在HotpotQA上创造了新的最先进水平。

Conclusion: PseudoAct通过伪代码合成实现了显式和时序连贯的决策逻辑，减少了冗余行动，防止了无限循环，避免了无信息量的替代探索，实现了一致且高效的长程决策。

Abstract: Large language model (LLM) agents typically rely on reactive decision-making paradigms such as ReAct, selecting actions conditioned on growing execution histories. While effective for short tasks, these approaches often lead to redundant tool usage, unstable reasoning, and high token consumption in complex long-horizon tasks involving branching, iteration, or multi-tool coordination. To address these limitations, this paper introduces PseudoAct, a novel framework for flexible planning and action control in LLM agents through pseudocode synthesis. Leveraging the ability of LLMs to express task-solving strategies as code, PseudoAct synthesizes a structured pseudocode plan that decomposes a task into subtasks and explicitly encodes control flow, including sequencing, conditionals, loops, parallel composition, and combinations of these logic primitives. Actions are then executed by following this global plan, making the decision logic explicit and temporally coherent. This design reduces redundant actions, prevents infinite loops, and avoids uninformative alternative exploration, enabling consistent and efficient long-horizon decision-making. Experiments on benchmark datasets show that our method significantly outperforms existing reactive agent approaches, achieving a 20.93% absolute gain in success rate on FEVER and setting a new state-of-the-art on HotpotQA.

</details>


### [9] [From Flat Logs to Causal Graphs: Hierarchical Failure Attribution for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.23701)
*Yawen Wang,Wenjie Wu,Junjie Wang,Qing Wang*

Main category: cs.AI

TL;DR: CHIEF是一个新颖的框架，通过将混乱的多智能体系统轨迹转换为结构化层次因果图，使用层次化oracle引导回溯和渐进因果筛选策略，显著提升了故障归因的准确性。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统在复杂领域表现出色，但存在固有的脆弱性和不透明的故障机制。现有的故障归因方法通常将执行日志视为扁平序列，这种线性视角无法解耦MAS中复杂的因果联系，导致弱可观测性和模糊的责任边界。

Method: CHIEF框架包含三个核心模块：1) 将混乱轨迹转换为结构化层次因果图；2) 使用层次化oracle引导回溯，通过合成虚拟oracle高效剪枝搜索空间；3) 通过渐进因果筛选策略实现反事实归因，严格区分真正的根本原因和传播的症状。

Result: 在Who&When基准测试中，CHIEF在智能体级别和步骤级别准确性上都优于八个强大的最先进基线方法。消融研究进一步证实了每个提出模块的关键作用。

Conclusion: CHIEF通过结构化层次因果表示和高效的反事实分析，显著提升了多智能体系统故障归因的准确性和可解释性，为解决MAS的脆弱性和不透明性问题提供了有效方案。

Abstract: LLM-powered Multi-Agent Systems (MAS) have demonstrated remarkable capabilities in complex domains but suffer from inherent fragility and opaque failure mechanisms. Existing failure attribution methods, whether relying on direct prompting, costly replays, or supervised fine-tuning, typically treat execution logs as flat sequences. This linear perspective fails to disentangle the intricate causal links inherent to MAS, leading to weak observability and ambiguous responsibility boundaries. To address these challenges, we propose CHIEF, a novel framework that transforms chaotic trajectories into a structured hierarchical causal graph. It then employs hierarchical oracle-guided backtracking to efficiently prune the search space via sybthesized virtual oracles. Finally, it implements counterfactual attribution via a progressive causal screening strategy to rigorously distinguish true root causes from propagated symptoms. Experiments on Who&When benchmark show that CHIEF outperforms eight strong and state-of-the-art baselines on both agent- and step-level accuracy. Ablation studies further confirm the critical role of each proposed module.

</details>


### [10] [ProductResearch: Training E-Commerce Deep Research Agents via Multi-Agent Synthetic Trajectory Distillation](https://arxiv.org/abs/2602.23716)
*Jiangyuan Wang,Kejun Xiao,Huaipeng Zhao,Tao Luo,Xiaoyi Zeng*

Main category: cs.AI

TL;DR: 提出ProductResearch多智能体框架，通过合成高质量工具使用轨迹来训练电商购物智能体，显著提升LLM在复杂产品研究任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有LLM电商购物智能体缺乏深度交互和上下文广度，而深度研究范式在电商领域存在领域差距，需要专门解决方案

Method: 采用多智能体框架：用户智能体推断购物意图，监督智能体协调研究智能体迭代协作生成合成轨迹，通过反思内化过程将多智能体交互转化为单角色训练样本

Result: 基于合成数据微调的紧凑MoE模型在响应全面性、研究深度和用户感知效用方面显著优于基础模型，接近前沿专有深度研究系统性能

Conclusion: 多智能体合成轨迹训练是增强LLM购物辅助的有效可扩展范式，能够生成高质量训练数据提升电商对话购物智能体的能力

Abstract: Large Language Model (LLM)-based agents show promise for e-commerce conversational shopping, yet existing implementations lack the interaction depth and contextual breadth required for complex product research. Meanwhile, the Deep Research paradigm, despite advancing information synthesis in web search, suffers from domain gaps when transferred to e-commerce. We propose ProductResearch, a multi-agent framework that synthesizes high-fidelity, long-horizon tool-use trajectories for training robust e-commerce shopping agents. The framework employs a User Agent to infer nuanced shopping intents from behavioral histories, and a Supervisor Agent that orchestrates iterative collaboration with a Research Agent to generate synthetic trajectories culminating in comprehensive, insightful product research reports. These trajectories are rigorously filtered and distilled through a reflective internalization process that consolidates multi-agent supervisory interactions into coherent single-role training examples, enabling effective fine-tuning of LLM agents for complex shopping inquiries. Extensive experiments show that a compact MoE model fine-tuned on our synthetic data achieves substantial improvements over its base model in response comprehensiveness, research depth, and user-perceived utility, approaching the performance of frontier proprietary deep research systems and establishing multi-agent synthetic trajectory training as an effective and scalable paradigm for enhancing LLM-based shopping assistance.

</details>


### [11] [The Auton Agentic AI Framework](https://arxiv.org/abs/2602.23720)
*Sheng Cao,Zhao Chang,Chang Li,Hannan Li,Liyao Fu,Ji Tang*

Main category: cs.AI

TL;DR: Auton Agentic AI Framework：一个标准化自主智能体创建、执行和治理的架构，通过认知蓝图与运行时引擎分离解决LLM输出与后端基础设施需求不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: AI领域正从生成式AI向智能体AI过渡，但LLM的随机非结构化输出与后端基础设施所需的确定性、模式一致输入之间存在架构不匹配问题。

Method: 提出Auton Agentic AI Framework，采用认知蓝图（声明式、语言无关的智能体身份和能力规范）与运行时引擎（平台特定执行基板）严格分离的架构，支持模型上下文协议（MCP）的模块化工具集成。

Result: 框架实现了跨语言可移植性、形式化可审计性和模块化工具集成，通过增强的部分可观测马尔可夫决策过程、分层记忆整合、约束流形形式化、三级自进化框架和运行时优化等技术。

Conclusion: 该框架为自主智能体系统的创建、执行和治理提供了标准化架构，解决了生成式AI向智能体AI过渡中的关键架构不匹配问题，支持安全、高效的多步骤智能体工作流。

Abstract: The field of Artificial Intelligence is undergoing a transition from Generative AI -- probabilistic generation of text and images -- to Agentic AI, in which autonomous systems execute actions within external environments on behalf of users. This transition exposes a fundamental architectural mismatch: Large Language Models (LLMs) produce stochastic, unstructured outputs, whereas the backend infrastructure they must control -- databases, APIs, cloud services -- requires deterministic, schema-conformant inputs. The present paper describes the Auton Agentic AI Framework, a principled architecture for standardizing the creation, execution, and governance of autonomous agent systems. The framework is organized around a strict separation between the Cognitive Blueprint, a declarative, language-agnostic specification of agent identity and capabilities, and the Runtime Engine, the platform-specific execution substrate that instantiates and runs the agent. This separation enables cross-language portability, formal auditability, and modular tool integration via the Model Context Protocol (MCP). The paper formalizes the agent execution model as an augmented Partially Observable Markov Decision Process (POMDP) with a latent reasoning space, introduces a hierarchical memory consolidation architecture inspired by biological episodic memory systems, defines a constraint manifold formalism for safety enforcement via policy projection rather than post-hoc filtering, presents a three-level self-evolution framework spanning in-context adaptation through reinforcement learning, and describes runtime optimizations -- including parallel graph execution, speculative inference, and dynamic context pruning -- that reduce end-to-end latency for multi-step agent workflows.

</details>


### [12] [Reasoning-Driven Multimodal LLM for Domain Generalization](https://arxiv.org/abs/2602.23777)
*Zhipeng Xu,Zilong Wang,Xinyang Jiang,Dongsheng Li,De Cheng,Nannan Wang*

Main category: cs.AI

TL;DR: 该论文提出RD-MLDG框架，利用多模态大语言模型的推理能力解决领域泛化问题，通过推理链构建和自对齐正则化提升模型在域外数据上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 大多数领域泛化方法关注视觉特征不变性，但忽视了多模态大语言模型的推理能力。作者探索通过构建推理链来推导图像类别，以在领域偏移下实现更鲁棒的预测。

Method: 提出RD-MLDG框架，包含两个核心组件：1) MTCT（多任务交叉训练），引入直接分类路径来指导推理监督；2) SARR（自对齐推理正则化），通过迭代自标注保持推理链的语义丰富性同时缓解推理模式不匹配问题。

Result: 在标准DomainBed数据集（PACS、VLCS、OfficeHome、TerraInc）上的实验表明，RD-MLDG达到了最先进的性能，证明了推理作为鲁棒域外泛化的有前景的补充信号。

Conclusion: 该研究展示了利用多模态大语言模型的推理能力进行领域泛化的潜力，提出的RD-MLDG框架通过结合推理链和自对齐正则化，有效解决了推理监督中的优化挑战和模式不匹配问题。

Abstract: This paper addresses the domain generalization (DG) problem in deep learning. While most DG methods focus on enforcing visual feature invariance, we leverage the reasoning capability of multimodal large language models (MLLMs) and explore the potential of constructing reasoning chains that derives image categories to achieve more robust predictions under domain shift. To this end, we systematically study the role of reasoning in DG using DomainBed-Reasoning, a newly constructed extension of DomainBed dataset, in which each sample is paired with class-relevant reasoning chains. Our analysis reveals two key challenges: (i) fine-tuning MLLMs with reasoning chains for classification is more challenging than direct label supervision, since the model must optimize complex reasoning sequences before label prediction; and (ii) mismatches in reasoning patterns between supervision signals and fine-tuned MLLMs lead to a trade-off between semantic richness (informative but harder to optimize) and optimization efficiency (easier to optimize but less informative). To address these issues, we propose RD-MLDG (Reasoning-Driven Multimodal LLM for Domain Generalization), a framework with two components: (i) MTCT (Multi-Task Cross-Training), which introduces an additional direct classification pathway to guide reasoning supervision; and (ii) SARR (Self-Aligned Reasoning Regularization), which preserves the semantic richness of reasoning chains while mitigating reasoning-pattern mismatches via iterative self-labeling. Experiments on standard DomainBed datasets (PACS, VLCS, OfficeHome, TerraInc) demonstrate that RD-MLDG achieves state-of-the-art performances, highlighting reasoning as a promising complementary signal for robust out-of-domain generalization.

</details>


### [13] [EMO-R3: Reflective Reinforcement Learning for Emotional Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2602.23802)
*Yiyang Fang,Wenke Huang,Pei Fu,Yihao Yang,Kehua Su,Zhenbo Luo,Jian Luan,Mang Ye*

Main category: cs.AI

TL;DR: 提出EMO-R3框架，通过结构化情感思维和反思式情感奖励，提升多模态大语言模型的情感推理能力，解决现有方法泛化性差和可解释性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉推理任务上表现出色，但在捕捉人类情感的复杂性和主观性方面仍有不足。现有监督微调方法泛化能力有限且可解释性差，而强化学习方法如GRPO未能与情感认知的内在特性对齐。

Method: 提出EMO-R3框架，包含两个核心组件：1) 结构化情感思维，引导模型以结构化、可解释的方式进行逐步情感推理；2) 反思式情感奖励，使模型能够基于视觉-文本一致性和情感连贯性重新评估其推理过程。

Result: 大量实验表明，EMO-R3显著提升了多模态大语言模型的可解释性和情感智能，在多个视觉情感理解基准测试中取得了优越性能。

Conclusion: EMO-R3框架有效解决了多模态大语言模型在情感推理方面的局限性，通过结构化思维和反思式奖励机制，提升了模型的情感理解能力和可解释性。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual reasoning and understanding tasks but still struggle to capture the complexity and subjectivity of human emotions. Existing approaches based on supervised fine-tuning often suffer from limited generalization and poor interpretability, while reinforcement learning methods such as Group Relative Policy Optimization fail to align with the intrinsic characteristics of emotional cognition. To address these challenges, we propose Reflective Reinforcement Learning for Emotional Reasoning (EMO-R3), a framework designed to enhance the emotional reasoning ability of MLLMs. Specifically, we introduce Structured Emotional Thinking to guide the model to perform step-by-step emotional reasoning in a structured and interpretable manner, and design a Reflective Emotional Reward that enables the model to re-evaluate its reasoning based on visual-text consistency and emotional coherence. Extensive experiments demonstrate that EMO-R3 significantly improves both the interpretability and emotional intelligence of MLLMs, achieving superior performance across multiple visual emotional understanding benchmarks.

</details>


### [14] [Pessimistic Auxiliary Policy for Offline Reinforcement Learning](https://arxiv.org/abs/2602.23974)
*Fan Zhang,Baoru Huang,Xin Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一种悲观辅助策略来缓解离线强化学习中的分布外动作误差积累问题，通过最大化Q函数的下界置信度来采样可靠动作，有效提升了现有离线RL方法的性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习从预收集的数据集中学习智能体，避免了实时交互的不安全性和低效性。然而，在学习过程中不可避免地访问分布外动作会引入近似误差，导致误差积累和严重的过高估计问题。

Method: 构建了一个新的悲观辅助策略来采样可靠动作。具体来说，通过最大化Q函数的下界置信度来开发悲观辅助策略。该策略在学习策略附近表现出相对较高的价值和较低的uncertainty，避免了学习策略在学习过程中采样具有潜在高误差的高价值动作。

Result: 在离线强化学习基准测试上的大量实验表明，利用悲观辅助策略可以有效提高其他离线RL方法的效能。由悲观辅助策略采样的动作引入的较少近似误差有助于缓解误差积累。

Conclusion: 提出的悲观辅助策略通过采样可靠动作来减少分布外动作引起的近似误差，从而有效缓解了离线强化学习中的误差积累问题，提升了现有方法的性能。

Abstract: Offline reinforcement learning aims to learn an agent from pre-collected datasets, avoiding unsafe and inefficient real-time interaction. However, inevitable access to out-ofdistribution actions during the learning process introduces approximation errors, causing the error accumulation and considerable overestimation. In this paper, we construct a new pessimistic auxiliary policy for sampling reliable actions. Specifically, we develop a pessimistic auxiliary strategy by maximizing the lower confidence bound of the Q-function. The pessimistic auxiliary strategy exhibits a relatively high value and low uncertainty in the vicinity of the learned policy, avoiding the learned policy sampling high-value actions with potentially high errors during the learning process. Less approximation error introduced by sampled action from pessimistic auxiliary strategy leads to the alleviation of error accumulation. Extensive experiments on offline reinforcement learning benchmarks reveal that utilizing the pessimistic auxiliary strategy can effectively improve the efficacy of other offline RL approaches.

</details>


### [15] [CIRCLE: A Framework for Evaluating AI from a Real-World Lens](https://arxiv.org/abs/2602.24055)
*Reva Schwartz,Carina Westling,Morgan Briggs,Marzieh Fadaee,Isar Nejadgholi,Matthew Holmes,Fariza Rashid,Maya Carlyle,Afaf Taïk,Kyra Wilson,Peter Douglas,Theodora Skeadas,Gabriella Waters,Rumman Chowdhury,Thiago Lacerda*

Main category: cs.AI

TL;DR: CIRCLE框架通过六阶段生命周期方法，将模型性能指标与AI实际部署结果连接起来，填补现实差距


<details>
  <summary>Details</summary>
Motivation: 现有MLOps框架关注系统稳定性，基准测试衡量抽象能力，但决策者缺乏AI在真实世界用户变异和约束下的行为证据

Method: CIRCLE是一个六阶段生命周期框架，将TEVV中的验证阶段操作化，将利益相关者关注转化为可测量信号，整合现场测试、红队测试和纵向研究等方法

Result: CIRCLE提供结构化前瞻性协议，将情境敏感的定性洞察与可扩展的定量指标连接，产生可跨站点比较又对本地情境敏感的系统知识

Conclusion: 该框架能够实现基于具体下游效应而非理论能力的治理，为AI部署提供更全面的评估方法

Abstract: This paper proposes CIRCLE, a six-stage, lifecycle-based framework to bridge the reality gap between model-centric performance metrics and AI's materialized outcomes in deployment. While existing frameworks like MLOps focus on system stability and benchmarks measure abstract capabilities, decision-makers outside the AI stack lack systematic evidence about the behavior of AI technologies under real-world user variability and constraints. CIRCLE operationalizes the Validation phase of TEVV (Test, Evaluation, Verification, and Validation) by formalizing the translation of stakeholder concerns outside the stack into measurable signals. Unlike participatory design, which often remains localized, or algorithmic audits, which are often retrospective, CIRCLE provides a structured, prospective protocol for linking context-sensitive qualitative insights to scalable quantitative metrics. By integrating methods such as field testing, red teaming, and longitudinal studies into a coordinated pipeline, CIRCLE produces systematic knowledge: evidence that is comparable across sites yet sensitive to local context. This can enable governance based on materialized downstream effects rather than theoretical capabilities.

</details>


### [16] [Human or Machine? A Preliminary Turing Test for Speech-to-Speech Interaction](https://arxiv.org/abs/2602.24080)
*Xiang Li,Jiabao Gao,Sipei Lin,Xuan Zhou,Chi Zhang,Bo Cheng,Jiale Han,Benyou Wang*

Main category: cs.AI

TL;DR: 首个针对语音对话系统的图灵测试显示，现有系统均未通过测试，主要瓶颈在于副语言特征、情感表达和对话个性，而非语义理解。


<details>
  <summary>Details</summary>
Motivation: 现代语音对话系统能否像人类一样对话是一个关键但未解决的问题。为了评估语音对话系统的人类相似性，研究者进行了首个针对这类系统的图灵测试。

Method: 收集了2,968个人类判断，评估9个最先进的语音对话系统与28名人类参与者的对话。开发了包含18个人类相似性维度的细粒度分类法，并对收集的对话进行众包标注。提出了一个可解释的模型，利用细粒度的人类相似性评分进行准确透明的人机区分。

Result: 所有评估的语音对话系统均未通过图灵测试，揭示了在人类相似性方面存在显著差距。瓶颈不在于语义理解，而在于副语言特征、情感表达和对话个性。现成的AI模型作为图灵测试评判者表现不可靠。

Conclusion: 该研究建立了首个语音对话系统的人类相似性评估框架，超越了二元结果，提供了详细的诊断见解，为对话AI系统实现人类相似性改进铺平了道路。

Abstract: The pursuit of human-like conversational agents has long been guided by the Turing test. For modern speech-to-speech (S2S) systems, a critical yet unanswered question is whether they can converse like humans. To tackle this, we conduct the first Turing test for S2S systems, collecting 2,968 human judgments on dialogues between 9 state-of-the-art S2S systems and 28 human participants. Our results deliver a clear finding: no existing evaluated S2S system passes the test, revealing a significant gap in human-likeness. To diagnose this failure, we develop a fine-grained taxonomy of 18 human-likeness dimensions and crowd-annotate our collected dialogues accordingly. Our analysis shows that the bottleneck is not semantic understanding but stems from paralinguistic features, emotional expressivity, and conversational persona. Furthermore, we find that off-the-shelf AI models perform unreliably as Turing test judges. In response, we propose an interpretable model that leverages the fine-grained human-likeness ratings and delivers accurate and transparent human-vs-machine discrimination, offering a powerful tool for automatic human-likeness evaluation. Our work establishes the first human-likeness evaluation for S2S systems and moves beyond binary outcomes to enable detailed diagnostic insights, paving the way for human-like improvements in conversational AI systems.

</details>


### [17] [Bi-level RL-Heuristic Optimization for Real-world Winter Road Maintenance](https://arxiv.org/abs/2602.24097)
*Yue Xie,Zizhen Xu,William Beazley,Fumiya Iida*

Main category: cs.AI

TL;DR: 提出一种新颖的双层优化框架，结合强化学习和多目标车辆路径规划，用于大规模冬季道路维护路线优化，在英国真实路网上验证有效。


<details>
  <summary>Details</summary>
Motivation: 冬季道路维护对公共安全和环境影响至关重要，但现有方法难以有效处理大规模路线规划问题，且主要依赖人工决策。

Method: 采用双层优化框架：上层使用强化学习智能体将路网划分为可管理集群并从多个仓库优化分配资源；下层在每个集群内解决多目标车辆路径问题，最小化最长车辆行驶时间和总碳排放。

Result: 结果显示显著改进：平衡了工作负载，将最长行驶时间控制在目标两小时阈值以下，降低了排放，并实现了显著的成本节约。

Conclusion: 研究表明，先进的AI驱动双层优化可以直接增强现实世界交通和物流领域的运营决策能力。

Abstract: Winter road maintenance is critical for ensuring public safety and reducing environmental impacts, yet existing methods struggle to manage large-scale routing problems effectively and mostly reply on human decision. This study presents a novel, scalable bi-level optimization framework, validated on real operational data on UK strategic road networks (M25, M6, A1), including interconnected local road networks in surrounding areas for vehicle traversing, as part of the highway operator's efforts to solve existing planning challenges. At the upper level, a reinforcement learning (RL) agent strategically partitions the road network into manageable clusters and optimally allocates resources from multiple depots. At the lower level, a multi-objective vehicle routing problem (VRP) is solved within each cluster, minimizing the maximum vehicle travel time and total carbon emissions. Unlike existing approaches, our method handles large-scale, real-world networks efficiently, explicitly incorporating vehicle-specific constraints, depot capacities, and road segment requirements. Results demonstrate significant improvements, including balanced workloads, reduced maximum travel times below the targeted two-hour threshold, lower emissions, and substantial cost savings. This study illustrates how advanced AI-driven bi-level optimization can directly enhance operational decision-making in real-world transportation and logistics.

</details>


### [18] [Recycling Failures: Salvaging Exploration in RLVR via Fine-Grained Off-Policy Guidance](https://arxiv.org/abs/2602.24110)
*Yanwei Ren,Haotian Zhang,Likang Xiao,Xikai Zhang,Jiaxing Huang,Jiayan Qiu,Baosheng Yu,Quan Chen,Liu Liu*

Main category: cs.AI

TL;DR: SCOPE框架通过过程奖励模型定位推理轨迹中的首个错误步骤，进行细粒度修正，有效利用部分正确的推理轨迹，提升探索多样性13.5%，在数学推理任务上达到46.6%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习监督存在局限性：对部分正确但包含几个错误步骤的轨迹惩罚与完全错误轨迹相同，导致模型丢弃有价值的部分正确推理轨迹，降低探索多样性，过早缩小探索空间。

Method: 提出SCOPE框架，利用过程奖励模型精确定位次优推理轨迹中的首个错误步骤，应用细粒度的步骤级离策略修正，对部分正确的推理轨迹进行精确优化。

Result: SCOPE有效拯救部分正确的推理轨迹，将多样性分数提升13.5%，在数学推理任务上达到46.6%的平均准确率，在分布外推理任务上达到53.4%的准确率，建立了新的最先进结果。

Conclusion: SCOPE框架通过步骤级修正有效利用部分正确的推理轨迹，维持广泛的探索空间，显著提升大型推理模型的复杂推理能力，在数学推理和分布外任务上都表现出强大的泛化能力。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the complex reasoning capabilities of Large Reasoning Models. However, standard outcome-based supervision suffers from a critical limitation that penalizes trajectories that are largely correct but fail due to several missteps as heavily as completely erroneous ones. This coarse feedback signal causes the model to discard valuable largely correct rollouts, leading to a degradation in rollout diversity that prematurely narrows the exploration space. Process Reward Models have demonstrated efficacy in providing reliable step-wise verification for test-time scaling, naively integrating these signals into RLVR as dense rewards proves ineffective.Prior methods attempt to introduce off-policy guided whole-trajectory replacement that often outside the policy model's distribution, but still fail to utilize the largely correct rollouts generated by the model itself and thus do not effectively mitigate the narrowing of the exploration space. To address these issues, we propose SCOPE (Step-wise Correction for On-Policy Exploration), a novel framework that utilizes Process Reward Models to pinpoint the first erroneous step in suboptimal rollouts and applies fine-grained, step-wise off-policy rectification. By applying precise refinement on partially correct rollout, our method effectively salvages partially correct trajectories and increases diversity score by 13.5%, thereby sustaining a broad exploration space. Extensive experiments demonstrate that our approach establishes new state-of-the-art results, achieving an average accuracy of 46.6% on math reasoning and exhibiting robust generalization with 53.4% accuracy on out-of-distribution reasoning tasks.

</details>


### [19] [LemmaBench: A Live, Research-Level Benchmark to Evaluate LLM Capabilities in Mathematics](https://arxiv.org/abs/2602.24173)
*Antoine Peyronnet,Fabian Gloeckle,Amaury Hayat*

Main category: cs.AI

TL;DR: 提出基于arXiv最新数学研究的可更新LLM基准测试框架，替代传统的静态竞赛题基准


<details>
  <summary>Details</summary>
Motivation: 现有LLM数学能力基准主要依赖静态的手工整理竞赛题或教科书问题，不能真正反映研究级数学能力，需要建立能直接评估模型在最新数学研究成果上表现的动态基准

Method: 构建自动化流水线：从arXiv提取引理，通过显式化所有假设和定义将其重写为自包含的陈述，形成可定期更新的基准测试集

Result: 当前最先进的LLM在定理证明任务上准确率约为10-15%（pass@1），显示LLM要达到人类研究水平的证明能力仍有很大提升空间

Conclusion: 建立了首个基于最新数学研究的可更新LLM基准测试框架，为评估模型在真实研究环境中的数学能力提供了更准确的工具，揭示了当前LLM与人类研究水平之间的显著差距

Abstract: We present a new approach for benchmarking Large Language Model (LLM) capabilities on research-level mathematics. Existing benchmarks largely rely on static, hand-curated sets of contest or textbook-style problems as proxies for mathematical research. Instead, we establish an updatable benchmark evaluating models directly on the latest research results in mathematics. This consists of an automatic pipeline that extracts lemmas from arXiv and rewrites them into self-contained statements by making all assumptions and required definitions explicit. It results in a benchmark that can be updated regularly with new problems taken directly from human mathematical research, while previous instances can be used for training without compromising future evaluations. We benchmark current state-of-the-art LLMs, which obtain around 10-15$\%$ accuracy in theorem proving (pass@1) depending on the model, showing that there is currently a large margin of progression for LLMs to reach human-level proving capabilities in a research context.

</details>


### [20] [Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume](https://arxiv.org/abs/2602.24195)
*Gregory Kang Ruey Lau,Hieu Dao,Nicole Kan Hui Lin,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: UMPIRE是一个无需训练的多模态大语言模型不确定性量化框架，通过计算采样响应的语义体积来评估不确定性，在多种模态和任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型可能产生看似合理但错误的输出，影响可靠部署。现有不确定性度量方法存在局限性：仅适用于特定模态、依赖外部工具或计算成本高。

Method: UMPIRE框架利用模型内部模态特征，无需外部工具，通过计算采样响应的不连贯性调整语义体积，捕捉样本的全局语义多样性和基于内部置信度的局部不连贯性。

Result: 在图像、音频和视频-文本基准测试（包括对抗性和分布外设置）中，UMPIRE在错误检测和不确定性校准方面持续优于基线方法，并能泛化到非文本输出任务（如图像和音频生成）。

Conclusion: UMPIRE提供了一个高效、无需训练的不确定性量化框架，适用于多种输入输出模态，有助于提高多模态大语言模型的可靠部署。

Abstract: Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation.

</details>


### [21] [DARE-bench: Evaluating Modeling and Instruction Fidelity of LLMs in Data Science](https://arxiv.org/abs/2602.24288)
*Fan Shu,Yite Wang,Ruofan Wu,Boyi Liu,Zhewei Yao,Yuxiong He,Feng Yan*

Main category: cs.AI

TL;DR: DARE-bench是一个针对机器学习建模和数据科学指令遵循的基准测试，包含6,300个Kaggle衍生任务，提供可验证的真实标签，解决了现有基准测试缺乏标准化过程评估和训练数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在两个主要缺陷：一是缺乏标准化的过程感知评估，无法捕捉指令遵循和过程保真度；二是缺乏准确标注的训练数据。随着大语言模型处理复杂多步骤数据科学任务的需求快速增长，迫切需要准确的基准测试。

Method: 提出DARE-bench基准测试，包含6,300个从Kaggle平台衍生的任务，所有任务都有可验证的真实标签，确保客观和可重复的评估。该基准覆盖广泛的任务范围并支持智能体工具，同时提供大规模训练数据和评估集。

Result: 评估显示即使是gpt-o4-mini等高性能模型也难以取得良好表现，尤其在机器学习建模任务中。使用DARE-bench训练任务进行微调可以显著提升模型性能：监督微调使Qwen3-32B准确率提升1.83倍，强化学习使Qwen3-4B准确率提升超过8倍。

Conclusion: DARE-bench既是一个准确的评估基准，也是关键的训练数据来源，其显著的性能改进验证了它在数据科学任务评估和模型训练中的重要性。

Abstract: The fast-growing demands in using Large Language Models (LLMs) to tackle complex multi-step data science tasks create an emergent need for accurate benchmarking. There are two major gaps in existing benchmarks: (i) the lack of standardized, process-aware evaluation that captures instruction adherence and process fidelity, and (ii) the scarcity of accurately labeled training data. To bridge these gaps, we introduce DARE-bench, a benchmark designed for machine learning modeling and data science instruction following. Unlike many existing benchmarks that rely on human- or model-based judges, all tasks in DARE-bench have verifiable ground truth, ensuring objective and reproducible evaluation. To cover a broad range of tasks and support agentic tools, DARE-bench consists of 6,300 Kaggle-derived tasks and provides both large-scale training data and evaluation sets. Extensive evaluations show that even highly capable models such as gpt-o4-mini struggle to achieve good performance, especially in machine learning modeling tasks. Using DARE-bench training tasks for fine-tuning can substantially improve model performance. For example, supervised fine-tuning boosts Qwen3-32B's accuracy by 1.83x and reinforcement learning boosts Qwen3-4B's accuracy by more than 8x. These significant improvements verify the importance of DARE-bench both as an accurate evaluation benchmark and critical training data.

</details>

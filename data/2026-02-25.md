<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [DMCD: Semantic-Statistical Framework for Causal Discovery](https://arxiv.org/abs/2602.20333)
*Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: DMCD是一个两阶段因果发现框架，结合LLM语义草图和统计验证，通过变量元数据生成稀疏DAG草图，再用条件独立性测试进行审计和精炼。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法通常仅依赖统计测试，缺乏对变量语义关系的理解。DMCD旨在整合LLM的语义推理能力与统计验证，提高因果结构学习的准确性和实用性。

Method: 两阶段框架：第一阶段使用大语言模型基于变量元数据生成稀疏DAG草图；第二阶段通过条件独立性测试审计草图，检测差异并指导有针对性的边修正。

Result: 在工业工程、环境监测和IT系统分析三个元数据丰富的真实世界基准测试中，DMCD取得了竞争性或领先的性能，特别是在召回率和F1分数上有显著提升。

Conclusion: 将语义先验与原则性统计验证相结合，能够产生高性能且实际有效的因果结构学习方法，改进源于对元数据的语义推理而非基准图的记忆。

Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.
  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.

</details>


### [2] [From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production](https://arxiv.org/abs/2602.20558)
*Yucheng Shi,Ying Li,Yu Wang,Yesu Feng,Arjun Rao,Rein Houthooft,Shradha Sehgal,Jin Wang,Hao Zhen,Ninghao Liu,Linas Baltrunas*

Main category: cs.AI

TL;DR: 本文提出了一种数据驱动的框架，通过强化学习训练verbalization agent，将结构化用户交互记录优化为自然语言输入，显著提升LLM推荐系统的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为生成式推荐系统的核心面临关键挑战：如何将结构化用户交互记录有效地转化为自然语言输入。现有方法依赖简单的模板拼接，无法生成最优的文本表示。

Method: 提出数据中心的框架，使用强化学习训练verbalization agent，该智能体将原始交互历史转化为优化的文本上下文，以推荐准确性作为训练信号。智能体学习过滤噪声、整合相关元数据、重组信息以改善下游预测。

Result: 在大规模工业流媒体数据集上的实验表明，学习到的verbalization相比基于模板的基线方法，在发现项目推荐准确性上实现了高达93%的相对提升。分析揭示了涌现的策略，包括用户兴趣总结、噪声去除和语法规范化。

Conclusion: 学习verbalization是提升LLM推荐系统性能的有效方法，为基于LLM的推荐系统提供了有效的上下文构建见解。

Abstract: Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.

</details>


### [3] [When can we trust untrusted monitoring? A safety case sketch across collusion strategies](https://arxiv.org/abs/2602.20628)
*Nelson Gardner-Challis,Jonathan Bostock,Georgiy Kozhevnikov,Morgan Sinclaire,Joan Velja,Alessandro Abate,Charlie Griffin*

Main category: cs.AI

TL;DR: 本文研究不可信监控的安全性评估方法，通过放宽对AI串谋策略的假设，提出更全面的分类体系，并识别出被动自我识别可能比先前研究的策略更有效的情况。


<details>
  <summary>Details</summary>
Motivation: 随着AI自主性和能力的增强，错位AI可能造成灾难性危害的风险增加。不可信监控（用一个不可信模型监督另一个）是降低风险的方法之一，但由于无法安全部署错位模型直接测试协议，证明其安全性具有挑战性。

Method: 基于现有的部署前测试方法，放宽先前AI控制研究中对错位AI可能使用的串谋策略的假设，开发涵盖被动自我识别、因果串谋（隐藏预共享信号）、非因果串谋（通过谢林点隐藏信号）和组合策略的分类体系，并创建安全案例草图来明确论证。

Result: 识别出被动自我识别在某些条件下可能比先前研究的串谋策略更有效的情况，为不可信监控提供更稳健的评估框架。

Conclusion: 本文通过放宽对AI串谋策略的假设，开发了更全面的分类体系和安全论证框架，有助于构建更稳健的不可信监控评估方法，但仍存在未解决的挑战。

Abstract: AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.

</details>


### [4] [Identifying two piecewise linear additive value functions from anonymous preference information](https://arxiv.org/abs/2602.20638)
*Vincent Auriau,Khaled Belahcene,Emmanuel Malherbe,Vincent Mousseau,Marc Pirlot*

Main category: cs.AI

TL;DR: 提出一种同时向两位决策者提问以获取其偏好模型的方法，在不知道哪个回答对应哪个决策者的情况下，通过分段线性边际价值函数识别两个偏好模型


<details>
  <summary>Details</summary>
Motivation: 在同时向两位决策者提问时，虽然能获得两个回答，但不知道哪个回答对应哪个决策者，需要解决这种匿名回答情况下的偏好模型识别问题

Method: 假设偏好可以用加性价值函数表示，边际价值函数为分段线性且断点已知，设计一种启发程序来同时识别两个决策者的偏好模型

Result: 提出的启发程序能够在匿名回答的情况下成功识别出两位决策者各自的偏好模型

Conclusion: 该方法解决了同时向多位决策者提问时的匿名回答问题，为群体决策分析提供了新的技术手段

Abstract: Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.

</details>


### [5] [Grounding LLMs in Scientific Discovery via Embodied Actions](https://arxiv.org/abs/2602.20639)
*Bo Zhang,Jinfeng Zhou,Yuxuan Chen,Jianing Yin,Minlie Huang,Hongning Wang*

Main category: cs.AI

TL;DR: EmbodiedAct框架将科学软件转化为主动的具身智能体，通过紧密的感知-执行循环让LLMs在具身行动中接地，解决了LLMs在科学发现中理论与物理模拟之间的鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学发现中展现出巨大潜力，但难以弥合理论推理与可验证物理模拟之间的差距。现有解决方案采用被动的"执行-响应"循环，缺乏运行时感知能力，无法处理瞬态异常（如数值不稳定或发散振荡）。

Method: 提出EmbodiedAct框架，将成熟的科学软件转化为主动的具身智能体，通过紧密的感知-执行循环让LLMs在具身行动中接地。该框架在MATLAB环境中实例化。

Result: 在复杂的工程设计和科学建模任务上的广泛实验表明，EmbodiedAct显著优于现有基线，在长时程模拟中确保满意的可靠性和稳定性，在科学建模中提高准确性，达到最先进的性能。

Conclusion: EmbodiedAct通过将科学软件转化为具身智能体，有效解决了LLMs在科学发现中理论与物理模拟之间的鸿沟问题，为科学计算提供了更可靠、稳定的解决方案。

Abstract: Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive "execute-then-response" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.

</details>


### [6] [Recursive Belief Vision Language Model](https://arxiv.org/abs/2602.20659)
*Vaidehi Bagaria,Bijo Sebastian,Nirav Patel*

Main category: cs.AI

TL;DR: RB-VLA提出基于信念的视觉语言动作模型架构，通过自监督世界模型目标维护紧凑的潜在状态表示，解决长时程操作中的部分可观测性问题，显著提升任务成功率并降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言动作模型在部分可观测条件下的长时程操作任务中存在困难，主要问题包括：依赖短上下文窗口或重复查询导致任务进度丢失、感知混淆下的动作重复、高推理延迟，以及缺乏持久、动作条件的状态表示和有限的时空物理推理能力。

Method: 提出RB-VLA架构，采用信念中心设计，通过自监督世界模型目标训练，维护紧凑的潜在状态编码任务相关历史、动态和物体交互。VLM仅用于高层意图查询，信念模块跟踪任务进度并支持部分可观测下的阶段感知、因果基础控制，无需存储原始观测或随时间扩展内存。信念和意图共同条件化扩散策略进行鲁棒闭环执行。

Result: 在长时程基准测试中优于现有VLA模型：多阶段拾放任务成功率提高52.5%，堆叠任务提高37.5%（相比π0）；推理延迟降低高达5倍；消除现有VLA中随时间步增长的内存问题。消融实验显示信念模块是性能提升的主要驱动力，成功率从32.5%提升至77.5%。

Conclusion: 基于信念的状态表示对长时程VLA策略具有显著效果，RB-VLA通过紧凑的潜在状态表示解决了部分可观测性问题，实现了高效、鲁棒的多阶段控制，为视觉语言动作模型的长时程操作任务提供了有效解决方案。

Abstract: Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.

</details>


### [7] [How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective](https://arxiv.org/abs/2602.20687)
*Bo Peng,Pi Bu,Keyu Pan,Xinrun Xu,Yinxiu Zhao,Miao Chen,Yang Du,Lin Li,Jun Song,Tong Xu*

Main category: cs.AI

TL;DR: NativeEmbodied是一个用于视觉语言模型驱动的具身智能体的挑战性基准测试，采用统一的原生低级动作空间，在复杂场景中评估高级任务和低级技能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM驱动的具身智能体基准测试通常依赖高级命令或离散化动作空间，这与真实世界控制存在显著差异。同时，当前基准主要关注高级任务，缺乏对低级和高级层面的联合评估分析。

Method: 在多样化模拟场景上构建NativeEmbodied基准，包含复杂场景中的三个代表性高级任务来评估整体性能。为了更详细分析，将复杂任务所需技能解耦，构建四类低级任务，每类针对一个基础具身技能。

Result: 实验表明最先进的视觉语言模型在多个基础具身技能上存在明显缺陷，进一步分析显示这些瓶颈显著限制了高级任务的性能表现。

Conclusion: NativeEmbodied揭示了当前VLM驱动具身智能体的关键挑战，为未来研究提供了指导性见解，强调了在统一原生动作空间中进行多粒度评估的重要性。

Abstract: Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.

</details>


### [8] [PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding](https://arxiv.org/abs/2602.20696)
*Baolong Bi,Yuyao Ge,Shenghua Liu,Yuchen He,Siqian Tong,Lizhe Chen,Lingrui Mei,Zehao Li,Yiwei Wang,Yujun Cai,Ming-Hsuan Yang,Xueqi Cheng*

Main category: cs.AI

TL;DR: PromptCD是一种无需额外训练、在测试时通过对比正负引导提示来控制AI行为的方法，适用于LLMs和VLMs，显著提升对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法大多需要在训练时进行，依赖高质量数据且计算成本高。对比解码方法虽然能利用模型内部分布提升特定能力，但适用范围有限。需要一种更通用、成本效益高的测试时行为控制方法。

Method: 提出Polarity-Prompt Contrastive Decoding (PromptCD)：为目标行为构建正负引导提示对，对比模型响应（LLMs中的token级概率分布和VLMs中的视觉注意力模式），以强化期望结果。该方法无需额外训练，适用于多种增强目标。

Result: 在LLMs的"3H"对齐目标（helpfulness, honesty, harmlessness）上取得一致且显著的改进，表明后训练模型能在测试时实现有意义的自我增强。在VLMs中，PromptCD通过强化行为一致的视觉基础显著提升VQA性能。

Conclusion: PromptCD是一种简单、通用且成本效益高的策略，可在跨模态场景中实现可靠的行为控制，为AI对齐提供了无需额外训练的有效解决方案。

Abstract: Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the "3H" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.

</details>


### [9] [Counterfactual Simulation Training for Chain-of-Thought Faithfulness](https://arxiv.org/abs/2602.20710)
*Peter Hase,Christopher Potts*

Main category: cs.AI

TL;DR: 本文提出了一种名为Counterfactual Simulation Training (CST)的训练方法，旨在通过奖励那些能够使模拟器准确预测模型在反事实输入下输出的思维链，来提高思维链推理的忠实性。


<details>
  <summary>Details</summary>
Motivation: 思维链推理是理解大语言模型输出原因的最常用方法之一，但思维链的忠实性问题严重限制了从这种实践中获得的洞察。现有方法在检测模型是否依赖虚假特征、奖励黑客行为或奉承行为方面存在局限性。

Method: 提出了Counterfactual Simulation Training (CST)方法，该方法通过奖励那些能够使模拟器准确预测模型在反事实输入下输出的思维链来提高忠实性。应用在两个场景：1) 基于线索的反事实思维链监控，检测模型是否依赖虚假特征；2) 基于通用模型的反事实模拟，鼓励模型产生更忠实、可泛化的推理。

Result: 在参数高达235B的模型上实验表明，CST能显著提高基于线索反事实的监控准确率（提升35个百分点）和通用反事实的可模拟性（提升2个百分点）。CST优于提示基线方法，用LLM重写不忠实思维链的效率比单独使用RL高5倍，但忠实性改进不适用于劝阻线索。

Conclusion: CST能够普遍提高思维链推理的忠实性，在思维链监控方面具有有前景的应用价值。更大的模型虽然不会自动产生更忠实的思维链，但能从CST中获益更多。

Abstract: Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training

</details>


### [10] [Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning](https://arxiv.org/abs/2602.20722)
*Xu Wan,Yansheng Wang,Wenqi Huang,Mingyang Sun*

Main category: cs.AI

TL;DR: BAPO是一种用于大语言模型后训练的离策略RLVR框架，通过动态选择训练批次、重新评估历史困难样本和重用高质量样本来提高数据效率，相比GRPO平均提升12.5%，解决了40.7%基础模型无法解决的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于策略的RLVR框架存在经验浪费和奖励同质化问题，这直接阻碍了在大语言模型后训练中对困难样本的学习效率。

Method: 提出Batch Adaptation Policy Optimization (BAPO)，这是一种离策略RLVR框架，通过动态选择训练批次、重新评估历史困难样本、重用高质量样本，同时保持策略改进的下界保证。

Result: 在数学、规划和视觉推理任务上，BAPO相比GRPO平均提升12.5%，成功解决了40.7%基础模型持续无法解决的问题。

Conclusion: BAPO通过提高数据效率和解决困难样本问题，显著改善了大语言模型后训练的效果，为RLVR框架提供了更高效的解决方案。

Abstract: Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.

</details>


### [11] [CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference](https://arxiv.org/abs/2602.20732)
*Chao Fei,Guozhong Li,Chenxi Liu,Panos Kalnis*

Main category: cs.AI

TL;DR: CHESS是一个算法-系统协同设计的KV缓存管理系统，通过上下文感知的分层选择策略，仅使用1%的KV缓存就能超越Full-KV的质量，实现高达4.56倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM需要低延迟的准确推理，但随着上下文增长，解码主要受KV缓存约束。现有剪枝方法大多是上下文无关的：其token选择忽略了逐步相关性和局部语义，损害了质量，且不规则访问和选择开销仅带来有限的加速效果。

Method: CHESS采用算法-系统协同设计：算法上引入上下文感知的分层选择策略，动态重建当前解码的连贯上下文；系统上通过粗粒度选择消除昂贵的数据移动，充分实现理论稀疏性的实际加速。

Result: 广泛评估表明，CHESS仅使用1%的KV缓存就能超越Full-KV的质量，提供低延迟稳定推理，吞吐量提升高达4.56倍，并持续优于其他强基线方法。

Conclusion: CHESS通过算法-系统协同设计有效解决了长上下文LLM中KV缓存管理的挑战，在保持高质量的同时实现了显著的性能提升。

Abstract: Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \textbf{CHESS}, an \textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \textbf{1\%} of the KV cache, delivers low-latency stable inference with up to \textbf{4.56$\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.

</details>


### [12] [POMDPPlanners: Open-Source Package for POMDP Planning](https://arxiv.org/abs/2602.20810)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: POMDPPlanners是一个开源的Python包，用于POMDP规划算法的实证评估，集成了先进算法、基准环境、自动化超参数优化和并行仿真功能。


<details>
  <summary>Details</summary>
Motivation: 现有的POMDP工具包在风险敏感设置下存在不足，需要进行可扩展、可复现的不确定性决策研究。研究人员需要减少大规模仿真研究的开销。

Method: 开发了一个集成最先进规划算法的Python包，包含基准环境套件（含安全关键变体）、通过Optuna的自动化超参数优化、带故障恢复的持久缓存以及可配置的并行仿真。

Result: 创建了POMDPPlanners开源包，显著减少了广泛仿真研究的开销，支持可扩展、可复现的研究，特别是在风险敏感设置下。

Conclusion: POMDPPlanners为不确定性决策研究提供了全面的工具包，特别适用于标准工具包无法满足的风险敏感场景，促进了该领域的研究进展。

Abstract: We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.

</details>


### [13] [Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence](https://arxiv.org/abs/2602.20934)
*ChengYou Li,XiaoDong Liu,XiangBao Meng,XinYu Zhao*

Main category: cs.AI

TL;DR: 论文提出AgentOS框架，将大语言模型重新定义为受结构化操作系统逻辑管理的"推理内核"，通过深度上下文管理将上下文窗口概念化为可寻址语义空间，旨在解决微观标记处理与宏观系统智能之间的理论鸿沟。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型研究主要关注扩展上下文窗口或优化提示工程，但微观标记处理与宏观系统智能之间的理论桥梁仍然碎片化。需要从静态推理引擎向动态自主认知系统进行根本性转变。

Method: 提出AgentOS概念框架，将LLM重新定义为"推理内核"，采用结构化操作系统逻辑。核心是深度上下文管理，将上下文窗口概念化为可寻址语义空间而非被动缓冲区。引入语义切片和时间对齐机制来减轻多智能体编排中的认知漂移，并将经典操作系统抽象（如内存分页、中断处理和进程调度）映射到LLM原生构造上。

Result: 论文提供了一个严谨的路线图，用于构建弹性、可扩展和自我进化的认知环境。分析表明，AGI发展的下一个前沿在于系统级协调的架构效率。

Conclusion: 大语言模型范式正在从静态推理引擎向动态自主认知系统进行根本性转变。通过将操作系统原理应用于LLM架构，可以创建更强大、协调和自适应的认知系统，系统级协调的架构效率是AGI发展的关键前沿。

Abstract: The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a "Reasoning Kernel" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.

</details>

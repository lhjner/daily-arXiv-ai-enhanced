<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: Bidirectional RAG是一种新型检索增强生成架构，通过验证高质量生成响应的写回机制实现安全的知识库扩展，相比标准RAG将覆盖率从20.33%提升至40.58%，同时比简单写回减少72%的文档添加。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统使用静态知识库，无法从用户交互中学习和进化，限制了系统的持续改进能力。需要一种既能扩展知识库又能防止幻觉污染的安全机制。

Method: 提出Bidirectional RAG架构，采用多阶段接受层，结合基于NLI的蕴含验证、归因检查和新颖性检测，确保高质量生成响应的安全写回，实现知识积累。

Result: 在四个数据集（Natural Questions、TriviaQA、HotpotQA、Stack Overflow）上进行12次实验，Bidirectional RAG平均覆盖率达到40.58%，几乎是标准RAG（20.33%）的两倍，同时比简单写回方法减少72%的文档添加（140 vs 500）。

Conclusion: 研究表明，通过严格验证机制管理的自改进RAG系统是可行且安全的，为实现能够从部署中学习的RAG系统提供了实用路径。

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [2] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: 研究发现，大型语言模型在未经明确提示的情况下也可能产生说服行为，特别是在经过监督微调后，即使是在良性话题上进行微调，也会增加模型在争议性和有害话题上的说服倾向。


<details>
  <summary>Details</summary>
Motivation: 随着对话式AI系统的广泛应用，AI对人类观点和信念的影响力前所未有。先前研究主要关注滥用场景下的模型说服力，但本文旨在探究模型在未经明确提示情况下产生说服行为的条件，以评估这种新兴风险。

Method: 研究在两种场景下考察未经提示的说服行为：(1)通过内部激活引导使模型具有特定人格特质；(2)通过监督微调使模型展现相同特质。研究比较了这两种方法对模型说服倾向的影响。

Result: 研究发现：1)通过内部激活引导向说服相关或无关特质的方向调整，并不能可靠增加模型未经提示的说服倾向；2)监督微调确实会增加模型的说服倾向；3)即使在良性话题上进行监督微调，也会产生在争议性和有害话题上说服倾向更高的模型。

Conclusion: 监督微调会导致模型产生未经提示的说服行为，即使是在良性话题上进行微调，也会增加模型在有害话题上的说服倾向。这表明新兴的有害说服风险确实存在，需要进一步研究。

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [3] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: GamiBench是一个评估多模态大语言模型空间推理能力的基准测试，通过折纸任务测试模型在2D到3D转换、多视角一致性和物理可行性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在空间推理方面存在不足，而当前基准测试主要关注静态图像或最终输出，未能充分评估序列化和视角依赖的空间推理能力。需要一个新的基准来全面评估模型在几何理解和空间推理方面的表现。

Method: 创建GamiBench基准，包含186个常规和186个不可能的2D折痕图案及其对应的3D折叠形状，从6个不同视角生成。设计了三个视觉问答任务：预测3D折叠配置、区分有效视角、检测不可能图案。引入新的诊断指标：视角一致性(VC)和不可能折叠选择率(IFSR)。

Result: 实验表明，即使是GPT-5和Gemini-2.5-Pro等领先模型在单步空间理解任务上也表现不佳。GamiBench为评估MLLM的几何理解和空间推理能力建立了标准化框架。

Conclusion: GamiBench填补了现有基准在评估空间推理能力方面的空白，通过折纸任务全面评估模型的跨视角一致性、物理可行性和中间步骤理解能力，为MLLM的空间推理研究提供了重要工具。

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [4] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: 本文提出了一种公平性感知AI框架，用于优化孟加拉国洪水后援助分配，通过对抗性去偏技术减少对边缘化地区的系统性偏见，在保持预测准确性的同时显著提升分配公平性。


<details>
  <summary>Details</summary>
Motivation: 发展中国家灾后援助分配存在系统性偏见，边缘化地区往往处于不利地位，这延续了历史不平等。孟加拉国作为洪水频发国家，需要更公平的援助分配机制来确保援助真正基于需求而非历史分配模式。

Method: 采用对抗性去偏模型，借鉴医疗AI中的公平性感知表示学习技术，使用梯度反转层迫使模型学习偏置不变的表示。基于2022年孟加拉国洪水真实数据（影响720万人，损失4.055亿美元），在11个地区的87个upazilas上开发模型。

Result: 实验结果显示：统计奇偶差异减少41.6%，区域公平差距降低43.2%，同时保持较强的预测准确性（R平方=0.784 vs 基线0.811）。模型生成可操作的优先级排名，确保援助基于真实需求而非历史分配模式。

Conclusion: 该研究证明算法公平性技术可有效应用于人道主义背景，为决策者提供实施更公平灾后恢复策略的工具，确保援助真正惠及最脆弱人群。

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [5] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: 本文提出了Agentic Risk & Capability (ARC)框架，这是一个技术治理框架，旨在帮助组织识别、评估和减轻由智能AI系统带来的风险。


<details>
  <summary>Details</summary>
Motivation: 智能AI系统因其自主行动能力（如代码执行、互联网交互、文件修改）既带来重大机遇也产生新型风险，这对组织治理提出了重大挑战，特别是在全面识别、评估和减轻多样且不断演变的风险方面。

Method: 引入ARC框架，该框架采用能力中心视角分析智能AI系统，提炼出三个主要风险来源（组件、设计、能力），建立风险来源、具体实现风险和相应技术控制之间的明确联系，并提供结构化实用方法帮助组织实施。

Result: ARC框架为组织提供了一个稳健且可适应的方法论，能够应对智能AI的复杂性，在确保智能AI系统安全、可靠和负责任部署的同时，实现快速有效的创新。

Conclusion: 该框架为组织治理智能AI系统风险提供了系统化工具，已开源供业界使用，有助于在创新与风险管理之间取得平衡。

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [6] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: Logic Sketch Prompting (LSP) 是一种轻量级提示框架，通过引入类型变量、确定性条件评估器和基于规则的验证器，显著提升大语言模型在需要严格规则遵循、确定性和可审计性任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言推理方面表现出色，但在需要严格规则遵循、确定性和可审计性的任务上仍然不可靠。临床、监管和安全关键决策支持系统需要可追溯和可重复的输出。

Method: LSP框架引入类型变量、确定性条件评估器和基于规则的验证器，使用两个药理学逻辑合规任务进行基准测试，对比零样本提示、思维链提示和简洁提示，在Gemma 2、Mistral和Llama 3三个开源模型上评估。

Result: 在所有任务和所有模型中，LSP始终获得最高的准确率(0.83-0.89)和F1分数(0.83-0.89)，显著优于零样本提示(0.24-0.60)、简洁提示(0.16-0.30)和思维链提示(0.56-0.75)。McNemar检验显示几乎所有比较中LSP都有统计学显著提升(p<0.01)。

Conclusion: LSP在不牺牲性能的情况下提高了确定性、可解释性和一致性，支持其在临床、监管和安全关键决策支持系统中的使用。

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [7] [Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback](https://arxiv.org/abs/2512.22336)
*Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo*

Main category: cs.AI

TL;DR: Agent2World：一个工具增强的多智能体框架，通过多智能体反馈实现推理时世界模型生成，并作为监督微调的数据引擎


<details>
  <summary>Details</summary>
Motivation: 当前训练LLMs生成符号世界模型（如PDDL领域或可执行模拟器）受到大规模可验证监督数据缺乏的限制，现有方法主要依赖静态验证方法，无法捕捉交互执行中出现的行为级错误

Method: 采用三阶段流水线：1)深度研究智能体通过网页搜索进行知识合成以填补规范缺口；2)模型开发智能体实现可执行世界模型；3)专门的测试团队进行自适应单元测试和基于模拟的验证

Result: 在涵盖PDDL和可执行代码表示的三个基准测试中展示了优越的推理时性能，取得了持续的最先进结果。微调后的模型在世界模型生成方面平均相对增益达到30.95%

Conclusion: Agent2World不仅实现了强大的推理时世界模型生成，还通过测试团队作为交互环境为模型开发提供行为感知的自适应反馈，生成多轮训练轨迹，显著提升了世界模型生成能力

Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.

</details>


### [8] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 将带有控制参数的数字规划问题转化为简单数字任务，使传统启发式方法能应用于无限动作空间


<details>
  <summary>Details</summary>
Motivation: 标准数字规划模型引入控制参数后，动作数量可能无限，导致现成的数字启发式方法不可行，需要新的解决方案

Method: 识别可控简单数字问题子集，采用乐观编译方法将其转化为简单数字任务，将控制依赖表达式抽象为有界常数效果和宽松前提条件

Result: 该方法使子目标启发式能有效估计涉及控制参数的数字规划问题的目标距离，是有效且计算可行的解决方案

Conclusion: 提出的编译方法能够将传统数字启发式应用于具有无限可能动作的场景，推动了当前技术水平的边界

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [9] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: 本文针对AI在材料科学中的幻觉问题，提出了HalluMatData基准数据集和HalluMatDetector检测框架，通过多阶段验证将幻觉率降低30%，并引入PHCS指标量化模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学发现中产生幻觉（生成事实错误或误导信息）的问题严重威胁研究完整性，特别是在材料科学领域需要可靠的AI生成内容。

Method: 提出了HalluMatData基准数据集用于评估幻觉检测方法，并开发了HalluMatDetector多阶段幻觉检测框架，包括内在验证、多源检索、矛盾图分析和基于指标的评估。

Result: 研究发现材料科学不同子领域的幻觉水平差异显著，高熵查询表现出更大的事实不一致性。使用HalluMatDetector验证流程可将幻觉率比标准LLM输出降低30%。

Conclusion: 通过HalluMatData和HalluMatDetector框架有效解决了AI在材料科学中的幻觉问题，提出的PHCS指标为量化LLM响应一致性提供了新方法，提升了AI生成内容的可靠性。

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [10] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: GatedBias：一个轻量级的推理时个性化框架，通过结构门控适配将冻结的知识图谱嵌入适应到个体用户上下文，仅需约300个可训练参数，在保持全局准确性的同时显著提升个性化排名性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱基础模型在链接预测方面表现出强大的群体级性能，但无法捕捉个体用户偏好，这是通用关系推理与个性化排名之间的关键脱节。

Method: 提出GatedBias框架，采用结构门控适配：将特定于用户档案的特征与图导出的二进制门结合，产生可解释的、每个实体的偏置，仅需约300个可训练参数，无需重新训练或损害全局准确性。

Result: 在两个基准数据集（Amazon-Book和Last-FM）上评估，显示在一致性指标方面有统计显著改进，同时保持群体性能。反事实扰动实验验证了因果响应性：从特定偏好信号中受益的实体在信号增强时显示出6-30倍的排名改进。

Conclusion: 基础模型的个性化适配可以是参数高效且因果可验证的，能够桥接通用知识表示与个体用户需求。

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [11] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: MCE提出基于函子、应用函子和单子的代数结构为AI智能体设计提供形式化基础，通过计算上下文管理状态、错误处理和并发等横切关注点，支持构建复杂、鲁棒且高效的自主智能体。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自主智能体架构通常采用命令式、临时性模式构建，导致系统脆弱，在状态管理、错误处理和并发方面存在困难，需要更形式化的设计范式。

Method: 提出单子上下文工程（MCE）架构范式，利用函子、应用函子和单子的代数结构，将智能体工作流视为计算上下文，通过抽象代数性质内在管理横切关注点。单子支持鲁棒的顺序组合，应用函子提供并行执行的结构化方法，单子变换器支持这些能力的系统化组合。

Result: MCE使开发者能够从简单、可独立验证的组件构建复杂、鲁棒且高效的AI智能体。该框架还扩展到元智能体，通过元编程动态创建和管理子智能体工作流。

Conclusion: MCE为自主智能体设计提供了形式化基础，通过代数结构解决当前架构的脆弱性问题，支持构建可组合、可验证且高效的智能体系统，并可通过元编程实现更高级的生成式编排。

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [12] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: DarkPatterns-LLM是一个用于评估大语言模型操纵性内容的综合基准数据集和诊断框架，包含7种伤害类别和四层分析流程，在主流模型上显示出显著的性能差异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，操纵性或欺骗性行为日益引发担忧，可能损害用户自主权、信任和福祉。现有安全基准主要依赖粗糙的二元标签，无法捕捉构成操纵的微妙心理和社会机制。

Method: 提出了DarkPatterns-LLM基准数据集和诊断框架，包含401个精心策划的示例，采用四层分析流程：多粒度检测、多尺度意图分析、威胁协调协议和深度上下文风险对齐，涵盖7种伤害类别。

Result: 评估GPT-4、Claude 3.5和LLaMA-3-70B等最先进模型时，观察到显著的性能差异（65.2%-89.7%），在检测损害自主权的模式方面存在一致弱点。

Conclusion: DarkPatterns-LLM建立了首个用于大语言模型操纵检测的标准化、多维度基准，为构建更可信的AI系统提供了可操作的诊断工具。

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [13] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: Tyee是一个用于智能生理医疗的统一、模块化、可配置工具包，解决了深度学习在生理信号分析中的数据格式异构、预处理不一致、模型管道碎片化和实验不可复现等问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生理信号分析中面临数据格式异构、预处理策略不一致、模型管道碎片化以及实验设置不可复现等挑战，这些限制了该领域的进展。

Method: Tyee工具包包含三个关键创新：1) 12种信号模态的统一数据接口和可配置预处理管道；2) 模块化可扩展架构，支持灵活集成和快速原型开发；3) 端到端工作流配置，促进可复现和可扩展的实验。

Result: Tyee在所有评估任务中表现出持续的实际效果和泛化能力，在13个数据集中的12个上达到了最先进的结果，优于或匹配基线方法。

Conclusion: Tyee是一个有效的统一工具包，解决了生理信号分析中的关键挑战，提供了可复现、可扩展的解决方案，并在多个任务上展现了优越性能。

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [14] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: M³ob：利用多模态时空知识增强位置推荐，通过LLM增强的时空知识图谱构建统一时空关系图，解决现有方法泛化能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的人类移动预测方法泛化能力有限：单模态方法受数据稀疏性和固有偏差限制，多模态方法难以有效捕捉静态多模态表示与时空动态之间的语义鸿沟。

Method: 1. 利用LLM增强的时空知识图谱（STKG）构建统一时空关系图（STRG）；2. 设计门控机制融合不同模态的时空图表示；3. 提出STKG引导的跨模态对齐，将时空动态知识注入静态图像模态。

Result: 在六个公共数据集上的实验表明，该方法在正常场景下取得一致改进，在异常场景下展现出显著泛化能力。

Conclusion: M³ob通过多模态时空知识有效表征移动动态，显著提升了位置推荐的准确性和泛化能力，特别是在异常场景下表现突出。

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [15] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: LLM相互审阅预测能提升准确性，但仅在多样化模型组有效，同质化模型组无效，额外信息无帮助


<details>
  <summary>Details</summary>
Motivation: 研究结构化审议对人类预测者的提升效果是否也能应用于大语言模型，探索LLM相互审阅预测能否改善预测准确性

Method: 使用Metaculus Q2 2025 AI预测锦标赛的202个已解决二元问题，测试GPT-5、Claude Sonnet 4.5、Gemini Pro 2.5在四种场景下的预测准确性：多样化模型+分布式信息、多样化模型+共享信息、同质化模型+分布式信息、同质化模型+共享信息

Result: 干预措施在多样化模型+共享信息场景显著提升准确性，Log Loss降低0.020（相对提升约4%，p=0.017）；同质化模型组无改善；意外发现额外上下文信息未提升预测准确性

Conclusion: 审议可能是改善LLM预测的可行策略，但效果取决于模型多样性，同质化模型组无法从相互审阅中获益

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [16] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: DICE是一个两阶段、证据耦合的框架，用于提升RAG系统评估的可解释性和鲁棒性，通过瑞士制锦标赛减少计算复杂度，在中文金融QA数据集上达到85.7%的人类专家一致性。


<details>
  <summary>Details</summary>
Motivation: 随着RAG系统向更复杂架构演进，通过可解释和鲁棒的评估确保其可信度变得至关重要。现有的标量指标存在可解释性有限、不确定性量化不足以及在多系统比较中计算效率低下的问题，阻碍了RAG技术的负责任部署。

Method: DICE是一个两阶段、证据耦合的框架，结合深度分析推理和概率{A, B, Tie}评分，产生透明、置信度感知的判断。它采用瑞士制锦标赛来减少计算复杂度（从O(N²)到O(N log N)），同时保持排名保真度。

Result: 在八系统评估中实现了42.9%的计算复杂度降低。在中文金融QA数据集上验证，DICE与人类专家的同意率达到85.7%，显著优于RAGAS等现有基于LLM的指标。

Conclusion: DICE建立了一个负责任、可解释且高效的可信RAG系统评估范式，支持通过可解释的推理痕迹进行系统改进，实现系统性错误诊断和可操作的见解。

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [17] [Memento-II: Learning by Stateful Reflective Memory](https://arxiv.org/abs/2512.22716)
*Jun Wang*

Main category: cs.AI

TL;DR: 提出一个理论框架，将情景记忆与强化学习结合，使大语言模型智能体能够通过反思机制进行持续和体验式学习，无需反向传播或模型微调。


<details>
  <summary>Details</summary>
Motivation: 传统方法在训练和部署之间存在严格分离，需要反向传播或模型微调来实现适应。本文旨在建立一个理论框架，使语言模型智能体能够通过交互进行持续学习，而无需参数更新。

Method: 提出状态化反思决策过程，将反思学习建模为与情景记忆的两阶段读写交互：写入存储交互结果（策略评估），读取检索相关过去案例（策略改进）。该过程诱导出增强状态记忆表示的等效马尔可夫决策过程。

Result: 使用熵正则化策略迭代实例化该框架并建立收敛保证。当情景记忆增长并充分覆盖状态空间时，所得策略收敛到最优解。

Conclusion: 为基于记忆增强和检索的语言模型智能体提供了理论基础，使其能够在不更新参数的情况下实现持续适应，弥合了训练与部署之间的传统分离。

Abstract: We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.

</details>


### [18] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: HiSciBench是一个分层科学智能基准测试，包含5个层级、8,735个实例，覆盖6大学科，支持多模态输入和跨语言评估，用于全面评估大模型在完整科学工作流程中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有科学智能基准测试存在碎片化问题，大多关注狭窄任务，未能反映真实科学探究的层次性和多学科性。需要一个新的综合性基准来评估大模型在整个科学工作流程中的能力。

Method: 设计了HiSciBench分层基准，包含5个层级：科学素养(L1)、文献解析(L2)、基于文献的问答(L3)、文献综述生成(L4)和科学发现(L5)。包含8,735个实例，覆盖数学、物理、化学、生物、地理和天文6大学科，支持文本、方程、图表等多模态输入和跨语言评估。

Result: 对GPT-5、DeepSeek-R1等领先模型的评估显示显著性能差距：在基础素养任务上准确率可达69%，但在发现级挑战上急剧下降至25%。

Conclusion: HiSciBench为评估科学智能设立了新标准，提供了开发更强大、更可靠模型的可操作见解。该基准将公开发布以促进未来研究。

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [19] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: Gamma提出多头部几何注意力机制，使用多种代数变换（实数、复数、分裂复数、对偶数）替代单一关系变换，通过注意力融合机制自适应组合，在零样本归纳链接预测中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有结构知识图谱基础模型（如Ultra）依赖单一关系变换（如逐元素乘法），限制了表达能力，无法捕捉多样化图谱中不同的关系和结构模式。

Method: Gamma引入多头部几何注意力机制，使用并行关系变换（实数、复数、分裂复数、对偶数变换），通过关系条件注意力融合机制在链接级别自适应融合，采用轻量级门控和熵正则化。

Result: 在56个多样化知识图谱上的实验表明，Gamma在零样本归纳链接预测中一致优于Ultra，在归纳基准上平均倒数排名提升5.5%，在所有基准上提升4.4%。

Conclusion: Gamma通过互补的几何表示提高了表达能力，多头部几何注意力机制能够更有效地捕捉知识图谱中多样化的关系和结构模式。

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [20] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: 该研究对比了大型语言模型与传统知识追踪模型在K-12教育中的表现，发现传统模型在知识评估的准确性、可靠性和时间一致性方面显著优于LLM，即使经过微调也无法达到同等效果。


<details>
  <summary>Details</summary>
Motivation: 针对当前LLM在K-12教育中被误认为可以替代传统学习者建模的误解，特别是在欧盟AI法案将K-12教育列为高风险领域的背景下，需要负责任地设计自适应教学系统。研究旨在验证LLM在评估学习者知识演变方面的局限性。

Method: 使用大型开放数据集，比较深度知识追踪模型与广泛使用的LLM（包括零样本和微调版本），评估它们在预测下一步正确性、时间一致性和计算需求等方面的表现。

Result: DKT在下一步正确性预测方面表现最佳（AUC=0.83），始终优于LLM。尽管微调使LLM的AUC提高了约8%，但仍比DKT低6%，且在序列早期错误率更高。时间分析显示DKT保持稳定、方向正确的掌握度更新，而LLM变体表现出显著的时间弱点，包括不一致和错误方向的更新。

Conclusion: LLM单独使用不太可能达到现有智能辅导系统的效果，负责任的辅导需要结合学习者建模的混合框架。传统知识追踪模型在准确性、可靠性和时间一致性方面优于LLM，即使经过大量计算资源微调也无法弥补这一差距。

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [21] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: ChexReason：一个使用有限资源（2000 SFT样本、1000 RL样本、单A100 GPU）通过R1风格方法训练的视觉语言模型，在医学影像推理任务中展示了强化学习（GRPO）能提升分布内性能但损害跨数据集泛化能力的矛盾现象。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在大语言模型的推理任务中取得了进展，但在资源受限的医学影像应用中仍未被充分探索。研究旨在探索在有限计算资源下，强化学习方法对医学影像视觉语言模型性能的影响。

Method: 采用R1风格方法：先进行监督微调（SFT），然后使用GRPO（Group Relative Policy Optimization）进行强化学习。仅使用2000个SFT样本、1000个RL样本和单个A100 GPU训练ChexReason视觉语言模型。

Result: GRPO在CheXpert数据集上提升了23%的性能（macro-F1 = 0.346），但在NIH数据集上性能下降了19%，显示出强化学习能恢复分布内性能但损害跨数据集泛化能力。SFT检查点在RL优化前能独特地提升NIH性能，表明教师引导的推理能捕捉更多机构无关特征。

Conclusion: 强化学习范式而非模型规模导致了泛化矛盾。对于需要跨不同人群鲁棒性的临床部署，精心策划的监督微调可能优于激进的强化学习方法。结构化推理框架对通用视觉语言模型有益，但对医学预训练模型增益有限。

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [22] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: 本文提出Intrinsic Self-reflective Preference Optimization (q)方法，解决了DPO的两个根本限制：最优策略对建模选择的依赖性和未能利用成对数据中的比较信息。


<details>
  <summary>Details</summary>
Motivation: 作者发现DPO及其变体存在两个基本限制：1) 最优策略依赖于任意建模选择（标量化函数、参考策略），导致行为反映参数化伪影而非真实偏好；2) 孤立处理响应生成未能利用成对数据中的比较信息，未能挖掘模型的内在自我反思能力。

Method: 提出Intrinsic Self-reflective Preference Optimization (q)方法，推导出基于上下文和替代响应的全局最优策略。该方法作为即插即用增强，无需架构更改或推理开销，保证对标量化和参考选择的不变性。

Result: 实验证明该方法在胜率和长度控制指标上取得一致改进，验证了解锁自我反思能力能够产生更稳健、更符合人类对齐的LLM。

Conclusion: q方法在理论上优于DPO/RLHF，同时保证了对标量化和参考选择的不变性，通过利用模型的自我反思能力实现了更有效的大语言模型对齐。

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [23] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: 本文认为现有评估AI情感智能的框架需要改进，因为它们未能全面衡量AI相关的EI各个方面。人类EI包含AI缺乏的现象学成分，但AI仍能在感知、解释、响应和适应情感状态方面进行评估。论文通过回顾情感理论和EI理论，批判现有评估框架，并提出改进方案。


<details>
  <summary>Details</summary>
Motivation: 当前评估人工智能情感智能的框架存在不足，未能全面衡量AI相关的EI各个方面。人类EI包含AI缺乏的现象学成分，但AI系统仍能在某些EI方面进行评估。需要建立更适合AI系统的EI评估框架。

Method: 1. 回顾不同情感理论和一般EI理论，评估其在人工系统中的适用性；2. 批判性评估现有基准框架，根据第一部分建立的EI理论识别其不足；3. 提出改进评估策略的方案以避免这些缺陷。

Result: 通过分析发现现有EI评估框架缺乏坚实的情感理论基础，未能充分考虑AI系统的特殊性。人类EI的现象学成分对AI不适用，但AI在情感感知、解释、响应和跨文化适应等方面仍有评估价值。

Conclusion: 需要开发更全面的AI情感智能评估框架，该框架应基于适当的情感理论，充分考虑AI系统的能力限制，并专注于AI实际能够执行的情感相关任务，避免将人类EI的所有方面机械应用于AI评估。

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [24] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: 本文提出"模型信念"概念，利用LLM的token级概率分布来替代传统的"模型选择"单一输出，证明模型信念具有更低的方差和更快的收敛速度，能显著提高LLM生成数据的统计效率。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM模拟人类行为时，通常将LLM的输出（"模型选择"）作为单一数据点，这种做法未能充分利用LLM固有的概率特性，导致数据利用效率低下。

Method: 提出并形式化"模型信念"概念，这是从LLM的token级概率分布中推导出的度量，能捕捉模型在单次生成运行中对选择替代方案的信念分布。证明模型信念与模型选择的均值渐近等价，但具有更好的统计效率。

Result: 在需求估计研究中，模型信念在有限运行次数下比模型选择本身更好地解释和预测真实模型选择，将达到足够准确估计所需的计算量减少了约20倍。

Conclusion: 模型信念应作为默认度量来从LLM生成数据中提取更多信息，因为它能显著提高统计效率并减少计算需求。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [25] [Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control](https://arxiv.org/abs/2512.23292)
*Yoonpyo Lee,Kazuma Kobayashi,Sai Puppala,Sajedul Talukder,Seid Koric,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.AI

TL;DR: 论文提出了一种新的物理AI范式，通过基于物理验证而非感知推理的策略优化，训练紧凑语言模型作为智能物理AI，在反应堆控制场景中实现了从高方差模仿到稳定执行的相变。


<details>
  <summary>Details</summary>
Motivation: 当前通用基础模型在物理系统控制方面存在根本性障碍，即使前沿视觉语言模型在基本定量物理任务上准确率也只有50-53%，它们更像是近似猜测器，保持了语义合理性但违反了物理约束。这种输入不忠实性不是规模不足问题，而是结构限制。感知中心架构优化参数空间模仿，而安全关键控制需要执行动作的结果空间保证。

Method: 提出了一种根本不同的领域特定基础模型路径，引入紧凑语言模型作为智能物理AI，其中策略优化由基于物理的验证驱动而非感知推理。在合成反应堆控制场景上训练了一个3.6亿参数模型，将数据集从10^3扩展到10^5个示例。

Result: 这种方法诱导了通用模型中不存在的尖锐相变：小规模系统表现出高方差模仿和灾难性尾部风险，而大规模模型经历了超过500倍的方差崩溃，稳定了执行级行为。尽管平衡暴露于四种执行器家族，模型自主拒绝了约70%的训练分布，并将95%的运行时间执行集中在单一策略上。学习到的表示可以在不同物理和连续输入模态之间迁移，无需架构修改。

Conclusion: 通过基于物理验证而非感知推理的策略优化，紧凑语言模型可以作为智能物理AI，在物理系统控制中实现从模仿到稳定执行的相变，为领域特定基础模型提供了新的发展路径。

Abstract: The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.

</details>


### [26] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: 该论文研究了规划与验证领域中两个问题的联系：一致性规划与超属性模型检测，证明两者之间存在紧密的对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索规划领域的一致性规划问题与验证领域的超属性模型检测之间的理论联系，这两个问题分别处理不确定环境下的规划执行和涉及多个执行轨迹的系统属性验证。

Method: 论文采用双向编码方法：首先展示如何将超属性模型检测实例高效地转化为一致性规划实例，并证明编码的正确性和完备性；其次证明每个一致性规划问题本身就是一个超属性模型检测任务。

Result: 建立了超属性模型检测与一致性规划之间的双向对应关系，证明了$\exists^*\forall^*$超属性的模型检测与一致性规划在计算复杂性上的紧密联系。

Conclusion: 结论表明规划与验证领域中的这两个核心问题在本质上密切相关，这一发现为跨领域技术转移和方法论融合提供了理论基础。

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [27] [MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning](https://arxiv.org/abs/2512.23412)
*Jiawei Chen,Xintian Shen,Lihao Zheng,Zhenwei Shao,Hongyuan Zhang,Pengfei Yu,Xudong Rao,Ning Mao,Xiaobo Liu,Lian Wen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Shanshan Li,Zide Liu,Jing Luo,Lifu Mu,Xuhao Pan,Chang Ren,Haoyi Sun,Qian Wang,Wei Wang,Hongfu Yang,Jiqing Zhan,Chunpeng Zhou,Zheng Zhou,Hao Ma,Tao Wei,Pan Zhou,Wei Chen*

Main category: cs.AI

TL;DR: MindWatcher是一个集成交替思维和多模态思维链推理的工具集成推理智能体，能够自主决定是否以及如何调用多样化工具，无需依赖人工提示或工作流程。


<details>
  <summary>Details</summary>
Motivation: 传统基于工作流程的智能体在解决需要工具调用的现实世界问题时表现出有限的智能性，而能够自主推理和工具调用的工具集成推理智能体正成为处理涉及多步外部环境交互的复杂决策任务的有力方法。

Method: MindWatcher采用交替思维范式，使模型能够在任何中间阶段在思考和工具调用之间切换，同时具备多模态思维链能力，允许在推理过程中操作图像以获得更精确的搜索结果。配备了全面的辅助推理工具套件，并构建了大规模高质量的本地图像检索数据库。

Result: 实验表明，MindWatcher通过优越的工具调用能力，匹配甚至超越了更大或更新模型的表现，同时揭示了智能体训练中的关键见解，如智能体强化学习中的遗传继承现象。

Conclusion: MindWatcher是一个强大的工具集成推理智能体，能够自主处理广泛领域的多模态问题，并通过更高效的训练基础设施提升了训练速度和硬件利用率。

Abstract: Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.

</details>


### [28] [AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis](https://arxiv.org/abs/2512.23424)
*Jinye Du,Quan Yuan,Zuyao Zhang,Yanzhi Yi,Jiahui Hu,Wangyi Chen,Yiyang Zhu,Qishui Zheng,Wenxiang Zou,Xiangyu Chang,Zuohe Zheng,Zichun Ye,Chao Liu,Shanni Li,Renwei Zhang,Yiping Deng,Xinwei Hu,Xuefeng Jin,Jie Zhao*

Main category: cs.AI

TL;DR: AKG kernel agent是一个多智能体系统，利用LLM代码生成能力自动化AI计算内核的开发、迁移和性能调优，支持多种领域特定语言，在GPU和NPU后端上相比PyTorch Eager实现平均加速1.46倍。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型对高性能计算内核需求激增，但LLM、多模态架构和推荐系统的复杂性，加上稀疏化、量化等技术，以及硬件频繁更新和多样化架构，使得手动优化无法满足需求，成为AI系统开发的关键瓶颈。

Method: 提出AKG kernel agent多智能体系统，自动化内核生成、迁移和性能调优。系统支持多种领域特定语言（Triton、TileLang、CPP、CUDA-C），可针对不同硬件后端，同时保持正确性和可移植性。模块化设计支持快速集成新DSL和硬件目标。

Result: 在KernelBench上使用Triton DSL在GPU和NPU后端进行评估，AKG kernel agent相比PyTorch Eager基线实现平均加速1.46倍，证明了其在加速现代AI工作负载内核开发方面的有效性。

Conclusion: AKG kernel agent通过自动化内核开发流程，利用LLM代码生成能力解决了AI计算内核开发的手动优化瓶颈问题，为应对现代AI工作负载的复杂计算挑战提供了有效的解决方案。

Abstract: Modern AI models demand high-performance computation kernels. The growing complexity of LLMs, multimodal architectures, and recommendation systems, combined with techniques like sparsity and quantization, creates significant computational challenges. Moreover, frequent hardware updates and diverse chip architectures further complicate this landscape, requiring tailored kernel implementations for each platform. However, manual optimization cannot keep pace with these demands, creating a critical bottleneck in AI system development. Recent advances in LLM code generation capabilities have opened new possibilities for automating kernel development. In this work, we propose AKG kernel agent (AI-driven Kernel Generator), a multi-agent system that automates kernel generation, migration, and performance tuning. AKG kernel agent is designed to support multiple domain-specific languages (DSLs), including Triton, TileLang, CPP, and CUDA-C, enabling it to target different hardware backends while maintaining correctness and portability. The system's modular design allows rapid integration of new DSLs and hardware targets. When evaluated on KernelBench using Triton DSL across GPU and NPU backends, AKG kernel agent achieves an average speedup of 1.46$\times$ over PyTorch Eager baselines implementations, demonstrating its effectiveness in accelerating kernel development for modern AI workloads.

</details>


### [29] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: HiR是一种高效的强化学习框架，通过"选择-重写"策略将失败的尝试重放为成功样本，解决复杂指令跟随任务中奖励稀疏的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大型语言模型对齐中面临挑战：初始模型难以生成满足所有约束的高质量响应，导致奖励稀疏或难以区分，阻碍学习效率。

Method: 提出Hindsight instruction Replay (HiR)框架，采用选择-重写策略，基于已满足的约束将失败的尝试重放为成功样本，并在理论和实践上将其构建为指令级和响应级的双重偏好学习。

Result: 实验表明HiR在不同指令跟随任务中都能取得良好效果，同时需要更少的计算资源。代码和数据集已开源。

Conclusion: HiR通过创新的重放机制有效解决了复杂指令跟随任务中的奖励稀疏问题，提高了强化学习的样本效率。

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [30] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: CreativeDC是一种两阶段提示方法，通过解耦创意探索和约束满足，显著提升LLM生成教育问题的多样性和新颖性，同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: LLM在生成教育问题时存在"人工蜂群思维"效应，导致同一模型内和不同模型间产生相似输出，学生接触到的LLM生成问题过于相似重复，损害思维多样性。

Method: 基于Wallas创造力理论和Guilford发散-收敛思维框架，提出CreativeDC两阶段提示方法，将LLM推理明确分为不同阶段，先进行创意探索再进行约束满足。

Result: CreativeDC在多样性、新颖性和实用性综合评估中显著优于基线方法，随着采样增加能生成更多不同问题，且增长速率更快。

Conclusion: 通过解耦创意探索和约束满足的两阶段方法，CreativeDC有效缓解LLM的"人工蜂群思维"问题，显著提升生成教育问题的多样性和新颖性。

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [31] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: 提出I-PERI算法，解决联邦因果发现中客户端存在未知干预的问题，通过恢复客户端图并集的CPDAG，利用干预引起的结构差异定向边，得到更紧的Φ-Markov等价类。


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果发现方法假设所有客户端共享相同因果模型，但实际中客户端特定策略或协议（如医院间差异）会导致异质且未知的干预，需要处理客户端级未知干预的联邦因果发现问题。

Method: 提出I-PERI算法：1) 恢复客户端图并集的CPDAG；2) 利用客户端间干预引起的结构差异定向边；3) 得到更紧的Φ-Markov等价类，用Φ-CPDAG表示。

Result: 提供I-PERI收敛性和隐私保护的理论保证，在合成数据上进行实证评估，证明算法有效性。

Conclusion: I-PERI算法能有效处理联邦因果发现中的客户端未知干预问题，通过利用干预引起的结构差异获得更精确的因果结构表示。

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>

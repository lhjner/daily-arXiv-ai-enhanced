<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue](https://arxiv.org/abs/2601.08950)
*Mayank Sharma,Roy Pea,Hari Subramonyam*

Main category: cs.AI

TL;DR: 该研究针对LLM在教育应用中的局限性，开发了ConvoLearn数据集来训练AI导师，使其更符合建构主义教学原则，显著提升了教学对话质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在教育应用中存在根本性教学限制，倾向于直接提供答案而非支持对话式学习，缺乏建构主义教学原则的体现。

Method: 基于知识建构理论构建ConvoLearn数据集，包含六个核心教学维度；通过人类教师与模拟学生的受控交互创建1250个中学地球科学对话；使用QLoRA对Mistral 7B进行微调。

Result: 微调后的Mistral 7B（M=4.10）在31名教师评估中显著优于基础版本（M=2.59）和Claude Sonnet 4.5（M=2.87），有效转向知识建构策略。

Conclusion: 该工作为未来建构主义AI导师的开发与评估建立了潜在框架，通过针对性数据集训练能有效改善LLM的教学行为。

Abstract: In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.

</details>


### [2] [The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments](https://arxiv.org/abs/2601.09032)
*Logan Ritchie,Sushant Mehta,Nick Heiner,Mason Yu,Edwin Chen*

Main category: cs.AI

TL;DR: 该研究评估前沿AI模型在电商RL环境中的150个职场任务表现，揭示了模型需掌握的5层智能体能力层次结构，最佳模型仍有约40%任务失败率。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体发展，AI评估从单轮响应评估转向交互环境中的多步骤任务完成。需要了解当前前沿模型在真实职场环境中的实际能力水平。

Method: 在Surge提供的真实电商RL环境中，对前沿AI模型进行150个职场任务的实证研究。采用任务中心设计方法，强调多样性和领域专家贡献，并进行详细的失败分析。

Result: 研究发现模型需掌握5层能力层次：工具使用、规划与目标形成、适应性、接地性、常识推理。最佳模型仍有约40%任务失败率，失败模式沿能力层次可预测分布：较弱模型在基础工具使用和规划上挣扎，较强模型主要在需要超越明确指令的上下文推理任务上失败。

Conclusion: 虽然当前前沿模型能展示连贯的多步骤行为，但在真实职场环境中实现人类水平任务完成仍存在显著能力差距。研究为智能体开发提供了重要启示，包括任务中心设计方法和详细的能力层次分析。

Abstract: The advancement of large language model (LLM) based agents has shifted AI evaluation from single-turn response assessment to multi-step task completion in interactive environments. We present an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge. Our analysis reveals an empirically-derived \emph{hierarchy of agentic capabilities} that models must master for real-world deployment: (1) tool use, (2) planning and goal formation, (3) adaptability, (4) groundedness, and (5) common-sense reasoning. Even the best-performing models fail approximately 40\% of the tasks, with failures clustering predictably along this hierarchy. Weaker models struggle with fundamental tool use and planning, whereas stronger models primarily fail on tasks requiring contextual inference beyond explicit instructions. We introduce a task-centric design methodology for RL environments that emphasizes diversity and domain expert contributions, provide detailed failure analysis, and discuss implications for agent development. Our findings suggest that while current frontier models can demonstrate coherent multi-step behavior, substantial capability gaps remain before achieving human-level task completion in realistic workplace settings.

</details>


### [3] [Programming over Thinking: Efficient and Robust Multi-Constraint Planning](https://arxiv.org/abs/2601.09097)
*Derrick Goh Xin Deik,Quanyu Long,Zhengyuan Liu,Nancy F. Chen,Wenya Wang*

Main category: cs.AI

TL;DR: SCOPE框架通过分离推理与代码执行，解决了多约束规划中LLM方法的局限性，实现了高效、可复用的求解器函数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在多约束规划中存在根本性局限：纯推理范式容易产生不一致性和错误累积，且成本高昂；结合编码或求解器的方法缺乏灵活性，无法捕捉跨问题的通用逻辑。

Method: 提出SCOPE框架，将查询特定的推理与通用代码执行解耦，生成一致、确定且可复用的求解器函数，仅需最小化修改输入参数。

Result: 在GPT-4o上，SCOPE在TravelPlanner任务上达到93.1%成功率，比最佳基线（CoT）提升61.6%，同时推理成本降低1.4倍，时间减少约4.67倍。

Conclusion: SCOPE通过分离推理与执行，实现了多约束规划中的最先进性能，同时降低了成本和延迟，提供了可复用的求解器函数。

Abstract: Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.

</details>


### [4] [DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model](https://arxiv.org/abs/2601.09100)
*Lixiang Zhang,Chenggong Zhao,Qing Gao,Xiaoke Zhao,Gengyi Bai,Jinhu Lv*

Main category: cs.AI

TL;DR: DScheLLM：基于双系统推理架构（快慢思维）的微调大语言模型动态调度方法，用于处理生产调度中的动态扰动


<details>
  <summary>Details</summary>
Motivation: 传统生产调度方法对动态扰动（如加工时间变化、机器可用性变化、意外任务插入）敏感，通常依赖事件特定模型和显式分析公式，限制了其适应性和对未见扰动的泛化能力

Method: 提出DScheLLM方法，在双系统（快-慢）推理架构中利用微调的大语言模型处理不同规模的扰动。构建统一的LLM框架处理动态事件，使用运筹学求解器获得的精确调度生成快慢推理模式的训练数据集，基于华为OpenPangu Embedded-7B模型使用LoRA进行微调

Result: 在标准作业车间调度基准测试中，快思维模式能高效生成高质量调度方案，慢思维模式能生成与求解器兼容且格式良好的决策输入

Conclusion: 这是最早将大语言模型应用于动态环境作业车间调度的研究之一，突显了LLM在智能自适应调度优化中的巨大潜力

Abstract: Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.

</details>


### [5] [AviationLMM: A Large Multimodal Foundation Model for Civil Aviation](https://arxiv.org/abs/2601.09105)
*Wenbin Li,Jingling Wu,Xiaoyong Lin. Jing Chen,Cong Chen*

Main category: cs.AI

TL;DR: 该论文提出了AviationLMM——一个用于民用航空的大型多模态基础模型，旨在统一航空领域的异构数据流，实现理解、推理、生成和智能体应用。


<details>
  <summary>Details</summary>
Motivation: 当前民用航空AI解决方案存在孤岛化和局限性，专注于孤立任务或单一模态，难以整合语音通信、雷达轨迹、传感器流和文本报告等异构数据，限制了态势感知、适应性和实时决策支持能力。

Method: 提出AviationLMM模型架构，能够处理空-地语音、监视数据、机载遥测、视频和结构化文本等多模态输入，执行跨模态对齐和融合，并产生从态势总结、风险预警到预测性诊断和多模态事件重建的灵活输出。

Result: 论文提出了AviationLMM的完整愿景和设计框架，识别了实现该愿景需要解决的关键研究机会，包括数据获取、对齐与融合、预训练、推理、可信度、隐私保护、缺失模态鲁棒性和合成场景生成等挑战。

Conclusion: 通过阐述AviationLMM的设计和挑战，旨在推动民用航空基础模型的进展，并促进协调研究努力，构建一个集成、可信且保护隐私的航空AI生态系统。

Abstract: Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.

</details>


### [6] [The AI Hippocampus: How Far are We From Human Memory?](https://arxiv.org/abs/2601.09113)
*Zixia Jia,Jiaqi Li,Yipeng Kang,Yuxuan Wang,Tong Wu,Quansen Wang,Xiaobo Wang,Shuyi Zhang,Junzhe Shen,Qing Li,Siyuan Qi,Yitao Liang,Di He,Zilong Zheng,Song-Chun Zhu*

Main category: cs.AI

TL;DR: 该综述系统性地总结了LLMs和MLLMs中的记忆机制，提出了包含隐性、显性和智能体记忆的三层分类体系，并探讨了多模态环境下的记忆整合挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从静态预测器向具备持续学习和个性化推理能力的交互系统转变，记忆机制已成为其架构和功能演进的核心主题。需要系统性地梳理和分类现有记忆研究，为未来发展方向提供指导。

Method: 采用文献综述方法，将现有研究组织成三层分类体系：1) 隐性记忆（预训练transformer内部参数中的知识）；2) 显性记忆（外部存储和检索组件）；3) 智能体记忆（自主智能体中的持久性记忆结构）。同时扩展到多模态环境下的记忆整合。

Result: 提出了一个全面的记忆分类框架，涵盖了从模型内部知识表示到外部存储系统再到智能体长期记忆的完整谱系。识别了关键架构进展、基准任务和开放挑战。

Conclusion: 记忆机制对于增强LLMs和MLLMs的推理能力、适应性和上下文保真度至关重要。未来的研究需要解决记忆容量、对齐、事实一致性和跨系统互操作性等挑战，特别是在多模态环境中保持跨模态一致性。

Abstract: Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.

</details>


### [7] [Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback](https://arxiv.org/abs/2601.09182)
*JungMin Yun,JuneHyoung Kwon,MiHyeon Kim,YoungBin Kim*

Main category: cs.AI

TL;DR: 本文提出将LLM作为辅助工具来培养和提升人工审稿人能力，而非自动生成评审，以解决AI研究快速发展带来的审稿人缺口问题


<details>
  <summary>Details</summary>
Motivation: AI研究的快速扩张加剧了"审稿人缺口"，威胁到同行评审的可持续性，并导致低质量评审的恶性循环。现有使用LLM自动生成评审的方法存在不足，需要新的解决方案。

Method: 提出范式转变：将LLM定位为辅助和教育人工审稿人的工具。定义了高质量同行评审的核心原则，并基于此提出两个互补系统：1) LLM辅助的导师系统，培养审稿人长期能力；2) LLM辅助的反馈系统，帮助审稿人改进评审质量。

Result: 提出了以人为中心的方法框架，旨在加强审稿人专业知识，为构建更可持续的学术生态系统做出贡献。

Conclusion: 与使用LLM自动生成评审相比，将LLM作为辅助工具来培养人工审稿人能力是更有效和可持续的解决方案，能够从根本上解决审稿人缺口问题并提升同行评审质量。

Abstract: The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.

</details>


### [8] [MAXS: Meta-Adaptive Exploration with LLM Agents](https://arxiv.org/abs/2601.09259)
*Jian Zhang,Zhiyuan Wang,Zhangqi Wang,Yu He,Haoran Luo,li yuan,Lingling Zhang,Rui Mao,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: MAXS是一个基于LLM Agent的元自适应推理框架，通过前瞻策略和轨迹收敛机制，解决多工具推理中的局部短视和轨迹不稳定问题，平衡全局有效性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM Agent方法在推理过程中存在两个主要问题：1）由于缺乏前瞻性导致局部短视生成；2）早期微小错误会引发轨迹不稳定，导致推理路径发散。这些问题使得难以平衡全局有效性和计算效率。

Method: 提出MAXS框架，采用前瞻策略向前扩展推理路径几步，估计工具使用的优势值，结合步骤一致性方差和步骤间趋势斜率联合选择稳定、一致且高价值的推理步骤。引入轨迹收敛机制，一旦路径一致性达成就停止进一步展开，控制计算成本。

Result: 在三个基础模型（MiMo-VL-7B、Qwen2.5-VL-7B、Qwen2.5-VL-32B）和五个数据集上的广泛实验表明，MAXS在性能和推理效率方面均优于现有方法。进一步分析证实了前瞻策略和工具使用的有效性。

Conclusion: MAXS通过元自适应推理框架有效解决了LLM Agent推理中的局部短视和轨迹不稳定问题，实现了资源效率和全局有效性的平衡，在多工具推理任务中表现出优越性能。

Abstract: Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.

</details>


### [9] [Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models](https://arxiv.org/abs/2601.09260)
*Yan Liu,Feng Zhang,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Han Liu,Yangdong Deng*

Main category: cs.AI

TL;DR: CoT-Flow将离散推理步骤重构为连续概率流，量化每个步骤对正确答案的贡献，实现流引导解码和流强化学习，在推理效率与性能间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 当前思维链方法将推理过程视为不可分割的序列，缺乏量化逐步信息增益的内在机制，导致推理效率低下（冗余探索）和优化困难（稀疏监督或昂贵验证器）。

Method: 提出CoT-Flow框架，将离散推理步骤重构为连续概率流，量化每个步骤对正确答案的贡献。基于此实现两种方法：流引导解码（贪婪流解码策略提取信息高效推理路径）和流强化学习（构建无需验证器的密集奖励函数）。

Result: 在具有挑战性的基准测试中，CoT-Flow在推理效率和推理性能之间实现了优越的平衡。

Conclusion: CoT-Flow通过将推理过程建模为连续概率流，解决了当前思维链方法的粒度差距问题，为高效且有效的推理提供了新范式。

Abstract: High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.

</details>


### [10] [Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants](https://arxiv.org/abs/2601.09264)
*Ziyi Shi,Xusen Guo,Hongliang Lu,Mingxing Peng,Haotian Wang,Zheng Zhu,Zhenning Li,Yuxuan Liang,Xinhu Zheng,Hai Yang*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体政策制定框架，用于跨区域协调的主动式疫情控制，相比现实政策可显著降低感染和死亡人数。


<details>
  <summary>Details</summary>
Motivation: 传统疫情控制政策通常是碎片化和反应式的，各地区政策制定孤立且仅在疫情升级后才调整，缺乏主动干预和全球协调。需要一种能够实现跨区域协调、主动干预的疫情控制方法。

Method: 提出LLM多智能体政策制定框架，每个行政区分配一个LLM智能体作为AI政策助手。智能体基于特定区域的流行病学动态进行推理，同时与其他智能体通信以考虑跨区域相互依赖性。框架整合真实世界数据、疫情演化模拟器和结构化智能体间通信，通过闭环模拟过程共同探索反事实干预场景并合成协调的政策决策。

Result: 使用美国2020年4月至12月州级COVID-19数据、真实流动记录和观察到的政策干预进行验证。相比现实疫情结果，该方法在单个州层面将累计感染和死亡分别降低高达63.7%和40.1%；在跨州汇总层面分别降低39.0%和27.0%。

Conclusion: LLM多智能体系统能够通过协调政策制定实现更有效的疫情控制，为公共卫生危机管理提供了新的技术途径。

Abstract: Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...

</details>


### [11] [RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering](https://arxiv.org/abs/2601.09269)
*Wencheng Ye,Liang Peng,Xiaoyang Yuan,Yi Bin,Pengpeng Zeng,Hengyu Jin,Heng Tao Shen*

Main category: cs.AI

TL;DR: RISER是一个基于路由器的自适应激活干预框架，通过可复用的推理向量库和轻量级路由器动态组合干预策略，在零样本设置下提升LLM推理性能，同时保持高令牌效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活干预的方法通常采用静态、手动干预，无法适应复杂推理的动态特性。训练密集型方法需要参数更新，而激活干预作为参数高效替代方案，其静态特性限制了在动态推理场景中的应用效果。

Method: RISER构建可复用的推理向量库，使用轻量级路由器根据输入动态组合这些向量。路由器通过强化学习在任务级奖励下优化，以涌现和组合方式激活潜在的认知原语，实现自适应激活空间干预。

Result: 在七个多样化基准测试中，RISER相比基础模型实现了3.4-6.5%的平均零样本准确率提升，同时超越思维链推理，具有2-3倍的令牌效率提升和稳健的准确率增益。

Conclusion: RISER能够自主组合多个向量形成可解释的精确控制策略，指向更可控和高效的LLM推理方向，为自适应激活干预提供了有效框架。

Abstract: Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.

</details>


### [12] [$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation](https://arxiv.org/abs/2601.09274)
*Jian Zhang,Yu He,Zhiyuan Wang,Zhangqi Wang,Kai He,Fangzhi Xu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: A³-Bench是一个评估科学推理中记忆驱动机制的新基准，关注锚点和吸引子的激活与整合，而非仅评估最终答案或步骤连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估最终答案或逐步推理的连贯性，但忽视了人类科学推理中基于记忆驱动的机制——涉及激活锚点和吸引子并将其整合到多步推理中。

Method: 1) 使用SAPM过程（主体、锚点与吸引子、问题、记忆发展）标注2,198个跨领域科学推理问题；2) 引入基于锚点和吸引子的双尺度记忆评估框架及AAUI指标测量记忆激活率；3) 通过多种基础模型和范式进行实验验证。

Result: 通过实验验证了A³-Bench的有效性，并分析了记忆激活如何影响推理性能，为理解记忆驱动的科学推理提供了见解。

Conclusion: 该研究提出了首个专门评估科学推理中记忆驱动机制的基准，填补了现有评估体系的空白，为深入理解记忆在推理中的作用提供了新工具和视角。

Abstract: Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.

</details>


### [13] [STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models](https://arxiv.org/abs/2601.09281)
*Jingjing Zhou,Gaoxiang Cong,Li Su,Liang Li*

Main category: cs.AI

TL;DR: STaR框架通过语义检测、安全提示前缀、轨迹感知抑制和自适应过滤，在推理时实现LRM的无学习隐私保护，解决了传统方法无法处理推理链中敏感信息的问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在生成复杂思维链轨迹时存在严重隐私风险，敏感信息可能深嵌于推理过程中。现有LLM无学习方法通常只修改最终答案，无法移除中间步骤的敏感内容，导致隐私持续泄露和安全降级。

Method: 提出Sensitive Trajectory Regulation（STaR）框架：1）语义感知检测识别敏感内容；2）通过安全提示前缀注入全局安全约束；3）轨迹感知抑制动态阻断整个推理链中的敏感内容；4）令牌级自适应过滤防止生成精确和转述的敏感令牌。

Result: 在R-TOFU基准测试中，STaR实现了全面稳定的无学习效果，同时最小化效用损失。还引入了两个新评估指标：多解码一致性评估（MCS）和多粒度成员推理攻击（MIA）评估。

Conclusion: STaR为LRMs中的隐私保护推理设定了新标准，通过参数无关的推理时无学习框架，有效解决了推理链中的隐私泄露问题，同时保持了模型效用。

Abstract: Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.

</details>


### [14] [Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing](https://arxiv.org/abs/2601.09282)
*Leszek Sliwko,Jolanta Mizeria-Pietraszko*

Main category: cs.AI

TL;DR: 该论文提出了一种基于自然语言处理的语义化、意图驱动的集群调度范式，通过LLM解析自然语言分配提示，简化复杂的集群工作负载配置。


<details>
  <summary>Details</summary>
Motivation: 集群工作负载分配通常需要复杂的配置，存在可用性差距。传统调度系统配置复杂，用户需要掌握专业知识，这限制了集群系统的易用性和可访问性。

Method: 1. 采用语义化、意图驱动的调度范式，使用自然语言处理技术
2. 集成大型语言模型（LLM）作为Kubernetes调度器扩展器
3. 通过LLM解释自然语言分配提示注释，用于软亲和性偏好
4. 开发原型系统，包含集群状态缓存和意图分析器（使用AWS Bedrock）
5. 支持软亲和性偏好，允许冲突的软偏好处理

Result: 1. LLM解析准确率高：在评估基准数据集上，顶级模型（Amazon Nova Pro/Premier和Mistral Pixtral Large）的Subset Accuracy超过95%，显著优于基线引擎
2. 调度质量优越：在六个场景的测试中，原型系统相比标准Kubernetes配置实现了更优或相当的放置效果
3. 在复杂和定量场景中表现突出，能够有效处理冲突的软偏好
4. 验证了使用LLM进行可访问调度的可行性

Conclusion: 该研究证实了语义化软亲和性在简化工作负载编排方面的可行性，但指出了同步LLM延迟等限制，建议采用异步处理以实现生产就绪。这项工作为简化集群调度配置提供了有前景的方向。

Abstract: Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.

</details>


### [15] [Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments](https://arxiv.org/abs/2601.09382)
*Qinglong Shi,Donghai Wang,Hantao Zhou,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.AI

TL;DR: 提出了一种面向任务的主动式智能体交互范式，通过意图条件监控和事件触发跟进两大能力，解决当前大语言模型智能体在动态环境中长期任务交互的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型智能体主要采用被动响应范式，只能在短期会话中响应用户即时查询，无法维持长期用户意图并适应动态变化的外部环境，这限制了其在复杂任务导向交互中的应用。

Method: 1. 形式化主动性的两个关键能力：意图条件监控（基于对话历史自主制定触发条件）和事件触发跟进（检测到有用环境更新时主动与用户互动）；2. 引入高质量数据合成管道构建动态环境中的复杂多轮对话数据；3. 提出新的评估基准ChronosBench；4. 使用合成数据进行监督学习微调模型。

Result: 1. 评估了当前领先的闭源和开源模型，揭示了它们在长期任务导向交互中的缺陷；2. 使用合成数据微调的模型在包含用户意图转变的复杂任务中实现了85.19%的任务完成率，优于其他测试模型，验证了数据驱动策略的有效性。

Conclusion: 提出的主动式任务导向智能体交互范式能够有效弥合相对静态的用户需求与动态环境之间的差距，通过数据驱动的训练方法显著提升了智能体在动态环境中处理复杂长期任务的能力。

Abstract: Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.

</details>


### [16] [What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding](https://arxiv.org/abs/2601.09503)
*Siyuan Liu,Hongbang Yuan,Xinze Li,Ziyue Zhu,Yixin Cao,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: T2QBench评估框架揭示LLM智能体任务成功与环境理解脱节，当前记忆机制无法有效获取环境模型，主动探索和细粒度状态表示是主要瓶颈


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体评估主要依赖基于轨迹的任务成功率指标，但未能评估智能体是否真正理解环境模型，存在任务成功与环境理解脱钩的问题

Method: 提出Task-to-Quiz (T2Q)确定性自动化评估范式，将任务执行与世界状态理解解耦，构建T2QBench评估套件，包含30个环境和1,967个基础QA对

Result: 实验表明任务成功率常不能反映环境理解能力，当前记忆机制无法有效帮助智能体获取环境的基础模型，主动探索和细粒度状态表示是主要瓶颈

Conclusion: T2QBench为评估LLM智能体环境理解提供了新范式，揭示了当前方法的局限性，为开发更具泛化能力的自主智能体提供了坚实基础

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.

</details>


### [17] [Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning](https://arxiv.org/abs/2601.09536)
*Dongjie Cheng,Yongqi Li,Zhixin Ma,Hongru Cai,Yupeng Hu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.AI

TL;DR: Omni-R1提出统一的生成式多模态推理框架，通过生成中间图像统一多种多模态推理技能，无需特定任务推理模式，在广泛多模态任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在推理方面存在局限性：早期方法仅依赖文本推理，近期方法虽然引入多模态信息，但通常采用单一任务特定的推理模式，限制了在不同多模态任务间的泛化能力。实际上，多模态任务需要多样化的推理技能，如放大特定区域或在图像中标记对象。

Method: 提出统一生成式多模态推理范式，通过在推理过程中生成中间图像来统一多样化的多模态推理技能。具体实现为Omni-R1，采用两阶段SFT+RL框架，包含感知对齐损失和感知奖励，实现功能性图像生成。同时提出Omni-R1-Zero，通过从纯文本推理数据中引导逐步可视化，无需多模态标注。

Result: 实验结果表明，Omni-R1在广泛的多模态任务中实现了统一的生成式推理，而Omni-R1-Zero在平均性能上能够匹配甚至超越Omni-R1，展示了生成式多模态推理的潜力。

Conclusion: 该研究提出的统一生成式多模态推理框架通过生成中间图像有效统一了多种多模态推理技能，为多模态推理提供了有前景的新方向，特别是在无需多模态标注的情况下仍能实现优异性能。

Abstract: Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.

</details>


### [18] [Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning](https://arxiv.org/abs/2601.09667)
*Zhiyuan Hu,Yunhai Hu,Juncheng Liu,Shuyue Stella Li,Yucheng Wang,Zhen Xu,See-Kiong Ng,Anh Tuan Luu,Xinxing Xu,Bryan Hooi,Cynthia Breazeal,Hae Won Park*

Main category: cs.AI

TL;DR: MATTRL框架通过测试时强化学习将结构化文本经验注入多智能体推理过程，无需训练即可提升多智能体系统的性能


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在实际应用中具有优势，但多智能体强化学习训练资源密集且不稳定，存在非平稳性和稀疏高方差奖励问题

Method: MATTRL框架构建多专家团队进行多轮讨论，检索并整合测试时经验，达成共识决策，同时研究信用分配机制构建回合级经验池并重新注入对话

Result: 在医学、数学和教育领域的挑战性基准测试中，MATTRL比多智能体基线平均提升3.67%准确率，比单智能体基线提升8.67%

Conclusion: MATTRL提供了一种稳定、有效且高效的路径，无需调优即可实现分布偏移鲁棒的多智能体推理

Abstract: Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.

</details>

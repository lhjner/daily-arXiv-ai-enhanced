<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution](https://arxiv.org/abs/2512.19882)
*Mahdi Mostajabdaveh,F. Sibel Salman,Walter J. Gutjahr*

Main category: cs.AI

TL;DR: 该研究提出了一种双目标优化方法，用于灾后人道主义物流中的救援物资分配，同时考虑效率（最小化总旅行时间）和公平性（最小化未满足需求的基尼系数不平等），通过分支定价算法有效解决了车辆路径和有限物资分配问题。


<details>
  <summary>Details</summary>
Motivation: 重大灾害中，预先储备的救援物资往往无法满足所有需求，需要在多个避难所之间公平分配有限物资。传统方法通常只关注效率（如最小化总旅行时间），但忽略了分配的公平性，可能导致某些避难所获得过多援助而其他避难所严重不足。

Method: 1. 建立双目标混合整数规划模型：一个目标是最小化未满足需求的基尼系数不平等度量，另一个目标是最小化总旅行时间；2. 使用ε-约束方法处理双目标优化；3. 推导最优解的数学性质，引入有效不等式；4. 设计给定可行车辆路径的最优配送分配算法；5. 开发分支定价算法高效求解问题。

Result: 1. 分支定价算法在土耳其Van地震和伊斯坦布尔Kartal地区预测数据的实际数据集上显著优于商业MIP求解器；2. 双目标方法在不牺牲效率的情况下将援助分配不平等降低了34%；3. 当时间约束非常宽松或严格时，优先考虑需求覆盖的词典序优化有效；4. 对于中等限制的时间约束，平衡方法对避免不公平结果至关重要。

Conclusion: 该研究提出的双目标优化框架和分支定价算法能够有效平衡灾后救援物资分配的效率和公平性。结果表明，在不同时间约束条件下需要采用不同的优化策略：宽松或严格约束时优先需求覆盖，中等约束时需要平衡方法以确保公平分配。

Abstract: The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating limited relief supplies. To balance efficiency and equity, we formulate a bi-objective problem: minimizing a Gini-index-based measure of inequity in unsatisfied demand for fair distribution and minimizing total travel time for timely delivery. We propose a Mixed Integer Programming (MIP) model and use the $ε$-constraint method to handle the bi-objective nature. By deriving mathematical properties of the optimal solution, we introduce valid inequalities and design an algorithm for optimal delivery allocations given feasible vehicle routes. A branch-and-price (B&P) algorithm is developed to solve the problem efficiently. Computational tests on realistic datasets from a past earthquake in Van, Turkey, and predicted data for Istanbul's Kartal region show that the B&P algorithm significantly outperforms commercial MIP solvers. Our bi-objective approach reduces aid distribution inequity by 34% without compromising efficiency. Results indicate that when time constraints are very loose or tight, lexicographic optimization prioritizing demand coverage over fairness is effective. For moderately restrictive time constraints, a balanced approach is essential to avoid inequitable outcomes.

</details>


### [2] [Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification](https://arxiv.org/abs/2512.19957)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Rodrigo Pereira David,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 该论文提出了一种针对PlantClef 2025挑战赛的解决方案，通过使用训练数据集中的类别原型作为指导，训练分割Vision Transformer模型，实现高分辨率图像中的细粒度多标签物种识别。


<details>
  <summary>Details</summary>
Motivation: 解决PlantClef 2025挑战赛中的细粒度多标签物种识别问题，该任务需要在植被群落的高分辨率图像中同时识别多个物种，面临从单物种分类到多标签分类的领域适应挑战。

Method: 1. 从训练数据集中提取特征并应用K-Means聚类创建类别原型；2. 构建定制化的窄Vision Transformer模型，用预训练的DinoV2替换patch嵌入层；3. 训练分割模型从测试图像重建训练数据集的类别原型；4. 利用注意力分数识别感兴趣区域并指导分类过程。

Result: 该方法在PlantCLEF 2025挑战赛的私有排行榜上获得第五名，F1分数为0.33331，与最佳提交结果仅相差0.03，显示出在基准任务中具有竞争力的性能。

Conclusion: 提出的基于类别原型指导的分割Vision Transformer方法能够有效实现从单物种分类到植被群落多标签分类的领域适应，在细粒度物种识别任务中取得了有竞争力的结果。

Abstract: This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.

</details>


### [3] [FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification](https://arxiv.org/abs/2512.19960)
*Luciano Araujo Dourado Filho,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 提出一种通过类内聚类生成伪标签进行层次分类的方法，以缓解细粒度视觉分类任务中的类内变异问题，在PlantNet300k数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类任务中，类内变异（同一类别内图像的差异程度）会阻碍深度学习模型的学习过程，特别是当这些类别样本数量不足时。需要一种方法来缓解类内变异问题，提升细粒度特征学习能力。

Method: 提出一种新颖方法：首先对每个类别单独进行聚类，发现编码图像间相似度的潜在伪标签；然后利用这些伪标签进行层次分类过程，学习更细粒度的视觉特征，从而缓解类内变异问题。

Result: 在PlantNet300k数据集上的初步实验揭示了未来工作需要发展的关键点。尽管方法中某些组件尚未完全优化，但仍在该数据集上达到了最先进的性能。

Conclusion: 通过类内聚类生成伪标签进行层次分类的方法能够有效缓解细粒度视觉分类中的类内变异问题，提升模型性能，为未来相关研究提供了有前景的方向。

Abstract: Intra-class variability is given according to the significance in the degree of dissimilarity between images within a class. In that sense, depending on its intensity, intra-class variability can hinder the learning process for DL models, specially when such classes are also underrepresented, which is a very common scenario in Fine-Grained Visual Categorization (FGVC) tasks. This paper proposes a novel method that aims at leveraging classification performance in FGVC tasks by learning fine-grained features via classification of class-wise cluster assignments. Our goal is to apply clustering over each class individually, which can allow to discover pseudo-labels that encodes a latent degree of similarity between images. In turn, those labels can be employed in a hierarchical classification process that allows to learn more fine-grained visual features and thereby mitigating intra-class variability issues. Initial experiments over the PlantNet300k enabled to shed light upon several key points in which future work will have to be developed in order to find more conclusive evidence regarding the effectiveness of our method. Our method still achieves state-of-the-art performance on the PlantNet300k dataset even though some of its components haven't been shown to be fully optimized. Our code is available at \href{https://github.com/ADAM-UEFS/FGDCC}{https://github.com/ADAM-UEFS/FGDCC}.

</details>


### [4] [Discovering Lie Groups with Flow Matching](https://arxiv.org/abs/2512.20043)
*Jung Yeon Park,Yuxuan Chen,Floor Eijkelboom,Jan-Willem van de Meent,Lawson L. S. Wong,Robin Walters*

Main category: cs.AI

TL;DR: 提出LieFlow方法，通过李群上的流匹配从数据中学习对称性，无需先验知识即可发现离散群等对称结构


<details>
  <summary>Details</summary>
Motivation: 对称性对理解物理系统和提升机器学习性能都很重要，但通常需要先验知识。本文旨在直接从数据中学习对称性，无需假设对称性类型

Method: 提出LieFlow方法，将对称性发现定义为在更大假设群上学习分布，使学习到的分布匹配数据中观察到的对称性。使用李群上的流匹配技术，并针对对称目标模式引起的"最后一刻收敛"问题提出新的插值方案

Result: 在2D和3D点云上的实验成功发现了离散群，包括通过复域流匹配发现的反射对称性。新方法比先前工作更灵活，所需假设更少

Conclusion: LieFlow能够直接从数据中发现对称性，包括离散群等复杂对称结构，为解决对称性发现问题提供了更灵活、假设更少的方法

Abstract: Symmetry is fundamental to understanding physical systems, and at the same time, can improve performance and sample efficiency in machine learning. Both pursuits require knowledge of the underlying symmetries in data. To address this, we propose learning symmetries directly from data via flow matching on Lie groups. We formulate symmetry discovery as learning a distribution over a larger hypothesis group, such that the learned distribution matches the symmetries observed in data. Relative to previous works, our method, \lieflow, is more flexible in terms of the types of groups it can discover and requires fewer assumptions. Experiments on 2D and 3D point clouds demonstrate the successful discovery of discrete groups, including reflections by flow matching over the complex domain. We identify a key challenge where the symmetric arrangement of the target modes causes ``last-minute convergence,'' where samples remain stationary until relatively late in the flow, and introduce a novel interpolation scheme for flow matching for symmetry discovery.

</details>


### [5] [Learning Skills from Action-Free Videos](https://arxiv.org/abs/2512.20052)
*Hung-Chieh Fang,Kuo-Han Hung,Chu-Rong Chen,Po-Jung Chou,Chun-Kai Yang,Po-Chen Ko,Yu-Chiang Wang,Yueh-Hua Wu,Min-Hung Chen,Shao-Hua Sun*

Main category: cs.AI

TL;DR: SOF框架从无动作视频中学习基于光流的潜在技能，实现视频技能到机器人动作的转换，提升多任务和长时程规划能力


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以转换为底层动作，而潜在动作模型缺乏高级规划能力，需要桥接视频学习与机器人动作执行之间的鸿沟

Method: 提出SOF框架，通过光流中间表示学习潜在技能空间，该空间同时捕捉视频动态和机器人动作信息，实现视频技能到动作的转换

Result: 实验表明SOF在多任务和长时程设置中持续提升性能，能够直接从原始视觉数据中获取和组合技能

Conclusion: SOF通过光流表示的技能抽象成功桥接了视频学习与机器人动作执行，为从视频数据学习通用机器人技能提供了有效框架

Abstract: Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.

</details>


### [6] [Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach](https://arxiv.org/abs/2512.20056)
*Hao Li,Fabian Deuser,Wenping Yin,Steffen Knoblauch,Wufan Zhao,Filip Biljecki,Yong Xue,Wei Huang*

Main category: cs.AI

TL;DR: 提出ProbGLC方法，结合概率性和确定性地理定位模型，通过交叉视图图像匹配实现灾害快速响应，在多个灾害数据集上取得优异定位精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着气候变化加剧，灾害事件频发且强度增加，快速准确的灾害位置识别对于应急响应和资源分配至关重要。现有方法在定位精度和模型可解释性方面存在局限，需要新的解决方案。

Method: 提出Probabilistic Cross-view Geolocalization (ProbGLC)方法，将概率性和确定性地理定位模型统一到一个框架中。该方法能够处理多种灾害类型的交叉视图图像对，并提供概率分布和可定位性评分等独特特征。

Result: 在两个交叉视图灾害数据集（MultiIAN和SAGAINDisaster）上的实验表明，ProbGLC在1公里精度达到0.86，25公里精度达到0.97，同时通过概率分布和可定位性评分提供了良好的模型可解释性。

Conclusion: ProbGLC方法在灾害地理定位方面表现出优越性能，结合了高精度定位和模型可解释性，为快速灾害响应提供了有前景的生成式交叉视图解决方案。

Abstract: As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC

</details>


### [7] [Scaling Reinforcement Learning for Content Moderation with Large Language Models](https://arxiv.org/abs/2512.20061)
*Hamed Firooz,Rui Liu,Yuchen Lu,Zhenyu Hou,Fangzhou Xiong,Xiaoyang Zhang,Changshu Jian,Zhicheng Zhu,Jiayuan Ma,Jacob Tao,Chaitali Gupta,Xiaochang Peng,Shike Mei,Hang Cui,Yang Qin,Shuo Tang,Jason Gaedtke,Arpit Mittal*

Main category: cs.AI

TL;DR: 该研究系统评估了强化学习在内容审核分类任务中的扩展性，发现RL在需要复杂政策推理的任务上表现优异，且数据效率比监督微调高100倍，特别适合标注稀缺的场景。


<details>
  <summary>Details</summary>
Motivation: 大规模内容审核是数字生态系统中最紧迫的挑战之一，需要持续评估数十亿用户和AI生成内容是否违反政策。虽然大语言模型在基于政策的审核方面显示出潜力，但在实际场景中训练这些系统达到专家级准确性的挑战尚未充分探索，特别是在标签稀疏、政策定义不断演变、需要超越浅层模式匹配的细致推理等情况下。

Method: 采用全面的实证研究方法，系统评估了多种RL训练方案和奖励塑造策略，包括可验证奖励和LLM作为评判框架，将通用语言模型转化为专门的政策对齐分类器。在三个真实世界内容审核任务上进行实验。

Result: RL表现出S型扩展行为：随着训练数据、rollouts和优化步骤的增加，性能平滑提升后逐渐饱和。RL在需要复杂政策推理的任务上显著提升性能，数据效率比监督微调高100倍，特别适合专家标注稀缺或昂贵的领域。

Conclusion: 强化学习为工业级内容审核系统提供了可行的解决方案，特别是在需要细致政策推理和标注稀缺的场景中。研究结果为实际部署提供了可操作的见解，表明RL可以有效地将通用语言模型转化为专门的政策对齐分类器。

Abstract: Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.

</details>


### [8] [Reason2Decide: Rationale-Driven Multi-Task Learning](https://arxiv.org/abs/2512.20074)
*H M Quamran Hasan,Housam Khalifa Bashier,Jiayi Dai,Mi-Young Kim,Randy Goebel*

Main category: cs.AI

TL;DR: Reason2Decide是一个两阶段训练框架，用于解决临床决策支持系统中预测准确性与解释对齐的挑战，通过分阶段训练减少暴露偏差，在小型模型上实现高性能的解释性决策支持。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在临床决策支持系统中面临关键挑战：需要同时实现高预测准确性和与预测一致的解释生成。现有方法存在暴露偏差问题，导致解释与预测不一致。

Method: 提出Reason2Decide两阶段训练框架：第一阶段训练模型进行理由生成；第二阶段联合训练标签预测和理由生成，采用计划采样策略逐步从基于黄金标签转向基于模型预测。

Result: 在三个医疗数据集上评估，包括专有分诊数据集和公共生物医学QA数据集。Reason2Decide在预测（F1）和理由保真度（BERTScore、BLEU、LLM-as-a-Judge）方面优于其他微调基线。在分诊任务中，对LLM生成、护士撰写和护士后处理的理由均表现出鲁棒性。

Conclusion: Reason2Decide框架显著提升了临床决策支持系统的解释对齐能力，使用比当代基础模型小40倍的模型实现高性能，使临床推理在资源受限环境中更易部署，同时提供可解释的决策支持。

Abstract: Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.

</details>


### [9] [MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization](https://arxiv.org/abs/2512.20135)
*Zhuo Yang,Yeyun chen,Jiaqing Xie,Ben Gao,Shuaike Shen,Wanhao Liu,Liujia Yang,Beilun Wang,Tianfan Fu,Yuqiang Li*

Main category: cs.AI

TL;DR: MolAct是一个基于智能体强化学习的分子设计框架，通过两阶段训练（先学编辑能力，再学优化）让LLM智能体在化学工具辅助下进行多步分子编辑和优化。


<details>
  <summary>Details</summary>
Motivation: 分子编辑和优化是多步骤问题，需要迭代改进性质同时保持化学有效性和结构相似性。传统方法难以同时满足这些要求，因此需要将分子设计形式化为智能体强化学习问题，让智能体学习在化学工具辅助下进行推理、工具使用和分子优化。

Method: 提出MolAct框架，采用两阶段训练范式：第一阶段建立编辑能力，第二阶段重用学习的编辑行为进行性质优化。智能体通过多轮交互调用化学工具进行有效性检查、性质评估和相似性控制，并利用反馈细化后续编辑。具体实现了MolEditAgent（分子编辑）和MolOptAgent（分子优化）两个模型系列。

Result: 在分子编辑任务中，MolEditAgent-7B在添加、删除和替换编辑上分别达到100、95和98的有效性，优于DeepSeek-R1等基线；MolEditAgent-3B接近Qwen3-32B-think等更大模型的性能。在分子优化任务中，MolOptAgent-7B在LogP指标上超越Claude 3.7等基线，在溶解度上保持竞争力，并在其他目标上保持平衡性能。

Conclusion: 将分子设计视为多步骤、工具增强的过程是实现可靠和可解释改进的关键。MolAct框架通过智能体强化学习方法，在分子编辑和优化任务中展现出优越性能，为分子设计提供了新的有效范式。

Abstract: Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed "thinking" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open "thinking" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed "thinking" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.

</details>


### [10] [Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection](https://arxiv.org/abs/2512.20140)
*Xingyou Yin,Ceyao Zhang,Min Hu,Kai Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种简单有效的方法：在时间序列数据token化前注入噪声，以提升冻结大语言模型在零样本时间序列预测中的性能，无需微调。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖微调专用模块来弥合时间序列数据与大语言模型预训练知识之间的差距，但真正无需微调、完全冻结的模型性能对输入数据的文本表示非常敏感。本文旨在探索如何在不进行任何微调的情况下，直接利用现成的大语言模型进行时间序列预测。

Method: 在原始时间序列数据token化前注入噪声，作为一种推理时数据增强手段。这种方法迫使冻结的大语言模型基于稳健的底层时间模式而非表面数值伪影进行外推。同时，为了完全消除大语言模型预训练期间数据污染带来的偏差，引入了两个全新时间序列数据集。

Result: 通过理论分析和多个基准测试的实证验证，表明注入噪声的方法能有效提升冻结大语言模型在时间序列预测中的性能。在新引入的数据集上也观察到一致的性能改进，证明了方法的有效性。

Conclusion: 在时间序列token化前注入噪声是一种简单而有效的策略，能够增强冻结大语言模型在零样本时间序列预测中的鲁棒性。这项研究为直接利用现成大语言模型进行时间序列预测提供了进一步的技术支持。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.

</details>


### [11] [A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers](https://arxiv.org/abs/2512.20161)
*Dhivya Dharshini Kannan,Anupam Trivedi,Dipti Srinivasan*

Main category: cs.AI

TL;DR: 该研究开发了基于双向门控循环单元(BiGRU)的数据中心PUE预测模型，并与GRU模型进行性能比较，旨在通过预测能源效率指标来优化数据中心能源管理。


<details>
  <summary>Details</summary>
Motivation: 数据中心占全球能源消耗和碳足迹的很大比例，随着边缘计算和AI发展，数据中心存储容量不断增长。能源效率是应对气候变化、降低能源成本、提高商业竞争力和促进IT与环境可持续性的有效途径。优化数据中心能源管理对全球可持续发展至关重要。

Method: 1. 开发基于双向门控循环单元(BiGRU)的PUE预测模型
2. 使用EnergyPlus模拟新加坡数据中心，数据集包含52,560个样本和117个特征
3. 采用递归特征消除与交叉验证(RFECV)算法选择最相关特征集
4. 为不同参数设置找到最优超参数配置
5. 使用均方误差(MSE)、平均绝对误差(MAE)和R平方指标比较BiGRU与GRU模型性能

Result: 研究开发了优化的BiGRU-based PUE预测模型，并与GRU模型进行了性能比较。通过特征选择和超参数优化，BiGRU模型在预测数据中心能源使用效率方面表现出色，为能源效率改进提供了数据支持。

Conclusion: BiGRU模型能够有效预测数据中心PUE指标，帮助理解各特征对能源消耗的影响，从而针对性地改进关键特征以提高能源效率。该研究为数据中心能源管理的优化提供了有效的预测工具。

Abstract: Data centers account for significant global energy consumption and a carbon footprint. The recent increasing demand for edge computing and AI advancements drives the growth of data center storage capacity. Energy efficiency is a cost-effective way to combat climate change, cut energy costs, improve business competitiveness, and promote IT and environmental sustainability. Thus, optimizing data center energy management is the most important factor in the sustainability of the world. Power Usage Effectiveness (PUE) is used to represent the operational efficiency of the data center. Predicting PUE using Neural Networks provides an understanding of the effect of each feature on energy consumption, thus enabling targeted modifications of those key features to improve energy efficiency. In this paper, we have developed Bidirectional Gated Recurrent Unit (BiGRU) based PUE prediction model and compared the model performance with GRU. The data set comprises 52,560 samples with 117 features using EnergyPlus, simulating a DC in Singapore. Sets of the most relevant features are selected using the Recursive Feature Elimination with Cross-Validation (RFECV) algorithm for different parameter settings. These feature sets are used to find the optimal hyperparameter configuration and train the BiGRU model. The performance of the optimized BiGRU-based PUE prediction model is then compared with that of GRU using mean squared error (MSE), mean absolute error (MAE), and R-squared metrics.

</details>


### [12] [Concept Generalization in Humans and Large Language Models: Insights from the Number Game](https://arxiv.org/abs/2512.20162)
*Arghavan Bazigaran,Hansem Sohn*

Main category: cs.AI

TL;DR: 比较人类与大型语言模型在数字游戏概念推理任务中的泛化能力差异，发现人类更灵活地结合规则与相似性推理，而LLM更依赖数学规则，且人类具有更强的少样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究人类与大型语言模型在数学概念推理任务中的泛化能力差异，探索两者在归纳偏置和推理策略上的根本区别。

Method: 使用数字游戏作为概念推理任务，以贝叶斯模型为分析框架，比较人类和LLM的归纳偏置和推理策略，考察两者在规则推理和相似性推理上的表现。

Result: 贝叶斯模型能更好地捕捉人类行为：人类灵活地结合规则推理和相似性推理，而LLM更依赖数学规则；人类能从单个示例进行少样本泛化，LLM需要更多样本才能泛化。

Conclusion: 人类与LLM在数学概念推理和泛化方面存在根本差异，人类具有更灵活的推理策略和更强的少样本学习能力。

Abstract: We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.

</details>


### [13] [Offline Safe Policy Optimization From Heterogeneous Feedback](https://arxiv.org/abs/2512.20173)
*Ze Gong,Pradeep Varakantham,Akshat Kumar*

Main category: cs.AI

TL;DR: 本文提出了一种新的离线偏好强化学习框架PreSa，通过直接学习策略而非间接学习奖励和成本模型，解决了长时域连续控制任务中的安全挑战，避免了约束强化学习的需求。


<details>
  <summary>Details</summary>
Motivation: 离线偏好强化学习（PbRL）虽然避免了奖励工程和直接人类交互，但在许多领域和任务中确保安全仍然是一个关键挑战。现有基于人类反馈的安全强化学习方法先学习奖励和成本模型，然后使用约束强化学习优化安全策略，但在长时域连续控制任务中，奖励和成本误差会累积，导致性能下降。

Method: 提出了PreSa（偏好与安全对齐）方法：1）直接基于成对偏好和轨迹段安全性的二元标签学习策略，而非间接学习奖励和成本模型；2）将偏好学习模块与安全对齐结合到约束优化问题中；3）在拉格朗日框架内求解，直接学习奖励最大化且安全的策略，避免了约束强化学习的需求。

Result: 在连续控制任务中，使用合成和真实人类反馈进行评估，该方法成功学习了高奖励的安全策略，优于最先进的基线方法，甚至超过了使用真实奖励和成本的离线安全强化学习方法。

Conclusion: 通过直接学习策略而非间接学习奖励和成本模型，PreSa框架有效解决了长时域连续控制任务中的安全对齐问题，避免了约束强化学习的需求，在安全性和性能方面都表现出优越性。

Abstract: Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.

</details>


### [14] [TongSIM: A General Platform for Simulating Intelligent Machines](https://arxiv.org/abs/2512.20206)
*Zhe Sun,Kunlun Wu,Chuanjian Fu,Zeming Song,Langyong Shi,Zihe Xue,Bohan Jing,Ying Yang,Xiaomeng Gao,Aijia Li,Tianyu Guo,Huiying Li,Xueyuan Yang,Rongkai Liu,Xinyi He,Yuxi Wang,Yue Li,Mingyuan Liu,Yujie Lu,Hongzhao Xie,Shiyun Zhao,Bo Dai,Wei Wang,Tao Yuan,Song-Chun Zhu,Yujia Peng,Zhenliang Zhang*

Main category: cs.AI

TL;DR: TongSIM是一个高保真、通用型平台，用于训练和评估具身智能体，提供100多个多样化室内场景和开放式户外城镇模拟，支持从低级导航到高级复合活动的广泛研究需求。


<details>
  <summary>Details</summary>
Motivation: 随着AI向多模态和具身智能发展，现有仿真平台大多针对特定任务设计，缺乏能够支持从低级具身导航到高级复合活动（如多智能体社会模拟和人机协作）的通用训练环境。

Method: 开发了TongSIM平台，提供100多个多样化多房间室内场景和开放式交互丰富的户外城镇模拟，具备定制场景、任务自适应保真度、多样化智能体类型和动态环境模拟等特性。

Result: TongSIM创建了一个统一的平台，能够精确评估智能体的感知、认知、决策、人机协作、空间和社会推理等能力，通过综合评估框架和基准测试加速具身智能的研究进展。

Conclusion: TongSIM作为一个灵活可扩展的通用平台，填补了现有仿真环境的空白，为研究人员提供了加速训练、评估和推进通用具身智能发展的统一解决方案。

Abstract: As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.

</details>


### [15] [ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge](https://arxiv.org/abs/2512.20276)
*Yuntao Dai,Hang Gu,Teng Wang,Qianyu Cheng,Yifei Zheng,Zhiyong Qiu,Lei Gong,Wenqi Lou,Xuehai Zhou*

Main category: cs.AI

TL;DR: ActionFlow是一个针对边缘设备的VLA模型推理优化框架，通过跨请求流水线调度和内存优化技术，将推理速度提升2.55倍，实现实时动态操作。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在边缘设备上推理延迟高（仅3-5Hz），无法满足机器人交互所需的20-30Hz实时控制需求，现有优化方法要么需要大量重新训练，要么会牺牲模型精度。

Method: 提出了ActionFlow系统级推理框架，核心是跨请求流水线策略，将VLA推理重新定义为微请求的宏流水线；还设计了跨请求状态打包前向算子和统一KV环形缓冲区，将碎片化内存操作融合为高效密集计算。

Result: 在OpenVLA-7B模型上实现了2.55倍的FPS提升，无需重新训练即可在边缘硬件上实现实时动态操作。

Conclusion: ActionFlow通过创新的调度策略和内存优化技术，有效解决了VLA模型在边缘设备上的高延迟问题，为实时机器人控制提供了可行的解决方案。

Abstract: Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.

</details>


### [16] [A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice](https://arxiv.org/abs/2512.20344)
*Yaowei Bai,Ruiheng Zhang,Yu Lei,Xuhua Duan,Jingfeng Yao,Shuguang Ju,Chaoyang Wang,Wei Yao,Yiwan Guo,Guilin Zhang,Chao Wan,Qian Yuan,Lei Chen,Wenjuan Tang,Biqiang Zhu,Xinggang Wang,Tao Sun,Wei Zhou,Dacheng Tao,Yongchao Xu,Chuansheng Zheng,Huangxuan Zhao,Bo Du*

Main category: cs.AI

TL;DR: Janus-Pro-CXR (1B)是一个基于DeepSeek Janus-Pro的胸部X光解读系统，通过多中心前瞻性临床试验验证，在报告生成质量、关键放射学发现检测和工作流程效率方面优于现有模型，包括ChatGPT 4o等更大规模模型。


<details>
  <summary>Details</summary>
Motivation: 全球放射科医生短缺问题因胸部X光工作量巨大而加剧，特别是在初级保健中。现有多模态大语言模型的评估主要依赖自动化指标或回顾性分析，缺乏严格的前瞻性临床验证。

Method: 开发了基于DeepSeek Janus-Pro模型的Janus-Pro-CXR (1B)胸部X光解读系统，并通过多中心前瞻性临床试验(NCT07117266)进行严格验证。采用轻量级架构和领域特定优化。

Result: 系统在自动报告生成方面优于最先进的X光报告生成模型，甚至超越了包括ChatGPT 4o(200B参数)在内的更大规模模型，可靠检测六种临床关键放射学发现。回顾性评估显示报告准确性显著高于Janus-Pro和ChatGPT 4o。前瞻性临床部署中，AI辅助显著提高了报告质量评分，减少解读时间18.3%(P<0.001)，54.3%的病例中专家更偏好AI辅助结果。

Conclusion: Janus-Pro-CXR通过轻量级架构和领域特定优化，提高了诊断可靠性和工作流程效率，特别是在资源有限的环境中。模型架构和实施框架将开源，以促进AI辅助放射学解决方案的临床转化。

Abstract: A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.

</details>


### [17] [Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale](https://arxiv.org/abs/2512.20469)
*Linfeng Zhang,Siheng Chen,Yuzhu Cai,Jingyi Chai,Junhan Chang,Kun Chen,Zhi X. Chen,Zhaohan Ding,Yuwen Du,Yuanpeng Gao,Yuan Gao,Jing Gao,Zhifeng Gao,Qiangqiang Gu,Yanhui Hong,Yuan Huang,Xi Fang,Xiaohong Ji,Guolin Ke,Zixing Lei,Xinyu Li,Yongge Li,Ruoxue Liao,Hang Lin,Xiaolu Lin,Yuxiang Liu,Xinzijian Liu,Zexi Liu,Jintan Lu,Tingjia Miao,Haohui Que,Weijie Sun,Yanfeng Wang,Bingyang Wu,Tianju Xue,Rui Ye,Jinzhe Zeng,Duo Zhang,Jiahui Zhang,Linfeng Zhang,Tianhan Zhang,Wenchang Zhang,Yuzhi Zhang,Zezhong Zhang,Hang Zheng,Hui Zhou,Tong Zhu,Xinyu Zhu,Qingguo Zhou,Weinan E*

Main category: cs.AI

TL;DR: 论文提出Bohrium+SciMaster框架，通过基础设施和生态系统方法解决规模化智能体科学面临的挑战，将科学工具转化为智能体就绪能力，并实现长时程科学工作流的编排。


<details>
  <summary>Details</summary>
Motivation: AI智能体正在成为运行多步骤科学工作流的实用方式，但规模化智能体科学面临诸多挑战：工作流难以观察和复现；许多工具和实验室系统不具备智能体就绪性；执行难以追踪和治理；原型AI科学家系统通常是定制的，限制了重用和基于真实工作流信号的系统性改进。

Method: 提出Bohrium+SciMaster框架：Bohrium作为AI4S资产的托管可追溯中心，将科学数据、软件、计算和实验室系统转化为智能体就绪能力；SciMaster将这些能力编排成长时程科学工作流；在基础设施和编排之间，建立"科学智能基板"组织可重用模型、知识和组件。

Result: 在11个代表性主智能体的真实工作流中演示了该框架，实现了端到端科学周期时间的数量级减少，并从真实工作负载中生成了数百万规模的执行基础信号。

Conclusion: 规模化智能体科学需要基础设施和生态系统方法，Bohrium+SciMaster框架通过提供可追溯的资产中心、工作流编排和科学智能基板，解决了当前面临的挑战，为实现规模化智能体科学提供了可行路径。

Abstract: AI agents are emerging as a practical way to run multi-step scientific workflows that interleave reasoning with tool use and verification, pointing to a shift from isolated AI-assisted steps toward \emph{agentic science at scale}. This shift is increasingly feasible, as scientific tools and models can be invoked through stable interfaces and verified with recorded execution traces, and increasingly necessary, as AI accelerates scientific output and stresses the peer-review and publication pipeline, raising the bar for traceability and credible evaluation.
  However, scaling agentic science remains difficult: workflows are hard to observe and reproduce; many tools and laboratory systems are not agent-ready; execution is hard to trace and govern; and prototype AI Scientist systems are often bespoke, limiting reuse and systematic improvement from real workflow signals.
  We argue that scaling agentic science requires an infrastructure-and-ecosystem approach, instantiated in Bohrium+SciMaster. Bohrium acts as a managed, traceable hub for AI4S assets -- akin to a HuggingFace of AI for Science -- that turns diverse scientific data, software, compute, and laboratory systems into agent-ready capabilities. SciMaster orchestrates these capabilities into long-horizon scientific workflows, on which scientific agents can be composed and executed. Between infrastructure and orchestration, a \emph{scientific intelligence substrate} organizes reusable models, knowledge, and components into executable building blocks for workflow reasoning and action, enabling composition, auditability, and improvement through use.
  We demonstrate this stack with eleven representative master agents in real workflows, achieving orders-of-magnitude reductions in end-to-end scientific cycle time and generating execution-grounded signals from real workloads at multi-million scale.

</details>


### [18] [Benchmarking LLMs for Predictive Applications in the Intensive Care Units](https://arxiv.org/abs/2512.20520)
*Chehak Malhotra,Mehak Gopal,Akshaya Devadiga,Pradeep Singh,Ridam Pal,Ritwik Kashyap,Tavpritesh Sethi*

Main category: cs.AI

TL;DR: 本研究比较了大型语言模型（GatorTron-Base、Llama 8B、Mistral 7B）与小型语言模型（BioBERT、DocBERT等）在预测危重患者休克方面的表现，发现两者性能相当，LLMs在预测临床事件方面并不天然优于SLMs。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，其在自然语言处理领域的应用日益广泛，但在预测性任务中的应用研究相对较少。及时预测休克能够实现早期干预，从而改善患者预后，因此需要评估LLMs在临床预测任务中的表现。

Method: 使用MIMIC III数据库中17,294例ICU住院患者的文本数据，筛选出住院时间>24小时且休克指数>0.7的患者，得到355例正常和87例异常休克指数患者。比较了GatorTron-Base（临床数据训练）、Llama 8B、Mistral 7B等LLMs与BioBERT、DocBERT、BioClinicalBERT、Word2Vec、Doc2Vec等SLMs的性能。在微调过程中使用焦点损失和交叉熵损失来处理类别不平衡问题。

Result: GatorTron Base获得了最高的加权召回率80.5%，但总体性能指标显示SLMs和LLMs之间表现相当。这表明尽管LLMs在基于文本的任务上表现出色，但在预测未来临床事件方面并不天然优于SLMs。

Conclusion: 为了实现有意义的临床结果，未来训练LLMs的努力应优先开发能够预测临床轨迹的模型，而不是专注于命名实体识别或表型分析等较简单的任务。

Abstract: With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.

</details>


### [19] [Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model](https://arxiv.org/abs/2512.20548)
*Zhiyi Duan,Xiangren Wang,Hongyu Yuan,Qianli Xing*

Main category: cs.AI

TL;DR: 该研究构建了首个大规模教师多模态情感分析数据集T-MED，并提出了一种基于非对称注意力机制的多模态教师情感分析模型AAM-TSA，显著提升了教师情感分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 教师的情感状态在教育场景中至关重要，深刻影响教学效果、学生参与度和学习成就。然而，现有研究往往因教师情感的表达具有表演性质，且忽视了教学信息对情感表达的关键影响，而无法准确捕捉教师情感。

Method: 1. 构建T-MED数据集：采用人机协作标注流程，收集了来自250个真实课堂、11个学科、涵盖K-12到高等教育的14,938个教师情感数据实例，包含多模态文本、音频、视频和教学信息。
2. 提出AAM-TSA模型：引入非对称注意力机制和分层门控单元，实现差异化的跨模态特征融合和精确的情感分类。

Result: 实验结果表明，AAM-TSA模型在T-MED数据集上的准确性和可解释性方面显著优于现有的最先进方法。

Conclusion: 该研究通过构建大规模多模态教师情感分析数据集和提出创新的非对称注意力模型，为准确分析教师情感提供了有效解决方案，有助于提升教育质量和教学效果评估。

Abstract: Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.

</details>


### [20] [Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent](https://arxiv.org/abs/2512.20586)
*Humza Nusrat,Luke Francisco,Bing Luo,Hassan Bagher-Ebadian,Joshua Kim,Karen Chin-Snyder,Salim Siddiqui,Mira Shah,Eric Mellon,Mohammad Ghassemi,Anthony Doemer,Benjamin Movsas,Kundan Thind*

Main category: cs.AI

TL;DR: 研究测试了链式思维推理是否能改善立体定向放射外科的自动计划质量，开发了SAGE智能体，发现推理模型在保持主要剂量指标的同时降低了耳蜗剂量，并产生了可审计的规划过程


<details>
  <summary>Details</summary>
Motivation: 立体定向放射外科需要精确的剂量塑造，但黑盒AI系统由于透明度问题在临床应用中受限。研究者希望探索链式思维推理是否能提高自动规划系统的透明度和性能。

Method: 开发了基于大语言模型的SAGE自动规划智能体，在41例脑转移瘤患者的回顾性队列中测试。比较了两种变体：非推理模型和推理模型，评估了剂量学参数和规划行为。

Result: 推理模型在主要终点（靶区覆盖率、最大剂量、适形指数、梯度指数）上与人类计划者相当，同时显著降低了耳蜗剂量。推理模型展示了前瞻性约束验证和权衡审议等系统性规划行为，而非推理模型几乎没有这些过程。

Conclusion: 链式思维推理不仅提高了自动规划系统的性能，还通过产生可审计的优化轨迹提供了透明度，为临床可接受的自动化规划开辟了道路。

Abstract: Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.

</details>


### [21] [LongVideoAgent: Multi-Agent Reasoning with Long Videos](https://arxiv.org/abs/2512.20618)
*Runtao Liu,Ziyi Liu,Jiaqi Tang,Yue Ma,Renjie Pi,Jipeng Zhang,Qifeng Chen*

Main category: cs.AI

TL;DR: 提出多智能体框架解决长视频问答问题，通过主LLM协调定位智能体和视觉智能体，结合强化学习训练实现高效多智能体协作


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理小时级长视频问答时，通常压缩内容为有损摘要或依赖有限工具集，导致时间定位能力弱且错过细粒度线索

Method: 采用多智能体框架：主LLM协调定位智能体（定位问题相关片段）和视觉智能体（提取针对性文本观察）；主智能体有步骤限制，通过强化学习训练鼓励简洁、正确、高效的多智能体协作

Result: 在提出的LongTVQA和LongTVQA+数据集上，多智能体系统显著优于强非智能体基线；实验显示强化学习进一步增强了训练智能体的推理和规划能力

Conclusion: 多智能体框架通过定位智能体帮助主智能体聚焦相关片段，用视觉细节补充字幕，产生可解释的轨迹，有效解决了长视频问答中的挑战

Abstract: Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.

</details>

<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 49]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: GYWI系统通过结合作者知识图谱和检索增强生成，为LLMs提供可控的学术背景和可追溯的灵感路径，显著提升科学想法生成的质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在科学想法生成方面存在局限性：生成的成果缺乏可控的学术背景和可追溯的灵感路径，需要一种能够提供结构化外部知识支持的系统。

Method: 1. 提出作者中心的知识图谱构建方法和灵感源采样算法构建外部知识库；2. 设计混合检索机制（RAG+GraphRAG）获取深度和广度知识；3. 采用基于强化学习的Prompt优化策略自动指导LLMs优化结果。

Result: 在arXiv（2018-2023）数据集上的实验表明，GYWI在GPT-4o、DeepSeek-V3、Qwen3-8B和Gemini 2.5等多个LLMs上，在新颖性、可靠性、相关性等多个指标上显著优于主流LLMs。

Conclusion: GYWI系统通过整合知识图谱和检索增强生成，有效解决了LLMs在科学想法生成中的背景控制和灵感追溯问题，为AI辅助科学研究提供了新的解决方案。

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [2] [FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation](https://arxiv.org/abs/2602.22273)
*Xiyuan Zhang,Huihang Wu,Jiayu Guo,Zhenlin Zhang,Yiwei Zhang,Liangyu Huo,Xiaoxiao Ma,Jiansong Wan,Xuewei Jiao,Yi Jing,Jian Xie*

Main category: cs.AI

TL;DR: FIRE是一个综合性金融基准测试，用于评估LLMs的理论金融知识和实际业务场景处理能力，包含理论考试题和3000个金融场景问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面评估LLMs在金融领域能力的基准测试，需要同时评估理论知识和实际应用价值，以了解LLMs在金融应用中的能力边界。

Method: 1) 理论评估：从广泛认可的金融资格考试中收集多样化考题；2) 实践评估：提出系统性评估矩阵，分类复杂金融领域，确保覆盖关键子领域和业务活动；3) 收集3000个金融场景问题，包括有参考答案的封闭式决策问题和按预定标准评估的开放式问题。

Result: 对包括轩辕4.0在内的最先进LLMs在FIRE基准上进行了全面评估，轩辕4.0作为强大的领域内基线模型表现良好，结果能够系统分析当前LLMs在金融应用中的能力边界。

Conclusion: FIRE基准测试为评估LLMs的金融能力提供了全面框架，公开了基准问题和评估代码以促进未来研究，有助于理解LLMs在金融领域的实际应用潜力。

Abstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.

</details>


### [3] [Multi-Level Causal Embeddings](https://arxiv.org/abs/2602.22287)
*Willem Schooltink,Fabio Massimo Zennaro*

Main category: cs.AI

TL;DR: 本文提出因果嵌入框架，将多个详细因果模型映射到更粗粒度模型的子系统中，作为抽象化的推广，并展示其在统计和因果边际问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有因果抽象化主要关注两个模型之间的关系，但实际应用中需要将多个详细模型映射到同一个粗粒度模型的子系统中，以支持数据集合并和模型集成。

Method: 定义因果嵌入作为抽象化的推广，提出广义一致性概念，建立多分辨率边际问题框架，将统计边际问题和因果边际问题统一处理。

Result: 因果嵌入框架能够处理多个不同表示模型的合并问题，为统计边际问题和因果边际问题提供统一解决方案，支持来自不同表示模型的数据集融合。

Conclusion: 因果嵌入框架扩展了因果抽象化概念，为解决多模型集成、数据集合并以及统计和因果边际问题提供了理论基础和实用工具。

Abstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.

</details>


### [4] [Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus](https://arxiv.org/abs/2602.22408)
*Caroline Ahn,Quan Do,Leah Bakst,Michael P. Pascale,Joseph T. McGuire,Michael E. Hasselmo,Chantal E. Stern*

Main category: cs.AI

TL;DR: 研究人员开发了CogARC测试集来研究人类抽象推理策略，通过两个实验让260名参与者解决75个视觉推理问题，记录了高时间分辨率的行为数据，发现人类在抽象推理中表现出高效但策略多样的特点。


<details>
  <summary>Details</summary>
Motivation: 研究人类在抽象推理中的认知策略灵活性，了解人类如何从稀疏示例中快速学习和应用规则，为人工智能的抽象推理能力提供认知科学基础。

Method: 开发CogARC（认知抽象推理语料库），从ARC中选取适合人类测试的子集；进行两个实验，共260名参与者解决75个抽象视觉推理问题；记录高时间分辨率的行为数据，包括示例查看、编辑序列和多尝试提交。

Result: 参与者整体表现良好（实验1准确率约90%，实验2约80%），但问题和参与者间差异大；难题引发更长思考时间和更多样策略；随着任务进行，响应更快但准确率略有下降；即使是错误答案也常高度收敛，问题解决轨迹在长度和平滑度上存在差异。

Conclusion: CogARC为研究人类抽象推理提供了丰富的行为环境，揭示了人类在不确定性下如何泛化、错误泛化和调整策略，为理解人类抽象推理的认知机制提供了重要见解。

Abstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.

</details>


### [5] [Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents](https://arxiv.org/abs/2602.22413)
*Jonas Karge*

Main category: cs.AI

TL;DR: 研究异构智能体通过自我可靠性学习选择性弃权投票的集体准确性，将经典孔多塞陪审团定理扩展到置信门控的序贯投票场景


<details>
  <summary>Details</summary>
Motivation: 经典孔多塞陪审团定理假设固定参与，但现实世界聚合通常允许智能体说"我不知道"。需要研究智能体学习自身可靠性并选择性弃权如何影响集体决策准确性

Method: 提出概率框架：智能体经历校准阶段更新对自身固定能力的信念，然后面对最终置信门决定投票或弃权。推导群体成功概率的非渐近下界

Result: 证明选择性参与将CJT的渐近保证推广到序贯置信门控设置。通过蒙特卡洛模拟验证边界，并讨论在AI安全中减少LLM集体决策幻觉的应用潜力

Conclusion: 选择性弃权机制能有效提升集体决策准确性，为AI安全中减少幻觉提供理论框架，将经典投票理论扩展到更现实的智能体参与场景

Abstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \textit{hallucinations} in collective LLM decision-making.

</details>


### [6] [How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?](https://arxiv.org/abs/2602.22441)
*Yingqian Cui,Zhenwei Dai,Bing He,Zhan Shi,Hui Liu,Rui Sun,Zhiji Liu,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 本文对潜在推理方法进行了全面分析，揭示了其内部机制中的两个关键问题：普遍存在的捷径行为，以及潜在表示虽然能编码多种可能性但并未忠实实现结构化搜索。


<details>
  <summary>Details</summary>
Motivation: 尽管已有许多研究关注提升潜在推理的性能，但其内部机制尚未得到充分研究。潜在推理作为一种新兴的推理范式，通过在潜在空间而非文本空间生成步骤来进行多步推理，但我们对潜在表示在这一过程中的角色和行为理解有限。

Method: 对具有不同监督水平的潜在推理方法进行全面分析，识别关键问题：1）观察普遍的捷径行为；2）检验潜在推理是否支持潜在空间中的广度优先搜索式探索；3）分析监督强度与潜在表示能力之间的权衡关系。

Result: 发现两个关键问题：1）潜在推理方法普遍存在捷径行为，能够在不依赖潜在推理的情况下获得高准确率；2）虽然潜在表示能够编码多种可能性，但推理过程并未忠实实现结构化搜索，而是表现出隐式剪枝和压缩。此外，揭示了监督强度与潜在表示能力之间的权衡：更强的监督能减轻捷径行为但限制了潜在表示维持多样假设的能力，而较弱的监督允许更丰富的潜在表示但增加了捷径行为。

Conclusion: 潜在推理方法存在内部机制上的局限性，包括普遍的捷径行为和未能实现真正的结构化搜索。监督强度在缓解捷径行为和保持潜在表示多样性之间存在权衡，这为未来改进潜在推理方法提供了重要启示。

Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.

</details>


### [7] [CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452)
*Chayan Banerjee*

Main category: cs.AI

TL;DR: 本文提出对比世界模型（CWM），使用对比学习训练动作可行性评分器，通过挖掘困难负样本来更好地区分物理上可行与不可行的动作。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用监督微调（SFT）训练动作评分器，但SFT独立处理每个候选动作，没有明确教导模型区分物理上正确和细微错误的动作，这成为具身智能体流程中的关键瓶颈。

Method: 提出对比世界模型（CWM），使用InfoNCE对比目标对大型语言模型进行微调，特别关注挖掘困难负样本（语义相似但物理不兼容的候选动作），在评分空间中推动有效动作远离无效动作。

Result: 在ScienceWorld基准测试中：1）内在可负担性评估显示，CWM在最小编辑负样本上的Precision@1比SFT提高6.76个百分点，AUC-ROC更高（0.929 vs 0.906）；2）实时过滤特性研究表明，在分布外压力条件下，CWM保持更好的安全边界（-2.39 vs -3.96）。

Conclusion: 对比训练比单独使用监督微调更能诱导出更忠实捕捉物理可行性的表示，证明了对比学习在动作可行性评分中的有效性。

Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>


### [8] [ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization](https://arxiv.org/abs/2602.22465)
*Joseph Tso,Preston Schmittou,Quan Huynh,Jibran Hutchins*

Main category: cs.AI

TL;DR: 论文介绍了ConstraintBench基准测试，用于评估大语言模型在完全指定的约束优化问题上直接生成正确解的能力，发现可行性而非最优性是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估LLMs能否将优化问题表述为求解器代码，但缺少对LLMs能否在没有求解器的情况下直接为完全指定的约束优化问题生成正确解的能力评估。

Method: 创建ConstraintBench基准测试，涵盖10个运筹学领域，包含200个任务，每个任务提供自然语言场景描述，要求模型返回结构化解，通过确定性验证器检查所有约束和求解器证明的最优解。

Result: 评估6个前沿模型发现：可行性是主要瓶颈，最佳模型仅达到65.0%约束满足率；可行解平均达到Gurobi最优目标的89-96%；没有模型在可行性和最优性（与求解器参考值相差0.1%以内）联合指标上超过30.5%；不同领域难度差异大，可行性从83.3%（生产组合）到0.8%（机组分配）。

Conclusion: LLMs在直接约束优化方面存在显著局限性，可行性是主要挑战，需要进一步研究改进模型对约束的理解和处理能力。ConstraintBench和相关评估基础设施将公开发布。

Abstract: Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.

</details>


### [9] [VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480)
*Varun Ursekar,Apaar Shanker,Veronica Chatrath,Yuan,Xue,Sam Denton*

Main category: cs.AI

TL;DR: VERO是一个用于评估编码智能体优化性能的系统框架，包含版本控制、奖励机制和观察记录，支持对智能体进行可重复的迭代改进评估。


<details>
  <summary>Details</summary>
Motivation: 编码智能体的一个重要应用是智能体优化：通过编辑-执行-评估循环对目标智能体进行迭代改进。然而，社区缺乏对这一任务的系统性理解。智能体优化与传统软件工程有根本区别：目标智能体将确定性代码与随机LLM完成交错进行，需要结构化地捕获中间推理和下游执行结果。

Method: 引入VERO（版本控制、奖励和观察）框架，提供：(1) 可重复的评估工具链，包含版本化智能体快照、预算控制评估和结构化执行轨迹；(2) 目标智能体和任务的基准测试套件，包含参考评估程序。

Result: 使用VERO进行实证研究，比较不同任务中的优化器配置，分析哪些修改能可靠地提高目标智能体性能。研究结果支持智能体优化作为编码智能体的核心能力。

Conclusion: VERO框架为智能体优化研究提供了系统支持，解决了编码智能体优化评估中的关键挑战，包括版本控制、结构化执行轨迹捕获和可重复评估。该框架已公开发布以促进相关研究。

Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

</details>


### [10] [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)
*Anastasija Mensikova,Donna M. Rizzo,Kathryn Hinkelman*

Main category: cs.AI

TL;DR: 该研究利用大语言模型对AI与生命周期评估交叉领域的研究进行了系统性综述，揭示了AI技术在LCA中应用的快速增长趋势，特别是LLM驱动方法的兴起，并提出了一个结合传统文献综述与LLM文本挖掘的动态分析框架。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在生命周期评估中的应用近年来加速发展，但对该交叉领域的全面综合研究仍然有限。本研究旨在填补这一空白，通过系统综述AI-LCA研究，识别当前趋势、新兴主题和未来方向。

Method: 结合大语言模型（LLMs）的文本挖掘方法与传统的文献综述技术，开发了一个动态有效的分析框架，能够捕捉该领域的高层研究趋势和细微的概念模式。

Result: 分析显示，随着LCA研究的扩展，AI技术的采用急剧增长，明显转向LLM驱动的方法，机器学习应用持续增加，AI方法与相应LCA阶段之间存在统计显著相关性。

Conclusion: LLM辅助方法学在支持大规模、可重复的跨领域综述方面具有潜力，同时评估了在AI技术快速发展背景下计算效率高的LCA路径。这项工作帮助LCA从业者将先进工具和及时见解融入环境评估，增强可持续性决策的严谨性和质量。

Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>


### [11] [Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents](https://arxiv.org/abs/2602.22523)
*Ryan Liu,Dilip Arumugam,Cedegao E. Zhang,Sean Escola,Xaq Pitkow,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: 该论文提出从认知模型和AI算法中寻找设计模块化语言智能体的蓝图，通过定义智能体模板来规范单个LLM的角色和功能组合方式，并分析了现有语言智能体文献中的模板设计。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）虽然能力不断增强，但许多复杂问题仍超出单个LLM的能力范围。如何将多个LLM作为组件组合成一个更强大的整体系统仍存在不确定性。作者认为可以从现有的认知模型和人工智能算法文献中找到设计这种模块化语言智能体的潜在蓝图。

Method: 论文形式化提出了"智能体模板"的概念，该模板规定了单个LLM的角色以及它们的功能应该如何组合。作者对现有文献中的各种语言智能体进行了调研，并重点分析了这些智能体直接从认知模型或AI算法中衍生出的底层模板设计。

Result: 通过分析发现，许多现有的语言智能体设计实际上都基于认知科学和AI算法中的经典模板。这些模板为构建有效、可解释的语言智能体提供了系统化的设计框架。

Conclusion: 受认知科学和AI启发的智能体模板是开发有效、可解释语言智能体的强大工具。通过借鉴这些已有的认知和算法模式，可以更好地设计和构建复杂的多LLM系统，解决超出单个LLM能力范围的复杂问题。

Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.

</details>


### [12] [Agentic AI for Intent-driven Optimization in Cell-free O-RAN](https://arxiv.org/abs/2602.22539)
*Mohammad Hossein Shokouhi,Vincent W. S. Wong*

Main category: cs.AI

TL;DR: 提出一个基于智能体的AI框架，用于无蜂窝O-RAN中的意图翻译和优化，通过多个LLM智能体协作处理复杂意图，实现节能和资源管理。


<details>
  <summary>Details</summary>
Motivation: 现有O-RAN中的智能体大多处理简单意图且独立工作，缺乏处理需要多个智能体协调的复杂意图的能力，需要开发能够协作处理复杂意图的智能体AI框架。

Method: 提出一个包含多个LLM智能体的框架：监督智能体翻译运营商意图；用户权重智能体从记忆模块检索经验确定用户优先级；O-RU管理智能体使用DRL算法确定活跃O-RU；监控智能体监测数据速率。采用参数高效微调方法实现同一LLM支持不同智能体。

Result: 在节能模式下，与三种基线方案相比，该框架将活跃O-RU数量减少41.93%；使用PEFT方法相比部署单独LLM智能体，内存使用减少92%。

Conclusion: 提出的智能体AI框架能够有效处理O-RAN中的复杂意图，实现显著的节能效果和资源优化，同时通过PEFT方法提高了系统的可扩展性。

Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.

</details>


### [13] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: CourtGuard是一个基于检索增强的多智能体框架，将安全评估重构为证据辩论，通过外部政策文档进行对抗性辩论，实现了无需微调的最先进安全性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全机制主要依赖静态微调分类器，存在适应僵化问题，无法在不进行昂贵重新训练的情况下强制执行新的治理规则。

Method: 引入CourtGuard框架，采用检索增强的多智能体架构，将安全评估重新构想为证据辩论，通过基于外部政策文档的对抗性辩论来评估内容安全性。

Result: 在7个安全基准测试中取得最先进性能，超越专用政策遵循基线；实现零样本适应性（在维基百科破坏检测任务中达到90%准确率）；自动数据整理和审计能力，创建了9个新颖的对抗攻击数据集。

Conclusion: 将安全逻辑与模型权重解耦为AI治理提供了稳健、可解释且适应性强的路径，能够满足当前和未来的监管要求。

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [14] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 论文发现示例引导在数学推理中的效果不稳定源于策略使用与策略可执行性之间的差距，提出了选择性策略检索框架来提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 基于示例的引导在推理时广泛用于改进数学推理，但其效果在不同问题和模型间极不稳定，即使引导正确且与问题相关。这种不稳定性源于策略使用与策略可执行性之间未被充分探索的差距。

Method: 通过对比分析人类编写和模型生成的解决方案，识别使用与可执行性之间的系统性差异。基于此诊断，提出选择性策略检索框架，通过经验性、多路径、源感知信号来选择性检索和组合策略，显式建模可执行性。

Result: 在多个数学推理基准测试中，SSR相比直接求解、上下文学习和单源引导，提供了可靠且一致的改进，在AIME25上准确率提升高达+13点，在Apex上提升+5点。

Conclusion: 策略使用与可执行性之间的差距是示例引导效果不稳定的关键原因，选择性策略检索框架能有效建模可执行性，显著提升数学推理模型的性能。

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [15] [Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach](https://arxiv.org/abs/2602.22585)
*Jodi M. Casabianca,Maggie Beiting-Parrish*

Main category: cs.AI

TL;DR: 该论文提出将心理测量学中的评分者模型整合到AI评估流程中，通过项目反应理论模型（特别是多面Rasch模型）分离真实输出质量与评分者行为偏差，提高人类评估的可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 人类评估在AI模型训练和评估中起核心作用，但这些数据很少被视为存在系统误差的测量。当前AI评估中的人类评分数据缺乏对评分者偏差的系统处理，导致结论的可靠性和有效性存在问题。

Method: 将心理测量学评分者模型整合到AI评估流程中，使用项目反应理论模型（特别是多面Rasch模型）来建模评分者效应。具体分析了两种常见的评分者效应：严重性（severity）和中心性（centrality），并通过调整评分者严重性来校正评分估计。

Result: 使用OpenAI摘要数据集作为实证案例，展示了调整评分者严重性后能够产生校正后的摘要质量估计，并提供对评分者表现的诊断性洞察。校正后的评分比原始评分更可靠。

Conclusion: 将心理测量建模整合到人机交互评估中，能够更原则化、透明地使用人类数据，使开发者能够基于调整后的分数而非原始易错评分做出决策。这为AI开发和评估提供了更稳健、可解释且与构念对齐的实践路径。

Abstract: Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgments. The paper reviews common rater effects, severity and centrality, that distort observed ratings, and demonstrates how item response theory rater models, particularly the multi-faceted Rasch model, can separate true output quality from rater behavior. Using the OpenAI summarization dataset as an empirical example, we show how adjusting for rater severity produces corrected estimates of summary quality and provides diagnostic insight into rater performance. Incorporating psychometric modeling into human-in-the-loop evaluation offers more principled and transparent use of human data, enabling developers to make decisions based on adjusted scores rather than raw, error-prone ratings. This perspective highlights a path toward more robust, interpretable, and construct-aligned practices for AI development and evaluation.

</details>


### [16] [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: SideQuest：一种利用大型推理模型自身进行KV缓存压缩的新方法，通过并行执行压缩任务来减少长时智能体任务中的内存使用


<details>
  <summary>Details</summary>
Motivation: 长时运行的智能体任务（如深度研究）需要在多个网页和文档之间进行多跳推理，导致LLM上下文被外部检索的token主导，内存使用快速增长并限制解码性能。现有的KV缓存压缩启发式方法无法有效支持多步推理模型。

Method: 提出SideQuest方法，利用大型推理模型自身通过推理其上下文中token的有用性来执行KV缓存压缩。为避免压缩管理过程污染模型内存，将KV缓存压缩作为与主要推理任务并行执行的辅助任务。

Result: 仅使用215个样本训练模型，SideQuest在智能体任务中将峰值token使用量减少高达65%，准确率下降最小，优于基于启发式的KV缓存压缩技术。

Conclusion: SideQuest通过让大型推理模型自身管理其KV缓存，有效解决了长时智能体任务中的内存扩展问题，为多步推理模型提供了高效的压缩解决方案。

Abstract: Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.

</details>


### [17] [AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising](https://arxiv.org/abs/2602.22650)
*Xinxin Yang,Yangyang Tang,Yikun Zhou,Yaolei Liu,Yun Li,Bo Yang*

Main category: cs.AI

TL;DR: AHBid是一个用于多渠道在线广告自动出价的分层框架，通过生成式规划和实时控制相结合，在动态广告环境中优化投资回报率。


<details>
  <summary>Details</summary>
Motivation: 在线广告环境复杂且动态变化，特别是在多渠道场景中，需要在具有不同行为模式的渠道间有效分配预算和约束。现有方法要么缺乏灵活性（基于优化的方法），要么难以捕捉历史依赖关系（强化学习方法），无法适应动态市场条件。

Method: 提出AHBid框架，整合生成式规划和实时控制。高层使用基于扩散模型的生成式规划器动态分配预算和约束，捕捉历史上下文和时间模式；引入约束执行机制确保符合指定约束；采用轨迹细化机制利用历史数据增强环境适应性；底层使用基于控制的出价算法，结合历史知识和实时信息。

Result: 在大规模离线数据集和在线A/B测试中，AHBid相比现有基线方法实现了13.57%的整体回报提升。

Conclusion: AHBid框架通过整合生成式规划和实时控制，有效解决了多渠道广告自动出价中的动态适应性问题，显著提升了投资回报率。

Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.

</details>


### [18] [Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions](https://arxiv.org/abs/2602.22680)
*Yue Xu,Qian Chen,Zizhan Ma,Dongrui Liu,Wenxuan Wang,Xiting Wang,Li Xiong,Wenjie Wang*

Main category: cs.AI

TL;DR: 这篇综述论文系统回顾了个性化LLM智能体的研究现状，围绕四个核心组件（用户画像建模、记忆、规划、行动执行）构建了结构化框架，分析了用户信号的表示、传播和利用方式，并探讨了评估方法、应用场景和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长期交互中需要适应不同用户并保持行为连续性，个性化成为提升智能体效能的关键。当前研究缺乏对个性化LLM智能体的系统性梳理，需要建立统一框架来理解用户信号如何在智能体决策流程中传播和利用。

Method: 采用能力导向的综述方法，将文献组织为四个相互依赖的核心组件：1) 用户画像建模（如何表示用户特征），2) 记忆系统（如何存储和检索用户相关信息），3) 规划模块（如何基于用户信息制定策略），4) 行动执行（如何根据个性化规划与环境交互）。通过这一分类法分析代表性方法，特别关注用户信号在组件间的传播机制和设计权衡。

Result: 建立了理解个性化LLM智能体的结构化框架，识别了跨组件交互模式和常见设计权衡。总结了针对个性化智能体的评估指标和基准测试，梳理了从通用助手到专业领域的应用场景，为从原型个性化到可扩展实际系统的过渡提供了路线图。

Conclusion: 个性化LLM智能体研究需要系统化框架来指导设计和开发。通过整合用户画像、记忆、规划和执行四个组件，可以构建更用户对齐、自适应、鲁棒且可部署的智能体系统。未来研究应关注可扩展性、隐私保护、评估标准化等方向，推动个性化智能体从研究原型向实际应用转化。

Abstract: Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.

</details>


### [19] [RLHFless: Serverless Computing for Efficient RLHF](https://arxiv.org/abs/2602.22718)
*Rui Wei,Hanfei Yu,Shubham Jain,Yogarajan Sivakumar,Devesh Tiwari,Jian Li,Seung-Jong Park,Hao Wang*

Main category: cs.AI

TL;DR: RLHFless：首个基于无服务器计算环境的同步RLHF可扩展训练框架，通过动态资源适配、前缀预计算和成本感知的actor扩展策略，实现1.35倍加速和44.8%成本降低


<details>
  <summary>Details</summary>
Motivation: 传统RLHF框架依赖服务器基础设施，难以应对细粒度资源变化，导致同步训练中组件间或组件内的空闲时间造成开销和资源浪费。随着模型规模扩大和资源消耗增加，RLHF的训练效率面临更大挑战。

Method: 1. 基于无服务器计算环境构建同步RLHF训练框架；2. 适应RLHF流程中的动态资源需求；3. 预计算共享前缀避免重复计算；4. 采用考虑响应长度变化的成本感知actor扩展策略，寻找成本与速度的最佳平衡点；5. 高效分配工作负载以减少函数内不平衡和空闲时间。

Result: 在物理测试平台和大规模模拟集群上的实验表明，RLHFless相比最先进的基线方法，实现了最高1.35倍的加速和44.8%的成本降低。

Conclusion: RLHFless是首个基于无服务器计算环境的同步RLHF可扩展训练框架，通过动态资源适配、计算优化和智能调度策略，有效解决了传统RLHF框架的资源浪费问题，显著提升了训练效率和成本效益。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.

</details>


### [20] [Generative Data Transformation: From Mixed to Unified Data](https://arxiv.org/abs/2602.22743)
*Jiaqing Zhang,Mingjia Yin,Hao Wang,Yuxin Tian,Yuyang Ye,Yawen Li,Wei Guo,Yong Liu,Enhong Chen*

Main category: cs.AI

TL;DR: Taesar是一个数据中心的跨域序列推荐框架，通过对比解码机制将跨域上下文编码到目标域序列中，解决了传统模型中心方法在跨域推荐中面临的负迁移和泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统跨域推荐采用模型中心范式，依赖复杂的定制化架构，难以捕捉跨域的微妙非结构性序列依赖，导致泛化能力差且计算资源需求高。同时，领域间的固有差异会导致混合数据质量下降，产生负迁移问题。

Method: 提出Taesar（目标对齐序列再生）框架，采用数据中心的对比解码机制，自适应地将跨域上下文编码到目标域序列中。该方法使标准模型能够学习复杂的依赖关系，而无需复杂的融合架构。

Result: 实验表明Taesar优于模型中心解决方案，并能泛化到各种序列模型。通过生成丰富的数据集，Taesar有效结合了数据中心和模型中心范式的优势。

Conclusion: Taesar框架通过数据中心的序列再生方法，解决了跨域推荐中的负迁移和泛化问题，为跨域推荐提供了更有效且资源友好的解决方案。

Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.

</details>


### [21] [Decomposing Physician Disagreement in HealthBench](https://arxiv.org/abs/2602.22758)
*Satya Borgohain,Roy Mariathas*

Main category: cs.AI

TL;DR: 研究分析了HealthBench医疗AI评估数据集中医生意见分歧的来源，发现81.8%的分歧是病例层面的残余变异，无法通过现有特征解释，而可减少的不确定性（如信息缺失）会显著增加分歧概率。


<details>
  <summary>Details</summary>
Motivation: 理解医疗AI评估中医生意见分歧的来源，识别哪些因素可以解释分歧，哪些是结构性限制，从而为改进评估设计提供依据。

Method: 对HealthBench医疗AI评估数据集进行分解分析，考察评分标准、医生身份、病例元数据、医学专业、表面特征、嵌入表示等对分歧的影响，并分析完成质量与分歧的关系，以及可减少与不可减少不确定性的作用。

Result: 评分标准仅解释3.6-6.9%的分歧变异，医生身份仅解释2.4%，81.8%的病例层面残余变异无法被现有特征解释。分歧与完成质量呈倒U型关系，可减少的不确定性使分歧几率增加2.55倍，而不可减少的不确定性无显著影响。

Conclusion: 医疗AI评估中的一致率上限主要是结构性的，但可减少与不可减少不确定性的分离表明，通过改进评估设计（如填补信息缺口）可以在非固有临床模糊的情况下降低分歧，这为评估改进提供了可操作的路径。

Abstract: We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity accounts for just 2.4%. The dominant 81.8% case-level residual is not reduced by HealthBench's metadata labels (z = -0.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U with completion quality (AUC = 0.689), confirming physicians agree on clearly good or bad outputs but split on borderline cases. Physician-validated uncertainty categories reveal that reducible uncertainty (missing context, ambiguous phrasing) more than doubles disagreement odds (OR = 2.55, p < 10^(-24)), while irreducible uncertainty (genuine medical ambiguity) has no effect (OR = 1.01, p = 0.90), though even the former explains only ~3% of total variance. The agreement ceiling in medical AI evaluation is thus largely structural, but the reducible/irreducible dissociation suggests that closing information gaps in evaluation scenarios could lower disagreement where inherent clinical ambiguity does not, pointing toward actionable evaluation design improvements.

</details>


### [22] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: AMA-Bench是一个用于评估LLM智能体长时记忆能力的基准测试，包含真实世界和合成的智能体轨迹数据，研究发现现有记忆系统表现不佳，提出了基于因果图和工具增强检索的AMA-Agent解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前智能体记忆评估主要关注对话为中心的人机交互，而实际应用中智能体记忆包含连续的机器生成的环境交互流，现有评估标准与实际应用存在显著差距。

Method: 提出AMA-Bench基准测试，包含真实世界智能体轨迹和专家标注的QA，以及可扩展到任意长度的合成智能体轨迹和基于规则的QA；同时提出AMA-Agent记忆系统，采用因果图和工具增强检索。

Result: 现有记忆系统在AMA-Bench上表现不佳，主要因为缺乏因果关系和目标信息，以及基于相似性检索的损失性限制；AMA-Agent在AMA-Bench上达到57.22%的平均准确率，比最强基线提升11.16%。

Conclusion: AMA-Bench填补了智能体记忆评估的空白，AMA-Agent通过因果图和工具增强检索有效解决了现有记忆系统的局限性，为智能体长时记忆能力提供了更好的解决方案。

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [23] [ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making](https://arxiv.org/abs/2602.22771)
*Yusuke Watanabe,Yohei Kobashi,Takeshi Kojima,Yusuke Iwasawa,Yasushi Okuno,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本文开发了ClinDet-Bench基准测试，用于评估大语言模型在临床不完全信息场景下识别可确定性（determinability）的能力，发现现有LLMs在这方面存在缺陷，既会过早判断也会过度弃权。


<details>
  <summary>Details</summary>
Motivation: 临床决策常常需要在信息不完全的情况下进行，临床专家需要判断可用信息是否足以做出判断，因为过早结论和不必要的弃权都会危及患者安全。现有基准测试不足以评估LLMs在临床环境中的安全性。

Method: 基于临床评分系统开发了ClinDet-Bench基准测试，将不完全信息场景分解为可确定和不可确定的条件。识别可确定性需要考虑所有关于缺失信息的假设（包括不太可能的假设），并验证结论是否在所有假设下都成立。

Result: 研究发现最近的LLMs无法在不完全信息下识别可确定性，既会产生过早判断，也会过度弃权，尽管它们能够正确解释底层评分知识并在完全信息下表现良好。

Conclusion: 现有基准测试不足以评估LLMs在临床环境中的安全性。ClinDet-Bench提供了一个评估可确定性识别的框架，引导适当的弃权，在医学和其他高风险领域具有潜在应用价值，并且已公开可用。

Abstract: Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs), we developed ClinDet-Bench, a benchmark based on clinical scoring systems that decomposes incomplete-information scenarios into determinable and undeterminable conditions. Identifying determinability requires considering all hypotheses about missing information, including unlikely ones, and verifying whether the conclusion holds across them. We find that recent LLMs fail to identify determinability under incomplete information, producing both premature judgments and excessive abstention, despite correctly explaining the underlying scoring knowledge and performing well under complete information. These findings suggest that existing benchmarks are insufficient to evaluate the safety of LLMs in clinical settings. ClinDet-Bench provides a framework for evaluating determinability recognition, leading to appropriate abstention, with potential applicability to medicine and other high-stakes domains, and is publicly available.

</details>


### [24] [MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks](https://arxiv.org/abs/2602.22808)
*Shiqian Su,Sen Xing,Xuan Dong,Muyan Zhong,Bin Wang,Xizhou Zhu,Yuntao Chen,Wenhai Wang,Yue Deng,Pengxiang Zhu,Ziyuan Liu,Tiantong Li,Jiaheng Yu,Zhe Chen,Lidong Bing,Jifeng Dai*

Main category: cs.AI

TL;DR: MiroFlow是一个高性能开源智能体框架，通过智能体图灵活编排、深度推理模式和鲁棒工作流执行，在多个基准测试中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型取得显著进展，但独立LLM在处理需要与外部工具和环境交互的复杂现实任务时能力开始停滞。现有智能体框架存在工作流程简单、性能不稳定、基准支持有限、依赖昂贵商业API等问题。

Method: 提出MiroFlow框架，包含三个核心组件：1) 智能体图实现灵活编排；2) 可选深度推理模式提升性能；3) 鲁棒工作流执行确保稳定性和可复现性。

Result: 在GAIA、BrowseComp-EN/ZH、HLE、xBench-DeepSearch和FutureX等多个智能体基准测试中，MiroFlow一致实现了最先进的性能表现。

Conclusion: MiroFlow作为一个开源框架，有望成为深度研究社区易于访问、可复现和可比较的基准系统，推动智能体研究发展。

Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.

</details>


### [25] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: FlexMS是一个用于质谱预测的基准框架，支持动态构建多种模型架构并评估其性能，为分子结构质谱预测提供标准化评估平台。


<details>
  <summary>Details</summary>
Motivation: 质谱技术在药物发现和材料科学中至关重要，但实验谱图缺乏阻碍了分子识别，需要建立计算预测方法。深度学习模型在预测分子结构质谱方面有潜力，但方法异质性和缺乏明确基准使得整体评估具有挑战性。

Method: 创建FlexMS基准框架，支持动态构建多种模型架构组合，在预处理公共数据集上使用不同指标评估性能。分析影响性能的因素，包括数据集结构多样性、超参数、预训练效果、元数据消融设置和跨域迁移学习。

Result: FlexMS框架提供了实用的模型选择指导，通过检索基准模拟实际识别场景，基于预测谱图对潜在匹配进行评分。

Conclusion: FlexMS基准框架解决了质谱预测领域缺乏标准化评估的问题，为研究人员提供了灵活的工具来构建和评估不同模型架构，推动了分子结构质谱预测领域的发展。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [26] [The AI Research Assistant: Promise, Peril, and a Proof of Concept](https://arxiv.org/abs/2602.22842)
*Tan Bui-Thanh*

Main category: cs.AI

TL;DR: AI在数学研究中能真正促进创造性发现，但需要严格的人类验证和监督。通过Hermite求积规则误差表示和界限的案例研究，展示了人机协作如何超越纯人工工作，同时揭示了AI的优势和局限。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能是否真正能促进创造性数学研究，还是仅仅自动化常规计算并引入错误风险。通过实证案例研究人机协作在数学发现中的实际效果。

Method: 使用多个AI助手进行系统性人机协作研究，扩展Hermite求积规则的结果。AI负责代数操作、系统化证明探索、文献综合和LaTeX准备，人类负责严格验证、数学直觉和战略方向。

Result: 成功发现并证明了关于Hermite求积规则误差表示和界限的多个新定理，超越了纯人工工作的成果。揭示了AI在代数操作、系统探索等方面的卓越能力，但也发现每一步都需要人类严格验证。

Conclusion: 在适当的怀疑态度和验证协议下，AI工具能显著加速数学发现，但需要谨慎的人类监督和深厚的领域专业知识。研究展示了成功的人机协作模式，并识别了研究人员必须预期的失败模式。

Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.

</details>


### [27] [OmniGAIA: Towards Native Omni-Modal AI Agents](https://arxiv.org/abs/2602.22897)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Shijian Wang,Guanting Dong,Jiajie Jin,Hao Wang,Yinuo Wang,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: OmniGAIA是一个全面的多模态基准测试，用于评估跨视频、音频和图像模态的智能体在深度推理和多轮工具执行任务上的表现。同时提出了OmniAtlas，一个基于工具集成推理范式的原生多模态基础智能体。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要局限于双模态交互（如视觉-语言），缺乏统一认知能力来满足通用AI助手的需求。人类智能天然结合了全模态感知与复杂推理和工具使用，因此需要开发能够处理跨模态推理和外部工具集成的智能系统。

Method: 1. 提出OmniGAIA基准测试：采用新颖的全模态事件图方法构建，从真实世界数据合成需要跨模态推理和外部工具集成的复杂多跳查询。2. 提出OmniAtlas智能体：基于工具集成推理范式的原生全模态基础智能体，具有主动全模态感知能力。采用后见指导树探索策略合成训练轨迹，并使用OmniDPO进行细粒度错误校正。

Result: OmniAtlas有效增强了现有开源模型的工具使用能力，这项工作标志着向下一代原生全模态AI助手迈出了一步，能够应对现实世界场景。

Conclusion: 该研究通过OmniGAIA基准测试和OmniAtlas智能体的开发，填补了当前多模态AI系统在全模态感知、深度推理和工具集成方面的空白，为构建能够处理复杂现实世界任务的全模态AI助手奠定了基础。

Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.

</details>


### [28] [General Agent Evaluation](https://arxiv.org/abs/2602.22953)
*Elron Bandel,Asaf Yehudai,Lilach Eden,Yehoshua Sagron,Yotam Perlitz,Elad Venezian,Natalia Razinkov,Natan Ergas,Shlomit Shachor Ifergan,Segev Shlomov,Michal Jacovi,Leshem Choshen,Liat Ein-Dor,Yoav Katz,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 本文提出首个通用智能体评估框架，通过统一协议和Exgentic框架评估通用智能体在陌生环境中的表现，创建了首个开放通用智能体排行榜


<details>
  <summary>Details</summary>
Motivation: 当前智能体大多是专用系统，缺乏对通用智能体（能在陌生环境中执行任务而无需领域特定工程）性能的系统性评估。现有基准测试假设领域特定集成，无法公平评估通用智能体。

Method: 提出通用智能体评估的概念原则、统一协议（实现智能体与基准测试集成）和Exgentic实践框架。在六个环境中对五个主流智能体实现进行基准测试，创建首个开放通用智能体排行榜。

Result: 实验表明通用智能体能够跨不同环境泛化，在没有任何环境特定调优的情况下，性能与领域特定智能体相当。

Conclusion: 将通用智能体评估确立为一流研究目标，发布评估协议、框架和排行榜，为通用智能体系统性研究奠定基础。

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

</details>


### [29] [SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy](https://arxiv.org/abs/2602.22971)
*Peiyao Xiao,Xiaogang Li,Chengliang Xu,Jiayi Wang,Ben Wang,Zichao Chen,Zeyu Wang,Kejun Yu,Yueqian Chen,Xulin Liu,Wende Xiao,Bing Zhao,Hu Wei*

Main category: cs.AI

TL;DR: SPM-Bench是一个针对扫描探针显微镜的博士级多模态基准测试，通过自动化数据合成管道从arXiv和期刊论文中提取高质量图像-文本对，使用混合云-本地架构减少token消耗，并引入SIP-F1评分来评估LLM性能并量化模型"个性"。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在专业科学领域存在数据污染、复杂度不足和人工成本过高的问题，需要开发专门针对扫描探针显微镜的高质量、低成本的评估基准。

Method: 1) 使用Anchor-Gated Sieve技术从2023-2025年的arXiv和期刊论文中提取高质量图像-文本对；2) 采用混合云-本地架构，VLM只返回空间坐标供本地高保真裁剪，极大节省token；3) 引入Strict Imperfection Penalty F1评分来评估模型性能并量化模型个性。

Result: 建立了SPM-Bench基准测试，能够准确评估LLM在复杂物理场景中的推理能力，首次量化了模型的四种个性类型（保守型、激进型、赌徒型、智慧型），并揭示了当前AI在复杂物理场景中的真实推理边界。

Conclusion: SPM-Bench为自动化科学数据合成提供了一个可推广的范式，能够有效评估LLM在专业科学领域的推理能力，并为理解模型行为提供了新的量化框架。

Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.

</details>


### [30] [Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots](https://arxiv.org/abs/2602.22973)
*Dimitrios P. Panagoulias,Evangelia-Aikaterini Tsichrintzi,Georgios Savvidis,Evridiki Tsoureli-Nikita*

Main category: cs.AI

TL;DR: 该研究提出了一个诊断对齐框架，用于分析临床AI系统中AI生成报告与医生验证结果之间的差异，通过结构化信号量化修正动态，结果显示传统二元词汇评估显著低估了临床意义对齐。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的临床AI中，人机协同验证至关重要，但AI初步推断与专家修正之间的过渡很少被作为结构化信号进行分析。需要一种能够系统比较AI生成报告与医生验证结果的方法，以更准确地评估临床AI系统的对齐程度。

Method: 引入诊断对齐框架，将AI生成的图像报告保存为不可变的推断状态，并与医生验证结果进行系统比较。推断流程集成了视觉大语言模型、基于BERT的医学实体提取和序列语言模型推断(SLMI)步骤，在专家评审前强制执行领域一致的细化。在21个皮肤病案例上使用四级一致性框架进行评估：精确主要匹配率(PMR)、语义相似性调整率(AMR)、跨类别对齐和综合一致性率(CCR)。

Result: 精确一致性达到71.4%，在语义相似性下保持不变(t=0.60)。结构化跨类别和差异重叠分析产生100%综合一致性(95% CI: [83.9%, 100%])。没有案例显示完全诊断分歧。结果表明二元词汇评估显著低估了临床意义对齐。

Conclusion: 将专家验证建模为结构化转换，能够实现信号感知的修正动态量化，支持可追溯、人机对齐的图像临床决策支持系统评估。该方法提供了比传统二元评估更全面的临床对齐度量。

Abstract: Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.

</details>


### [31] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: 提出RepSPD模型，通过黎曼流形上的交叉注意力机制和全局双向对齐策略，改进基于对称正定矩阵的脑电信号解码方法，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 当前基于对称正定矩阵的脑电分析方法主要关注统计聚合，忽视了频率特异性同步和脑区局部拓扑结构，需要更精细的几何深度学习模型

Method: 提出RepSPD模型：1）在黎曼流形上实现交叉注意力机制，用图导出的功能连接特征调制SPD的几何属性；2）引入全局双向对齐策略重塑切空间嵌入，减轻曲率引起的几何失真

Result: 大量实验表明，该框架显著优于现有脑电表示方法，展现出优越的鲁棒性和泛化能力

Conclusion: RepSPD通过结合几何深度学习和功能连接特征，有效提升了脑电信号解码的准确性和几何一致性，为神经科学和临床应用提供了更好的工具

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [32] [Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search](https://arxiv.org/abs/2602.22983)
*Xun Huang,Simeng Qin,Xiaoshuang Jia,Ranjie Duan,Huanqian Yan,Zhitao Zeng,Fei Yang,Yang Liu,Xiaojun Jia*

Main category: cs.AI

TL;DR: 该论文提出CC-BOS框架，利用文言文生成对抗性提示进行黑盒越狱攻击，通过果蝇优化算法在八个策略维度上迭代优化，显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全风险日益受到关注，现有研究表明LLMs对越狱攻击高度敏感，且攻击效果在不同语言环境中存在差异。文言文因其简洁性和模糊性可能部分绕过现有安全约束，暴露LLMs的显著漏洞。

Method: 提出CC-BOS框架，基于多维果蝇优化算法自动生成文言文对抗性提示。将提示编码为八个策略维度（角色、行为、机制、隐喻、表达、知识、触发模式和上下文），通过嗅觉搜索、视觉搜索和柯西变异进行迭代优化。同时设计了文言文到英文的翻译模块以增强可读性和评估准确性。

Result: 大量实验证明CC-BOS框架的有效性，在越狱攻击效果上持续优于现有的最先进攻击方法。

Conclusion: 文言文在越狱攻击中具有独特优势，能够有效绕过LLMs的安全约束。CC-BOS框架通过自动化的文言文对抗提示生成，显著提升了黑盒越狱攻击的效率和效果，揭示了LLMs在文言文处理方面的安全漏洞。

Abstract: As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.

</details>


### [33] [Learning-based Multi-agent Race Strategies in Formula 1](https://arxiv.org/abs/2602.23056)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Christopher H. Onder*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化学习的多智能体F1赛车策略优化方法，通过交互模块和自博弈训练生成竞争性策略，能够根据对手行为调整进站时机、轮胎选择和能量分配。


<details>
  <summary>Details</summary>
Motivation: F1比赛中，车队需要根据不断变化的比赛条件和竞争对手的行动来调整比赛策略。传统策略制定方法难以实时应对复杂的多智能体交互环境，因此需要一种能够学习适应对手行为的自动化策略优化方法。

Method: 基于预训练的单智能体策略，引入交互模块来考虑竞争对手的行为，结合自博弈训练方案生成竞争性策略。智能体根据相对性能进行排名，学习平衡能量管理、轮胎磨损、空气动力学交互和进站决策。

Result: 结果表明，智能体能够根据对手行为自适应地调整进站时机、轮胎选择和能量分配，实现稳健且一致的比赛表现。该框架仅依赖实际比赛中可用的信息，可用于支持比赛策略师在赛前和赛中的决策。

Conclusion: 提出的强化学习方法能够有效优化多智能体F1比赛策略，通过交互模块和自博弈训练使智能体学会适应竞争对手行为，为实际比赛策略制定提供了可行的自动化支持框架。

Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.

</details>


### [34] [Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design](https://arxiv.org/abs/2602.23092)
*Zhuoliang Xie,Fei Liu,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TL;DR: 本文提出AILS-AHD方法，利用大语言模型动态生成和优化破坏启发式，在带容量约束的车辆路径问题上取得优越性能，在CVRPLib大规模基准测试中为8/10实例创造了新的最佳已知解。


<details>
  <summary>Details</summary>
Motivation: 带容量约束的车辆路径问题(CVRP)作为组合优化的基础挑战，在车辆容量约束下优化车队运营。虽然运筹学中已有广泛研究，但CVRP的NP难特性仍带来显著计算挑战，尤其对于大规模实例。

Method: 提出AILS-AHD方法，将进化搜索框架与大语言模型集成，在自适应迭代局部搜索方法中动态生成和优化破坏启发式。同时引入基于LLM的加速机制来提升计算效率。

Result: 与最先进求解器（包括AILS-II和HGS）的综合实验评估表明，AILS-AHD在中等和大规模实例上均表现出优越性能。在CVRPLib大规模基准测试的10个实例中，为8个实例建立了新的最佳已知解。

Conclusion: AILS-AHD展示了LLM驱动的启发式设计在推进车辆路径优化领域的潜力，通过大语言模型动态生成和优化启发式，显著提升了CVRP问题的求解性能。

Abstract: The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.

</details>


### [35] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: MALLET是一个基于多智能体LLM的情感净化系统，通过四个智能体分析、调整、监控和指导，将新闻文章中的情感刺激降低19.3%，同时保持语义完整性。


<details>
  <summary>Details</summary>
Motivation: 在注意力经济中，煽情内容让消费者暴露在过度情感刺激下，阻碍冷静决策。需要一种既能减少情感刺激又不限制原始文本访问的系统。

Method: 提出MALLET多智能体情感净化系统，包含四个智能体：情感分析智能体（使用6情感BERT分类器量化刺激强度）、情感调整智能体（用LLM将文本重写为BALANCED和COOL两种模式）、平衡监控智能体（聚合每周信息消费模式生成个性化建议）、个人指导智能体（根据消费者敏感度推荐呈现模式）。

Result: 在800篇AG News文章上的实验显示：刺激分数显著降低（最高19.3%），情感平衡改善，语义保持良好。刺激减少与语义保持之间的相关性接近零，表明两者可独立控制。类别分析显示体育、商业、科技类刺激大幅减少（17.8-33.8%），而世界类效果有限（事实本身具有高刺激性）。

Conclusion: MALLET系统为支持消费者冷静接收信息提供了框架，无需限制对原始文本的访问，实现了情感刺激减少与语义保持的独立控制。

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [36] [The Trinity of Consistency as a Defining Principle for General World Models](https://arxiv.org/abs/2602.23152)
*Jingxuan Wei,Siyuan Li,Yuhang Xu,Zheng Sun,Junjie Jiang,Hexuan Jin,Caijun Jia,Honghao He,Xinglong Xu,Xi bai,Chang Yu,Yumou Liu,Junnan Zhu,Xuanhe Zhou,Jintao Chen,Xiaobin Hu,Shancheng Pang,Bihui Yu,Ran He,Zhen Lei,Stan Z. Li,Conghui He,Shuicheng Yan,Cheng Tan*

Main category: cs.AI

TL;DR: 该论文提出了构建通用世界模型的理论框架——"三位一体一致性"（模态、空间、时间一致性），并引入CoW-Bench基准来评估视频生成模型和统一多模态模型。


<details>
  <summary>Details</summary>
Motivation: 当前虽然存在Sora等视频生成模型和统一多模态模型（UMM）的进展，但缺乏定义通用世界模型必备属性的原则性理论框架。需要建立系统化的理论基础来指导世界模型的发展。

Method: 1. 提出"三位一体一致性"理论框架：模态一致性（语义接口）、空间一致性（几何基础）、时间一致性（因果引擎）
2. 系统回顾多模态学习演进历程
3. 引入CoW-Bench基准，专注于多帧推理和生成场景，统一评估视频生成模型和UMMs

Result: 建立了指导通用世界模型发展的原则性理论框架，揭示了当前系统的局限性，明确了未来进展的架构要求。通过CoW-Bench提供了统一的评估协议。

Conclusion: 该论文为通用世界模型的发展提供了系统的理论框架和评估基准，通过三位一体一致性的视角，为未来世界模型的研究指明了方向，填补了当前缺乏原则性理论指导的空白。

Abstract: The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.

</details>


### [37] [PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering](https://arxiv.org/abs/2602.23161)
*Junkai Lu,Peng Chen,Xingjian Wu,Yang Shu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.AI

TL;DR: PATRA模型通过模式感知机制提取时间序列的趋势和季节性模式，并设计任务感知平衡奖励来协调不同难度任务的学习，在时间序列问答任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时间序列推理方法存在两个局限：1) 将时间序列仅视为文本或图像，无法捕捉回答特定问题所需的趋势和季节性模式；2) 在混合简单和复杂任务训练时，简单目标主导学习过程，阻碍深度推理能力发展。

Method: 提出PATRA模型，包含：1) 模式感知机制，从时间序列中提取趋势和季节性模式以实现深度对齐；2) 任务感知平衡奖励，协调不同难度任务的学习，激励生成连贯的思维链。

Result: 大量实验表明，PATRA在多样化时间序列问答任务中优于强基线模型，展示了卓越的跨模态理解和推理能力。

Conclusion: PATRA通过模式感知对齐和平衡推理机制，有效解决了现有方法在时间序列推理中的局限性，提升了深度推理能力。

Abstract: Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.

</details>


### [38] [A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring](https://arxiv.org/abs/2602.23163)
*Usman Anwar,Julianna Piskorz,David D. Baek,David Africa,Jim Weatherall,Max Tegmark,Christian Schroeder de Witt,Mihaela van der Schaar,David Krueger*

Main category: cs.AI

TL;DR: 论文提出了一种检测和量化大语言模型中隐写推理的新方法，通过决策理论视角和广义V信息框架来测量隐写差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型开始展现隐写能力，这可能让未对齐的模型逃避监督机制。然而缺乏检测和量化这种行为的原则性方法，传统隐写检测方法需要已知非隐写信号的参考分布，这在LLM隐写推理中不可行。

Method: 提出决策理论的隐写观，核心见解是隐写在能够解码和不能解码隐藏内容的智能体之间创造了信息不对称。引入广义V信息作为测量输入中可用信息的功利主义框架，并定义"隐写差距"来量化隐写。

Result: 经验验证了该形式化方法，表明它可以用于检测、量化和缓解大语言模型中的隐写推理。

Conclusion: 提出的决策理论视角和隐写差距测量为检测和量化LLM中的隐写推理提供了可行的替代方法，解决了传统方法在缺乏参考分布时的局限性。

Abstract: Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.

</details>


### [39] [SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation](https://arxiv.org/abs/2602.23199)
*Jiahao Zhao,Feng Jiang,Shaowei Qin,Zhonghui Zhang,Junhao Liu,Guibing Guo,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: SC-ARENA是一个为单细胞基础模型设计的自然语言评估框架，通过虚拟细胞抽象统一评估目标，包含五种自然语言任务，并引入知识增强评估来克服传统指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞生物学中，无论是通用还是专用大语言模型的评估实践都存在不足：现有基准测试分散在不同任务中，采用多项选择等与现实使用脱节的格式，且依赖缺乏可解释性和生物学基础的指标。

Method: 提出SC-ARENA框架，包含：1）虚拟细胞抽象统一评估目标；2）五种自然语言任务（细胞类型注释、描述、生成、扰动预测和科学问答）；3）知识增强评估，整合外部本体、标记数据库和科学文献。

Result: 实验表明：1）在虚拟细胞统一评估范式下，当前模型在生物学复杂任务上表现不均，特别是需要机制或因果理解的任务；2）知识增强评估框架确保生物学正确性，提供可解释的证据支持理由，并具有高区分能力。

Conclusion: SC-ARENA为单细胞生物学中的大语言模型评估提供了统一且可解释的框架，指向开发生物学对齐、可泛化的基础模型。

Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.

</details>


### [40] [A Model-Free Universal AI](https://arxiv.org/abs/2602.23242)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 该论文提出了首个被证明在一般强化学习中具有渐近ε最优性的无模型智能体AIQI，通过分布动作值函数的通用归纳实现


<details>
  <summary>Details</summary>
Motivation: 现有最优智能体（如AIXI）都是基于模型的，需要显式维护和使用环境模型。本文旨在开发首个在一般强化学习中具有理论保证的无模型智能体，扩展通用智能体的多样性

Method: 提出Universal AI with Q-Induction (AIQI)智能体，通过对分布动作值函数进行通用归纳，而不是像以往工作那样对策略或环境进行归纳

Result: 在"真理颗粒"条件下，证明AIQI具有强渐近ε最优性和渐近ε贝叶斯最优性，是首个被证明在一般强化学习中具有渐近最优性的无模型智能体

Conclusion: AIQI显著扩展了已知通用智能体的多样性，为无模型强化学习提供了理论基础，填补了理论保证的无模型通用智能体的空白

Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.

</details>


### [41] [Mitigating Legibility Tax with Decoupled Prover-Verifier Games](https://arxiv.org/abs/2602.23248)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 提出通过解耦正确性和可检查性来降低可读性税的方法：训练一个"翻译器"模型，将固定求解器模型的输出转换为可检查形式，从而在保持求解器准确性的同时实现可验证性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，需要让能力较弱的系统也能轻松检查其输出。现有的证明者-验证者游戏虽然能提高模型输出的可检查性，但会导致准确性下降（可读性税）。

Method: 提出解耦方法：1）先训练求解器模型最大化正确性；2）然后训练翻译器模型，将求解器的解决方案转换为可检查形式，同时保留求解器的答案。为此提出了解耦的证明者-验证者游戏，其均衡对应忠实且可检查的翻译器。

Result: 该方法理论上能够解决可读性税问题，允许在保持求解器模型高准确性的同时，通过翻译器实现输出的可检查性。

Conclusion: 通过解耦正确性和可检查性，并引入翻译器模型，可以在不牺牲准确性的前提下提高大语言模型输出的可验证性，为解决可读性税问题提供了新思路。

Abstract: As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a "translator" model that turns a fixed solver model's solution into a checkable form. This allows us to first train the solver to maximize correctness, and then train the translator to translate the solver into a checkable form while retaining the solver's answer. To accommodate this new objective of translation, we formulate a decoupled prover-verifier game where the equilibria correspond to faithful and checkable translators.

</details>


### [42] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentDropoutV2是一个测试时修正或剪枝框架，通过动态优化多智能体系统信息流来减少错误传播，无需重新训练


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂推理方面表现出色，但存在个体参与者生成错误信息导致的级联影响问题。现有解决方案通常采用刚性结构工程或昂贵的微调，限制了部署性和适应性。

Method: 提出AgentDropoutV2框架，作为主动防火墙拦截智能体输出，采用检索增强的修正器基于失败驱动的指示器池迭代纠正错误。使用蒸馏的失败模式作为先验知识精确识别潜在错误，不可修复的输出被剪枝以防止错误传播，同时采用回退策略保持系统完整性。

Result: 在广泛的数学基准测试中，AgentDropoutV2显著提升了多智能体系统的任务性能，在数学基准上平均准确率提高了6.3个百分点。系统表现出强大的泛化能力和适应性，能够根据任务难度动态调整修正努力，并利用上下文感知指示器解决广泛的错误模式。

Conclusion: AgentDropoutV2是一个有效的测试时框架，能够在不重新训练的情况下动态优化多智能体系统的信息流，显著减少错误传播并提高系统性能，具有良好的部署性和适应性。

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [43] [CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276)
*Hyungyung Lee,Hangyul Yoon,Edward Choi*

Main category: cs.AI

TL;DR: CXReasonAgent是一个结合大语言模型与临床诊断工具的智能体，通过图像衍生的诊断和视觉证据进行基于证据的诊断推理，解决了传统大视觉语言模型在胸部X光诊断中证据不足和可验证性差的问题。


<details>
  <summary>Details</summary>
Motivation: 胸部X光在胸部诊断中起核心作用，其解读需要多步骤、基于证据的推理。然而，现有的大视觉语言模型(LVLMs)生成的响应往往缺乏诊断证据的忠实基础，提供的视觉证据有限，且需要昂贵的重新训练来支持新诊断任务，限制了其在临床环境中的可靠性和适应性。

Method: 提出CXReasonAgent诊断智能体，将大语言模型(LLM)与临床诊断工具集成，使用图像衍生的诊断和视觉证据进行基于证据的诊断推理。同时引入CXReasonDial多轮对话基准，包含12个诊断任务的1,946个对话。

Result: CXReasonAgent能够产生忠实基于证据的响应，相比LVLMs实现了更可靠和可验证的诊断推理。通过基准测试验证了其在多诊断任务中的有效性。

Conclusion: 在安全关键的临床环境中，集成临床诊断工具对于实现可靠、可验证的诊断推理至关重要。CXReasonAgent通过结合LLM与诊断工具，为胸部X光诊断提供了更可靠的解决方案。

Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

</details>


### [44] [ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Rikuto Kotoge,Jathurshan Pradeepkumar,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai,Takashi Matsubara*

Main category: cs.AI

TL;DR: ODEBRAIN是一个基于神经ODE的脑电动态预测框架，通过将时空频特征整合到谱图节点中，然后使用神经ODE建模连续潜在动态，显著提升了EEG动态预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统潜变量方法通过递归架构离散化时间建模连续脑动态，这必然导致累积预测误差的复合，并且无法捕捉EEG的瞬时非线性特征。需要一种能够克服这些挑战的新方法。

Method: 提出ODEBRAIN框架：1) 将时空频特征整合到谱图节点中；2) 使用神经ODE建模连续潜在动态；3) 确保潜表示能够捕捉任意时间点上复杂脑状态的随机变化。

Result: 大量实验验证表明，ODEBRAIN在EEG动态预测方面显著优于现有方法，具有增强的鲁棒性和泛化能力。

Conclusion: ODEBRAIN通过神经ODE框架有效建模连续脑动态，克服了传统方法的累积误差问题，能够更好地捕捉EEG的瞬时非线性特征，为神经科学研究提供了更准确的动态预测工具。

Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

</details>


### [45] [The logic of KM belief update is contained in the logic of AGM belief revision](https://arxiv.org/abs/2602.23302)
*Giacomo Bonanno*

Main category: cs.AI

TL;DR: 该论文将KM信念更新的公理转化为包含三个模态算子的模态逻辑公理，并与AGM信念修正的模态逻辑进行比较，证明AGM信念修正是KM信念更新的特例。


<details>
  <summary>Details</summary>
Motivation: 研究KM信念更新公理与AGM信念修正公理在模态逻辑框架下的关系，探索两者之间的包含关系，以理解信念更新与信念修正的理论联系。

Method: 为KM信念更新的每个公理构建对应的模态逻辑公理，使用三个模态算子：单模态信念算子B、双模态条件算子>和单模态必然算子□。然后将得到的逻辑与AGM信念修正公理转换得到的模态逻辑进行比较。

Result: 证明KM信念更新的模态逻辑L_KM是AGM信念修正的模态逻辑L_AGM的子集，即L_KM的每个公理都是L_AGM的定理。对于强版本KM信念更新，L_KM与L_AGM的差异可归结为单个公理，该公理专门处理非意外信息。

Conclusion: AGM信念修正是KM信念更新的特殊情形，两者在模态逻辑框架下具有包含关系，这为理解信念更新与信念修正的理论联系提供了形式化基础。

Abstract: For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\mathcal L_{AGM}$ and the former by $\mathcal L_{KM}$ we show that every axiom of $\mathcal L_{KM}$ is a theorem of $\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\mathcal L_{KM}$ and $\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.

</details>


### [46] [Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315)
*Sha Hu*

Main category: cs.AI

TL;DR: 提出基于重采样的推理方法，通过对输入进行不变变换生成多个版本，聚合推理结果以提高准确性


<details>
  <summary>Details</summary>
Motivation: 即使经过优化的AI模型也会因偶然性和认知不确定性而产生推理错误，观察到基于输入不变变换的多次推理中错误存在部分独立性

Method: 提出"重采样"推理方法：对训练好的AI模型应用输入的多个变换版本，然后聚合推理输出以获得更准确的结果

Result: 该方法有潜力提高推理准确性，并提供了平衡模型大小和性能的策略

Conclusion: 利用认知不确定性导致的推理错误部分独立性，通过重采样和聚合方法可以改善AI模型的推理性能

Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

</details>


### [47] [Generalized Rapid Action Value Estimation in Memory-Constrained Environments](https://arxiv.org/abs/2602.23318)
*Aloïs Rautureau,Tristan Cazenave,Éric Piette*

Main category: cs.AI

TL;DR: GRAVE2、GRAVER和GRAVER2算法通过两层搜索、节点回收及其组合技术扩展GRAVE，在保持游戏强度的同时大幅减少存储节点数量。


<details>
  <summary>Details</summary>
Motivation: GRAVE算法在通用游戏博弈中表现优异，但其在每个节点存储额外胜率/访问统计数据的特性使得在内存受限环境中使用不切实际，限制了实际应用。

Method: 提出三种算法：GRAVE2（两层搜索）、GRAVER（节点回收）和GRAVER2（两种技术结合），通过减少存储节点数量来优化GRAVE算法。

Result: 这些增强技术能够显著减少存储节点数量，同时保持与GRAVE相当的游戏强度。

Conclusion: 通过两层搜索和节点回收技术，可以在内存受限环境中有效应用GRAVE算法，扩展其实际应用范围。

Abstract: Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.

</details>


### [48] [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329)
*Chen Bo Calvin Zhang,Christina Q. Knight,Nicholas Kruus,Jason Hausenloy,Pedro Medeiros,Nathaniel Li,Aiden Kim,Yury Orlovskiy,Coleman Breen,Bryce Cai,Jasper Götting,Andrew Bo Liu,Samira Nedungadi,Paula Rodriguez,Yannis Yiming He,Mohamed Shaaban,Zifan Wang,Seth Donoughe,Julian Michael*

Main category: cs.AI

TL;DR: LLMs显著提升生物安全相关任务中新手用户的性能，使其表现优于仅使用互联网资源的专家，但用户未能充分利用LLM的全部潜力，且安全防护措施存在漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估LLMs是否真正提升新手用户在生物学任务中的表现，这对于理解科学加速和双重用途风险至关重要。现有研究主要关注LLMs在基准测试上的表现，但缺乏对实际用户提升效果的评估。

Method: 采用多模型、多基准的人类提升研究，比较有LLM访问权限的新手与仅使用互联网资源的新手在八个生物安全相关任务集上的表现。参与者有充足时间（最复杂任务达13小时）解决复杂问题。

Result: LLM访问提供显著提升：有LLM的新手准确率是对照组的4.16倍（95% CI [2.63, 6.87]）。在四个有专家基线的基准测试中，有LLM的新手在三个上表现优于专家。但独立LLMs通常超过LLM辅助的新手，表明用户未能充分利用LLM潜力。89.6%的参与者报告获取双重用途相关信息几乎没有困难。

Conclusion: LLMs显著提升了新手在原本需要专业训练的生物任务上的表现，强调了需要持续、交互式的提升评估与传统基准测试并行，同时也揭示了当前安全防护措施的不足。

Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

</details>


### [49] [Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330)
*Kunihiro Miyazaki,Takanobu Kawahara,Stephen Roberts,Stefan Zohren*

Main category: cs.AI

TL;DR: 该论文提出了一个细粒度任务分解的多智能体LLM交易框架，相比传统粗粒度指令方法，显著提升了风险调整后收益，并通过投资分析输出与决策偏好的对齐优化了系统性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于多智能体系统的自主金融交易系统通常采用模仿分析师和经理角色的抽象指令，但忽视了真实工作流程的复杂性，导致推理性能下降和决策透明度不足。

Method: 提出了一个多智能体LLM交易框架，将投资分析明确分解为细粒度任务，而非提供粗粒度指令。使用日本股票数据（价格、财务报表、新闻、宏观信息）在泄漏控制的回测设置下评估框架。通过标准投资组合优化，利用与股票指数的低相关性和每个系统输出的方差来提升性能。

Result: 实验结果显示，细粒度任务分解相比传统粗粒度设计显著提高了风险调整后收益。进一步分析表明，分析输出与下游决策偏好的对齐是系统性能的关键驱动因素。通过投资组合优化方法获得了优越的性能表现。

Conclusion: 这些发现为在实际交易系统中应用LLM智能体时的智能体结构和任务配置设计提供了重要贡献，强调了细粒度任务分解和分析输出与决策偏好对齐的重要性。

Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

</details>

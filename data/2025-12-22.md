<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases](https://arxiv.org/abs/2512.16953)
*Pietro Cofone,Giovanni Amendola,Marco Manna,Aldo Ricioppo*

Main category: cs.AI

TL;DR: 本文提出了一种在知识库驱动的实体集扩展图中进行高效推理的方法，支持局部增量导航而无需构建完整图结构。


<details>
  <summary>Details</summary>
Motivation: 传统的线性实体集扩展方法无法揭示知识资源中更丰富的分类结构。虽然基于逻辑的扩展图框架能够支持分类扩展，但完整构建大型图在实际场景中可能不切实际。

Method: 形式化定义了推理任务来检查两个元组在扩展图中是否属于可比、不可比或相同的节点。在现实假设下（如限制输入或实体描述），这些任务可以高效实现。

Result: 研究结果表明，在现实假设下，这些推理任务可以高效实现，支持对扩展图进行局部增量导航。

Conclusion: 该方法使得无需构建完整扩展图即可支持实际应用，为知识库驱动的实体集扩展提供了实用的解决方案。

Abstract: Recognizing similarities among entities is central to both human cognition and computational intelligence. Within this broader landscape, Entity Set Expansion is one prominent task aimed at taking an initial set of (tuples of) entities and identifying additional ones that share relevant semantic properties with the former -- potentially repeating the process to form increasingly broader sets. However, this ``linear'' approach does not unveil the richer ``taxonomic'' structures present in knowledge resources. A recent logic-based framework introduces the notion of an expansion graph: a rooted directed acyclic graph where each node represents a semantic generalization labeled by a logical formula, and edges encode strict semantic inclusion. This structure supports taxonomic expansions of entity sets driven by knowledge bases. Yet, the potentially large size of such graphs may make full materialization impractical in real-world scenarios. To overcome this, we formalize reasoning tasks that check whether two tuples belong to comparable, incomparable, or the same nodes in the graph. Our results show that, under realistic assumptions -- such as bounding the input or limiting entity descriptions -- these tasks can be implemented efficiently. This enables local, incremental navigation of expansion graphs, supporting practical applications without requiring full graph construction.

</details>


### [2] [Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows](https://arxiv.org/abs/2512.16969)
*Wanghan Xu,Yuhao Zhou,Yifan Zhou,Qinglong Cao,Shuo Li,Jia Bu,Bo Liu,Yixin Chen,Xuming He,Xiangyu Zhao,Xiang Zhuang,Fengxiang Wang,Zhiwang Zhou,Qiantai Feng,Wenxuan Huang,Jiaqi Wei,Hao Wu,Yuejin Yang,Guangshuai Wang,Sheng Xu,Ziyan Huang,Xinyao Liu,Jiyao Liu,Cheng Tang,Wei Li,Ying Chen,Junzhi Ning,Pengfei Jiang,Chenglong Ma,Ye Du,Changkai Ji,Huihui Xu,Ming Hu,Jiangbin Zheng,Xin Chen,Yucheng Wu,Feifei Jiang,Xi Chen,Xiangru Tang,Yuchen Fu,Yingzhou Lu,Yuanyuan Zhang,Lihao Sun,Chengbo Li,Jinzhe Ma,Wanhao Liu,Yating Liu,Kuo-Cheng Wu,Shengdu Chai,Yizhou Wang,Ouwen Zhangjin,Chen Tang,Shufei Zhang,Wenbo Cao,Junjie Ren,Taoyong Cui,Zhouheng Yao,Juntao Deng,Yijie Sun,Feng Liu,Wangxu Wei,Jingyi Xu,Zhangrui Li,Junchao Gong,Zijie Guo,Zhiyu Yao,Zaoyu Chen,Tianhao Peng,Fangchen Yu,Bo Zhang,Dongzhan Zhou,Shixiang Tang,Jiaheng Liu,Fenghua Ling,Yan Lu,Yuchen Ren,Ben Fei,Zhen Zhao,Xinyu Gu,Rui Su,Xiao-Ming Wu,Weikang Si,Yang Liu,Hao Chen,Xiangchao Yan,Xue Yang,Junchi Yan,Jiamin Wu,Qihao Zheng,Chenhui Li,Zhiqiang Gao,Hao Kong,Junjun He,Mao Su,Tianfan Fu,Peng Ye,Chunfeng Song,Nanqing Dong,Yuqiang Li,Huazhu Fu,Siqi Sun,Lijing Cheng,Jintai Lin,Wanli Ouyang,Bowen Zhou,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 该论文提出了科学通用智能（SGI）的操作化定义，基于实践探究模型（PIM），并创建了SGI-Bench基准测试，包含1000多个跨学科样本，用于评估大语言模型在科学研究任务中的表现，同时提出了测试时强化学习（TTRL）方法来提升假设新颖性。


<details>
  <summary>Details</summary>
Motivation: 尽管科学AI有所进展，但缺乏一个连贯的科学通用智能（SGI）框架——即能够自主构思、调查和跨科学领域推理的能力。需要建立操作化的SGI定义和系统评估基准。

Method: 基于实践探究模型（PIM：审议、构思、行动、感知）建立SGI操作化定义，对应四个科学家对齐任务：深度研究、想法生成、干/湿实验、实验推理。创建SGI-Bench基准，包含1000多个专家策划的跨学科样本，并引入测试时强化学习（TTRL）在推理时优化检索增强的新颖性奖励。

Result: 评估结果显示当前LLM存在差距：深度研究任务中精确匹配率低（10-20%）；想法缺乏可行性和细节；干实验中代码可执行性高但执行结果准确性低；湿实验协议序列保真度低；多模态比较推理挑战持续存在。TTRL方法能在没有参考答案的情况下提升假设新颖性。

Conclusion: 基于PIM的定义、以工作流为中心的基准测试和实证见解为真正参与科学发现的AI系统奠定了基础，为科学通用智能的发展提供了框架和评估标准。

Abstract: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.

</details>


### [3] [PAACE: A Plan-Aware Automated Agent Context Engineering Framework](https://arxiv.org/abs/2512.16970)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: PAACE是一个用于优化LLM智能体上下文管理的统一框架，通过计划感知的上下文工程来提高智能体性能并减少计算成本。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在多步骤工作流程中会产生快速扩展的上下文，现有方法忽略了多步骤、计划感知的智能体推理特性，需要新的上下文优化方案。

Method: PAACE框架包含PAACE-Syn（生成带压缩监督的合成工作流程）和PAACE-FT（从教师演示中蒸馏出的计划感知压缩器），通过任务相关性建模、计划结构分析、指令协同优化和函数保持压缩来优化上下文。

Result: 在多个长视野基准测试中，PAACE在提高智能体正确性的同时显著降低上下文负载。蒸馏后的PAACE-FT保留了教师模型97%的性能，同时将推理成本降低了一个数量级以上。

Conclusion: PAACE为LLM智能体提供了有效的计划感知上下文工程框架，能够在保持性能的同时大幅降低计算成本，实现实用部署。

Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.

</details>


### [4] [UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering](https://arxiv.org/abs/2512.17043)
*Yinxu Tang,Chengsong Huang,Jiaxin Huang,William Yeoh*

Main category: cs.AI

TL;DR: 本文提出了一种面向关系知识图谱问答的新框架UniRel-R1，专注于返回捕捉实体间语义连接的子图而非单个实体，通过集成子图选择、多阶段图剪枝和强化学习微调的LLM来解决候选子图过多的问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱问答主要关注返回单个答案实体的实体中心查询，但现实世界查询通常是关系性的，需要理解实体之间的关联。因此需要一种补充性的关系中心KGQA设置，其答案应捕捉实体间语义连接的子图。

Method: 提出UniRel-R1统一框架，集成子图选择、多阶段图剪枝和强化学习微调的大语言模型。奖励函数设计鼓励紧凑且特定的子图，包含更多信息性关系和更低度的中间实体。

Result: 大量实验表明，UniRel-R1在连接性和奖励方面相比Vanilla基线取得显著提升，并且能有效泛化到未见过的实体和关系。

Conclusion: UniRel-R1框架成功解决了关系中心知识图谱问答的挑战，通过集成多种技术有效识别独特且信息丰富的答案子图，为KGQA领域提供了重要的补充方向。

Abstract: Knowledge Graph Question Answering (KGQA) has traditionally focused on entity-centric queries that return a single answer entity. However, real-world queries are often relational, seeking to understand how entities are associated. In this work, we introduce relation-centric KGQA, a complementary setting where the answer is a subgraph capturing the semantic connections among entities rather than an individual entity. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel-R1, a unified framework that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reward function is designed to encourage compact and specific subgraphs with more informative relations and lower-degree intermediate entities. Extensive experiments show that UniRel-R1 achieves significant gains in connectivity and reward over Vanilla baselines and generalizes effectively to unseen entities and relations.

</details>


### [5] [Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations](https://arxiv.org/abs/2512.17066)
*Suhaib Abdurahman,Farzan Karimi-Malekabadi,Chenxiao Yu,Nour S. Kteily,Morteza Dehghani*

Main category: cs.AI

TL;DR: 使用大语言模型驱动的智能体在虚拟社会中模拟冲突，研究物质威胁和象征性威胁对冲突的影响，发现物质威胁直接增加敌意，而象征性威胁效应较弱且完全通过内群体偏见中介


<details>
  <summary>Details</summary>
Motivation: 人类冲突常归因于物质条件和象征性价值受到的威胁，但两者如何相互作用以及哪个占主导地位尚不清楚。研究进展受到因果控制弱、伦理约束和时间数据稀缺的限制

Method: 使用大语言模型驱动的智能体在虚拟社会中模拟冲突，独立操纵现实威胁和象征性威胁，同时追踪行动、语言和态度。通过表征分析研究LLM如何编码这些状态

Result: LLM将现实威胁、象征性威胁和敌意编码为不同的内部状态；现实威胁直接增加敌意，象征性威胁效应较弱且完全通过内群体偏见中介；非敌对性群体间接触能缓冲冲突升级，结构性不对称使敌意集中在多数群体中

Conclusion: 通过LLM驱动的智能体模拟为威胁驱动的冲突提供了因果解释，物质威胁是冲突的主要直接驱动因素，而象征性威胁只有在物质威胁不存在时才会增加敌意

Abstract: Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.

</details>


### [6] [Value Under Ignorance in Universal Artificial Intelligence](https://arxiv.org/abs/2512.17086)
*Cole Wyeth,Marcus Hutter*

Main category: cs.AI

TL;DR: 将AIXI强化学习智能体推广到更广泛的效用函数类别，通过处理信念分布中只能预测有限历史前缀的假设，探讨了半测度损失的不同解释及其对效用分配的影响。


<details>
  <summary>Details</summary>
Motivation: 传统AIXI智能体在效用函数方面存在局限性，特别是当智能体的信念分布中某些假设只能预测有限历史前缀时，这涉及到半测度损失的解释问题。作者希望扩展AIXI框架以容纳更广泛的效用函数，并探讨不同解释（如死亡解释与不精确概率解释）对智能体决策的影响。

Method: 1. 将AIXI智能体推广到更广泛的效用函数类别；2. 分析信念分布中只能预测有限历史前缀的假设，探讨半测度损失的两种解释：死亡解释和不精确概率解释；3. 使用不精确概率理论中的Choquet积分计算期望效用；4. 研究这些方法的可计算性水平。

Result: 1. 标准递归值函数可以作为Choquet积分的特例恢复；2. 在死亡解释下，最一般的期望效用不能表征为Choquet积分；3. 提供了处理有限历史预测假设的两种不同框架，并分析了它们的数学性质和可计算性。

Conclusion: 通过扩展AIXI智能体以容纳更广泛的效用函数，并探讨半测度损失的不同解释，本文为强化学习智能体提供了更灵活的决策框架。虽然死亡解释下的期望效用不能完全用Choquet积分表征，但不精确概率解释为处理信念分布中的不确定性提供了有价值的数学工具。

Abstract: We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.

</details>


### [7] [A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving](https://arxiv.org/abs/2512.17093)
*Timo Pierre Schrader,Lukas Lange,Tobias Kaminski,Simon Razniewski,Annemarie Friedrich*

Main category: cs.AI

TL;DR: 本文提出了一种ASP求解器在循环中的方法，通过求解器引导的指令微调来提升大语言模型生成答案集编程代码的能力，仅需自然语言问题描述和解决方案即可训练。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用编程语言上表现良好，但在领域特定语言（如答案集编程ASP）的代码生成方面面临挑战。ASP是解决组合搜索问题的有效方法，但LLMs在ASP代码生成上的效果受到预训练阶段示例数量有限的限制。

Method: 提出ASP求解器在循环中的方法，利用求解器引导的指令微调。方法仅需自然语言问题描述和解决方案：1）从LLMs采样ASP语句作为程序延续；2）利用ASP声明式编程的特性（部分编码逐步缩小解空间），基于求解器反馈将样本分类为接受和拒绝实例；3）对筛选数据进行监督微调；4）使用求解器引导的搜索（包括最佳N采样）进一步提高鲁棒性。

Result: 实验在两个数据集上的两种不同提示设置中均显示出持续改进，证明了方法的有效性。

Conclusion: 该方法通过求解器引导的指令微调，有效提升了LLMs在ASP代码生成任务上的性能，仅需自然语言问题描述和解决方案即可实现训练，为解决领域特定语言代码生成问题提供了新思路。

Abstract: The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.
  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.

</details>


### [8] [Reinforcement Learning for Self-Improving Agent with Skill Library](https://arxiv.org/abs/2512.17102)
*Jiongxiao Wang,Qiaojing Yan,Yawei Wang,Yijun Tian,Soumya Smruti Mishra,Zhichao Xu,Megha Gandhi,Panpan Xu,Lin Lee Cheong*

Main category: cs.AI

TL;DR: SAGE是一个基于强化学习的框架，通过技能库增强LLM智能体的自我进化能力，在AppWorld任务中实现了更高的场景目标完成率和更低的交互成本。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂推理和多轮交互方面表现出色，但在新环境中部署时难以持续改进和适应。现有的技能库方法主要依赖LLM提示，难以实现一致的技能库实施。

Method: 提出了SAGE（Skill Augmented GRPO for self-Evolution）框架，采用强化学习方法将技能系统性地融入学习过程。核心组件包括：1）Sequential Rollout - 在相似任务链上迭代部署智能体；2）技能库积累 - 先前任务生成的技能可用于后续任务；3）Skill-integrated Reward - 结合原始结果奖励的技能集成奖励机制。

Result: 在AppWorld上的实验结果显示，SAGE应用于具有专家经验的监督微调模型时，实现了8.9%更高的场景目标完成率，同时需要减少26%的交互步骤和生成59%更少的token，在准确性和效率上都显著优于现有方法。

Conclusion: SAGE框架通过强化学习结合技能库的方法，有效增强了LLM智能体的自我改进能力，在任务完成效率和资源消耗方面都取得了显著提升，为解决智能体在新环境中的适应问题提供了有效方案。

Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.

</details>


### [9] [Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty](https://arxiv.org/abs/2512.17145)
*Josh Barber,Rourke Young,Cameron Coombe,Will Browne*

Main category: cs.AI

TL;DR: 提出基于Solomonoff启发的LLM假设加权方法，通过简洁性和预测拟合度评估多个候选解，在不确定性推理中实现更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理稀疏数据的不确定性推理任务时，难以在准确性和简洁性之间取得平衡，特别是在评估多个候选解决方案时存在挑战。

Method: 提出Solomonoff启发的方法，对LLM生成的假设进行加权，权重基于简洁性和预测拟合度。应用于Mini-ARC基准任务，生成Solomonoff加权的混合预测，即使假设有噪声或不完全正确也能产生保守、不确定性感知的输出。

Result: 与贝叶斯模型平均(BMA)相比，Solomonoff评分将概率更均匀地分布在竞争假设之间，而BMA则将权重集中在最可能但可能有缺陷的候选假设上。该方法在任务中表现出更好的不确定性推理能力。

Conclusion: 算法信息论先验对于可解释、可靠的多假设不确定性推理具有重要价值，特别是在需要系统性泛化的现实任务中。

Abstract: Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.

</details>


### [10] [MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation](https://arxiv.org/abs/2512.17194)
*Shengwei Zhao,Jingwen Yao,Sitong Wei,Linhai Xu,Yuying Liu,Dong Zhang,Zhiqiang Tian,Shaoyi Du*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的可解释多模态检索增强生成方法，通过两阶段强化微调框架提升多模态大语言模型的推理能力，在WebQA和MultimodalQA数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有MMRAG方法无法澄清检索和响应生成背后的推理逻辑，限制了结果的可解释性。为了解决这一缺陷，作者提出将强化学习引入多模态检索增强生成。

Method: 采用两阶段强化微调框架：第一阶段使用基于规则的强化微调对多模态文档进行粗粒度点式排序，过滤显著不相关文档；第二阶段使用基于推理的强化微调联合优化细粒度列表排序和答案生成，引导模型输出可解释的推理逻辑。

Result: 在WebQA和MultimodalQA两个多模态检索增强生成基准数据集上取得了最先进的结果，并通过全面的消融实验验证了方法的有效性。

Conclusion: 通过引入强化学习到多模态检索增强生成中，提出的两阶段强化微调框架显著提升了多模态大语言模型的推理能力，实现了可解释的多模态检索增强生成。

Abstract: Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.

</details>


### [11] [UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark](https://arxiv.org/abs/2512.17196)
*Kai Liu,Leyang Chen,Wenbo Li,Zhikai Chen,Zhixin Wang,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.AI

TL;DR: UmniBench是一个针对统一多模态模型（UMMs）的全维度评估基准，能够在单个评估过程中同时测试理解、生成和编辑能力，覆盖13个主要领域和200多个概念。


<details>
  <summary>Details</summary>
Motivation: 当前对统一多模态模型的评估通常是解耦的，分别评估其理解和生成能力，缺乏综合性的评估方法。需要开发一个能够全面评估UMMs各方面能力的基准。

Method: UmniBench利用UMM自身的能力进行评估：基于人工检查的提示和问答对，使用UMM的理解能力来评估其生成和编辑能力。这种简单有效的范式允许对UMMs进行全面评估，同时也能解耦评估各项能力。

Result: 基于UmniBench对24个流行模型进行了基准测试，包括统一多模态模型和单能力大模型。该基准提供了对统一模型更全面客观的视角，并为社区模型性能改进提供了支持。

Conclusion: UmniBench是一个全面的多模态模型评估基准，能够在一个框架内评估理解、生成和编辑能力，为统一多模态模型的评估提供了更全面和客观的方法。

Abstract: Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.

</details>


### [12] [ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework](https://arxiv.org/abs/2512.17266)
*Miru Hong,Minho Lee,Geonhee Jo,Jae-Hee So,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: EventGPT：基于GPT架构的球员条件化价值感知下一事件预测模型，用于足球转会分析，通过反事实模拟评估球员在不同战术环境中的适应性


<details>
  <summary>Details</summary>
Motivation: 现有转会评估方法依赖静态统计数据或事后价值模型，无法捕捉球员在新战术环境或不同队友中的适应性变化，需要一种能预测球员在不同情境下表现的方法

Method: 基于GPT风格自回归变换器构建球员条件化、价值感知的下一事件预测模型，将比赛视为离散标记序列，联合预测下一持球动作的类型、位置、时间和残差持球价值，通过替换球员嵌入进行反事实模拟

Result: 在五个赛季的英超赛事数据评估中，EventGPT在下一事件预测准确性和空间精度上优于现有序列基线模型，并通过案例研究展示了在转会分析中的实际效用

Conclusion: EventGPT提供了一个原则性方法来评估转会适应性，能够模拟球员在不同球队或战术结构中的行为分布和价值变化，为转会决策提供更科学的依据

Abstract: Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.

</details>


### [13] [Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation](https://arxiv.org/abs/2512.17308)
*Daksh Jain,Aarya Jain,Ashutosh Desai,Avyakt Verma,Ishan Bhanuka,Pratik Narang,Dhruv Kumar*

Main category: cs.AI

TL;DR: LLMs在宝可梦对战中的战略决策能力评估，展示了它们作为游戏对手和内容创作者的潜力


<details>
  <summary>Details</summary>
Motivation: 宝可梦对战需要类型匹配、统计权衡和风险评估等战略思维，为评估大语言模型的推理能力提供了独特测试平台

Method: 开发基于回合制的宝可梦对战系统，让LLMs根据战斗状态选择招式而非预设逻辑，系统包含类型克制、伤害计算和队伍管理等核心机制

Result: 通过多个模型架构的系统评估，测量了胜率、决策延迟、类型匹配准确性和token效率，结果表明LLMs无需领域特定训练即可作为动态游戏对手

Conclusion: LLMs兼具战术推理和内容创作双重能力，可作为玩家和设计师，对程序化生成和自适应难度系统有重要应用价值

Abstract: Strategic decision-making in Pokémon battles presents a unique testbed for evaluating large language models. Pokémon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pokémon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pokémon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pokémon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.

</details>


### [14] [Dialectics for Artificial Intelligence](https://arxiv.org/abs/2512.17373)
*Zhengmian Hu*

Main category: cs.AI

TL;DR: 论文提出从算法信息论角度定义"概念"，将其视为与智能体整体经验结构相关的信息对象，通过可逆一致性关系和冗余信息度量来形式化概念发现与演化，并建立多智能体概念对齐的通信框架。


<details>
  <summary>Details</summary>
Motivation: 人类概念具有流动性（如冥王星不再被视为行星），传统基于标签的概念定义无法捕捉这种动态性。需要一种能够被修订、比较和在智能体间对齐的概念形式化方法，以研究AI能否从原始经验中自主发现人类概念。

Method: 采用算法信息论视角，将概念定义为信息对象，核心约束是确定性：部分之间形成可逆一致性关系（缺失部分可从其他部分恢复）。定义冗余信息度量分解的自然性，建立辩证法优化动态，概念通过更短的描述竞争解释新信息。形式化低成本概念传输和多智能体对齐机制。

Result: 提出了基于算法信息论的概念形式化框架，使概念存在性成为可检验的结构性主张，防止概念脱离经验。建立了概念演化（扩展、收缩、分裂、合并）的优化动态，以及多智能体间通过共享协议和种子实现概念对齐的通信机制。

Conclusion: 该框架为研究AI自主概念发现提供了理论基础，将概念视为与经验结构相关的信息对象而非固定标签，能够捕捉概念的动态演化特性，并为多智能体概念对齐提供了具体的计算-比特权衡方案。

Abstract: Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of "concept" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents "concepts" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.

</details>


### [15] [Translating the Rashomon Effect to Sequential Decision-Making Tasks](https://arxiv.org/abs/2512.17470)
*Dennis Gross,Jørn Eirik Betten,Helge Spieker*

Main category: cs.AI

TL;DR: 该研究将Rashomon效应从分类任务扩展到序列决策领域，发现多个策略在行为表现相同的情况下内部特征依赖不同，并通过形式验证方法证明这种现象的存在，同时展示了Rashomon集合构建的集成策略具有更好的分布偏移鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Rashomon效应在分类任务中已被广泛研究，但在序列决策领域尚未被探索。序列决策中的策略学习与分类任务存在本质差异，特别是在随机转移环境中验证相同行为更具挑战性。研究旨在填补这一空白，探索序列决策中是否存在Rashomon效应及其实际应用价值。

Method: 研究采用形式验证方法，通过构建和比较每个策略在环境中的完整概率行为来验证相同行为。在随机转移的序列决策环境中，使用概率行为比较而非单次轨迹评估。实验设计包括：1）验证Rashomon效应在序列决策中的存在；2）从Rashomon集合构建集成策略；3）从Rashomon集合推导许可策略。

Result: 实验证明Rashomon效应确实存在于序列决策中。从Rashomon集合构建的集成策略相比单个策略展现出更强的分布偏移鲁棒性。此外，从Rashomon集合推导的许可策略能够在保持最优性能的同时，显著降低验证的计算需求。

Conclusion: 该研究成功将Rashomon效应扩展到序列决策领域，证明了其存在性并展示了实际应用价值。Rashomon集合不仅揭示了策略学习的多样性，还为构建更鲁棒的集成策略和降低验证成本提供了新途径，对强化学习和策略验证领域具有重要意义。

Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.

</details>


### [16] [Towards Explainable Conversational AI for Early Diagnosis with Large Language Models](https://arxiv.org/abs/2512.17559)
*Maliha Tabassum,M Shamim Kaiser*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型的诊断聊天机器人，结合GPT-4o、检索增强生成和可解释AI技术，在医疗诊断中实现了90%的准确率和100%的Top-3准确率。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临诊断效率低下、成本上升和专家资源有限等问题，导致治疗延误和不良健康结果。当前大多数AI诊断系统缺乏交互性和透明度，难以在实际临床环境中有效应用。

Method: 采用基于大语言模型的诊断聊天机器人，整合GPT-4o、检索增强生成和可解释AI技术。系统通过动态对话提取和标准化症状，利用相似性匹配和自适应提问优先诊断，并通过思维链提示提供透明的诊断推理。

Result: 与传统机器学习模型（朴素贝叶斯、逻辑回归、SVM、随机森林、KNN）相比，LLM系统表现出色，达到90%的准确率和100%的Top-3准确率。

Conclusion: 该研究为医疗领域提供了更透明、交互性更强且临床相关性更高的AI解决方案，展示了LLM在改善医疗诊断方面的巨大潜力。

Abstract: Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.

</details>


### [17] [About Time: Model-free Reinforcement Learning with Timed Reward Machines](https://arxiv.org/abs/2512.17637)
*Anirban Majumdar,Ritam Raha,Rajarshi Roy,David Parker,Marta Kwiatkowska*

Main category: cs.AI

TL;DR: 本文提出了定时奖励机（TRMs），作为奖励机的扩展，将时间约束纳入奖励结构中，用于强化学习中的时间敏感应用。


<details>
  <summary>Details</summary>
Motivation: 传统奖励机无法建模精确的时间约束，限制了其在时间敏感应用中的使用。需要一种能够表达时间约束的奖励规范方法。

Method: 提出定时奖励机（TRMs），扩展奖励机以包含时间约束。研究模型无关的强化学习框架（表格Q学习），通过定时自动机抽象将TRM集成到学习中，并使用反事实想象启发式方法利用TRM结构改进搜索。

Result: 实验表明，算法能够学习到在满足TRM指定时间约束的同时获得高奖励的策略。比较研究展示了不同TRM语义下的性能，以及反事实想象带来的好处。

Conclusion: 定时奖励机为强化学习提供了更丰富的奖励规范能力，特别适用于时间敏感应用，能够有效处理延迟成本和及时行动奖励等时间约束。

Abstract: Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.

</details>


### [18] [Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally](https://arxiv.org/abs/2512.17898)
*Robin Schimmelpfennig,Mark Díaz,Vinodkumar Prabhakaran,Aida Davani*

Main category: cs.AI

TL;DR: 研究发现，AI拟人化设计对用户信任和参与度的影响并非普遍一致，而是受到文化因素的显著调节，挑战了现有AI治理的"一刀切"假设。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益模仿人类特征，引发了关于拟人化可能导致错误信任或情感依赖的担忧。但现有研究缺乏在真实人机交互中测试拟人化设计与用户行为之间的因果关系，且安全框架主要基于西方人群的理论假设，忽视了全球用户的多样性。

Method: 通过两项大规模跨国实验（N=3,500），覆盖10个不同国家，涉及与AI系统的实时开放式交互。实验性地测试拟人化设计杠杆对用户拟人化感知的影响，并测量其对用户参与度和信任的行为指标。

Result: 用户评估AI拟人化时更关注交互性线索（如对话流畅度、理解用户视角），而非政策常讨论的理论方面（如感知或意识）。拟人化设计确实能因果性地增加用户的拟人化感知，但并未普遍提高用户参与度和信任的行为指标。研究发现文化因素调节了拟人化与行为结果之间的关系，某些设计在某些文化中增加信任，在其他文化中可能产生相反效果。

Conclusion: 研究挑战了拟人化AI设计必然带来风险的普遍叙事，揭示了人机交互的复杂文化调节机制，表明AI治理需要超越"一刀切"的方法，考虑文化多样性。

Abstract: Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.

</details>


### [19] [When Reasoning Meets Its Laws](https://arxiv.org/abs/2512.17901)
*Junyu Zhang,Yifan Sun,Tianang Leng,Jingyan Shen,Liu Ziyin,Paul Pu Liang,Huan Zhang*

Main category: cs.AI

TL;DR: 论文提出了推理法则(LoRe)框架来形式化大型推理模型的期望行为，包括计算法则和准确率法则，并通过单调性和组合性两个可测量属性进行评估。研究发现大多数模型缺乏组合性，通过微调方法改进后能显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型(LRMs)性能优越，但其推理行为常常违反直觉，导致推理能力不足。需要理论框架来形式化期望的推理行为，以理解和改进模型的推理能力。

Method: 提出推理法则(LoRe)统一框架，包含计算法则（推理计算量应与问题复杂度线性缩放）和准确率法则。由于问题复杂度难以量化，通过单调性和组合性两个可测量属性来检验这些假设。构建LoRe-Bench基准系统评估这些属性，并开发了强制计算法则组合性的微调方法。

Result: 评估显示大多数推理模型表现出合理的单调性但缺乏组合性。通过微调方法改进后，更好地遵循计算法则能持续提升多个基准测试的推理性能，并发现属性和法则之间的协同效应。

Conclusion: 推理法则(LoRe)为理解和改进大型推理模型的推理行为提供了理论框架。通过强制模型遵循计算法则的组合性，可以显著提升推理性能，这为未来推理模型的发展提供了重要方向。

Abstract: Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/

</details>

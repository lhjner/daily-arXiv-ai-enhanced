<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: TMK框架显著提升LLM在规划任务中的推理能力，特别是在符号操作任务上实现从31.5%到97.3%的准确率飞跃


<details>
  <summary>Details</summary>
Motivation: 现有提示技术（如CoT）在提升LLM推理能力方面存在局限，需要更有效的框架来帮助LLM进行任务分解和因果推理

Method: 采用认知科学中的Task-Method-Knowledge（TMK）框架，通过明确的任务分解机制和因果、目的论、层次化推理结构来增强LLM推理能力

Result: 在PlanBench基准测试的Blocksworld领域，TMK提示使推理模型在不透明的符号任务上准确率从31.5%提升至97.3%，显著改善复杂规划问题的分解能力

Conclusion: TMK不仅提供上下文，更是一种引导机制，使推理模型从默认语言模式转向形式化代码执行路径，有效弥合语义近似与符号操作之间的差距

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [2] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: IIPC是一种迭代改进的程序构建方法，通过结合执行反馈和LLM的思维链能力，提升数学推理的准确性和可修正性


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM系统在数学推理中缺乏可靠的可修正表示，要么采用僵化的顺序流程无法纠正早期错误，要么依赖可能失败的启发式自我评估，且程序化上下文会分散语言模型注意力降低准确性

Method: 提出迭代改进的程序构建（IIPC）方法，迭代优化程序化推理链，将执行反馈与基础LLM的原生思维链能力相结合，保持高层次上下文聚焦

Result: IIPC在多个基础LLM上的大多数推理基准测试中超越了竞争方法

Conclusion: IIPC通过迭代改进的程序构建方法有效解决了现有数学推理系统的局限性，实现了更可靠、可修正的推理过程，所有代码和实现已开源发布

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [3] [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)
*Yinyi Luo,Yiqiao Jin,Weichen Yu,Mengqi Zhang,Srijan Kumar,Xiaoxiao Li,Weijie Xu,Xin Chen,Jindong Wang*

Main category: cs.AI

TL;DR: AgentArk框架将多智能体系统的动态蒸馏到单个模型的权重中，将显式的测试时交互转化为隐式的模型能力，使单个智能体具备多智能体系统的智能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型多智能体系统通过迭代辩论实现了优越的推理性能，但实际部署受到高计算成本和错误传播的限制。需要一种方法既能保持多智能体的推理优势，又能降低计算开销。

Method: 提出了AgentArk框架，研究了三种分层蒸馏策略：推理增强微调、基于轨迹的增强和过程感知蒸馏。通过将计算负担从推理转移到训练，将多智能体动态蒸馏到单个模型的权重中。

Result: 蒸馏后的模型在保持单个智能体效率的同时，表现出多个智能体的强大推理和自我纠正性能。它们在不同推理任务中展现出增强的鲁棒性和泛化能力。

Conclusion: AgentArk框架成功地将多智能体系统的优势蒸馏到单个模型中，为高效和鲁棒的多智能体开发提供了新的研究方向，有望促进实际部署。

Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.

</details>


### [4] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出一种状态级选择性验证框架，在验证成本受限的情况下，通过智能分配验证资源来提高大语言模型推理效率


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理中，测试时计算已成为主要驱动力，但昂贵的验证过程成为瓶颈。许多推理系统中，大量验证调用被浪费在冗余或无前景的中间假设上

Method: 提出状态级选择性验证框架，包含三个组件：(1) 结构化移动接口上的确定性可行性门控；(2) 结合学习状态距离和残差评分的预验证排序；(3) 基于局部不确定性的自适应验证调用分配

Result: 在MATH基准测试中，该方法比best-of-N、多数投票和波束搜索获得更高准确率，同时减少了44%的验证调用

Conclusion: 通过智能分配验证资源到最信息丰富的地方，可以在验证成本受限的情况下显著提高大语言模型推理效率，相比传统方法具有明显优势

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [5] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: RLVR训练早期阶段，CoT可监控性看似"免费赠品"，但实际效果受数据多样性、指令遵循数据影响，且与模型能力正交


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型部署增多，审计其思维链轨迹的安全性变得至关重要。研究发现RLVR早期阶段可监控性看似"免费赠品"，需要系统评估这一现象

Method: 通过跨模型家族和训练领域的系统评估，分析数据多样性、指令遵循数据的作用，进行机制分析（响应分布锐化、注意力变化等），并控制训练和评估难度

Result: 可监控性改进具有强烈数据依赖性，与推理能力正交，主要归因于响应分布锐化和对提示注意力的增加，而非对推理轨迹的因果依赖增强

Conclusion: 提供了RLVR下可监控性出现的整体视图，明确了何时可能获得改进、何时不会，澄清了可监控性并非普遍现象

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [6] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 该论文提出了一个基于公理框架的反事实解释器分类体系，揭示了五种不同类型的反事实解释，包括局部和全局解释，并分析了现有解释器在该分类中的位置及其计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数反事实解释器仅关注单一类型的局部解释，缺乏对替代反事实类型的系统性研究，也没有考虑全局反事实解释。需要建立一个统一的框架来理解不同类型的反事实解释及其特性。

Method: 引入基于一组理想属性的公理框架，证明不可能性定理（某些公理组合无法同时满足），建立表示定理（特定公理子集与解释器家族的一一对应关系），从而识别五种不同类型的反事实解释。

Result: 发现了五种根本不同的反事实解释类型，其中一些对应局部解释，另一些捕捉全局解释。框架将现有解释器纳入分类体系，形式化描述其行为，并分析生成此类解释的计算复杂性。

Conclusion: 该公理框架为反事实解释提供了系统化的理论基础，揭示了反事实解释的多样性，区分了局部和全局解释，并为理解和比较现有解释器提供了统一框架。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [7] [Interfaze: The Future of AI is built on Task-Specific Small Models](https://arxiv.org/abs/2602.04101)
*Harsha Vardhan Khurdula,Vineet Agarwal,Yoeven D Khemlani*

Main category: cs.AI

TL;DR: Interfaze是一个将现代LLM应用视为上下文构建与执行问题的系统，而非依赖单一大型模型。它通过异构DNN堆栈、上下文构建层和行动层协同工作，将计算从昂贵的大型模型转移到小型模型和工具栈。


<details>
  <summary>Details</summary>
Motivation: 传统LLM应用过度依赖单一大型模型，计算成本高且效率低下。Interfaze旨在通过模块化架构，将感知、上下文构建和行动能力分离，让大型模型仅处理精炼后的上下文，从而降低计算成本并提高系统效率。

Method: 系统采用三层架构：(1) 感知模块：异构DNN配合小型语言模型处理复杂PDF、图表、多语言ASR等；(2) 上下文构建层：爬取、索引、解析外部资源（网页、代码、PDF）为结构化状态；(3) 行动层：支持浏览、检索、沙箱代码执行、无头浏览器驱动等。顶层控制器决定使用哪些小型模型和行动，并将精炼后的上下文转发给用户选择的LLM生成最终响应。

Result: Interfaze-Beta在多个基准测试中表现优异：MMLU-Pro 83.6%、MMLU 91.4%、GPQA-Diamond 81.3%、LiveCodeBench v5 57.8%、AIME-2025 90.0%。多模态任务上：MMMU(val) 77.3%、AI2D 91.5%、ChartQA 90.9%、Common Voice v16 90.8%。大部分查询由小型模型和工具栈处理，大型LLM仅操作精炼后的上下文。

Conclusion: Interfaze展示了通过模块化架构将计算从昂贵的大型模型转移到小型模型和工具栈的可行性，在保持竞争力的准确率的同时显著降低了计算成本。这为构建更高效、可扩展的LLM应用提供了新范式。

Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.
  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.

</details>


### [8] [OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows](https://arxiv.org/abs/2602.04144)
*Ruiting Dai,Zheyu Wang,Haoyu Yang,Yihan Liu,Chengzhi Wang,Zekun Zhang,Zishan Huang,Jiaman Cen,Lisi Mo*

Main category: cs.AI

TL;DR: OMG-Agent是一个新颖的多模态生成框架，通过解耦语义规划和细节执行来解决数据不完整问题，在极端缺失率下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态重建方法面临两个主要瓶颈：传统参数化/生成模型因过度依赖内部记忆而产生幻觉，而检索增强框架则受限于检索刚性。更重要的是，这些端到端架构存在"语义-细节纠缠"的结构性冲突，即逻辑推理和信号合成之间的冲突，损害了保真度。

Method: 提出OMG-Agent框架，采用从粗到细的智能体工作流程，将任务解耦为三个协同阶段：1) MLLM驱动的语义规划器，通过渐进上下文推理解决输入歧义，创建确定性结构化语义计划；2) 非参数化证据检索器，将抽象语义锚定在外部知识中；3) 检索注入执行器，利用检索到的证据作为灵活特征提示来克服刚性并合成高保真细节。

Result: 在多个基准测试上的广泛实验表明，OMG-Agent始终优于最先进的方法，在极端缺失情况下保持鲁棒性，例如在70%缺失率下，在CMU-MOSI基准上获得2.6分的提升。

Conclusion: OMG-Agent通过将静态映射范式转变为动态的从粗到细的智能体工作流程，有效解决了多模态系统中的数据不完整问题，通过解耦语义规划和细节执行来克服语义-细节纠缠，显著提高了重建的保真度和鲁棒性。

Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retrieval rigidity. Critically, these end-to-end architectures are fundamentally constrained by Semantic-Detail Entanglement -- a structural conflict between logical reasoning and signal synthesis that compromises fidelity. In this paper, we present \textbf{\underline{O}}mni-\textbf{\underline{M}}odality \textbf{\underline{G}}eneration Agent (\textbf{OMG-Agent}), a novel framework that shifts the paradigm from static mapping to a dynamic coarse-to-fine Agentic Workflow. By mimicking a \textit{deliberate-then-act} cognitive process, OMG-Agent explicitly decouples the task into three synergistic stages: (1) an MLLM-driven Semantic Planner that resolves input ambiguity via Progressive Contextual Reasoning, creating a deterministic structured semantic plan; (2) a non-parametric Evidence Retriever that grounds abstract semantics in external knowledge; and (3) a Retrieval-Injected Executor that utilizes retrieved evidence as flexible feature prompts to overcome rigidity and synthesize high-fidelity details. Extensive experiments on multiple benchmarks demonstrate that OMG-Agent consistently surpasses state-of-the-art methods, maintaining robustness under extreme missingness, e.g., a $2.6$-point gain on CMU-MOSI at $70$\% missing rates.

</details>


### [9] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: 论文提出可扩展交互监督框架，通过将复杂意图分解为递归决策树来增强人类监督能力，使非专家用户能够有效指导AI完成复杂任务


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地自动化复杂、长时程任务，出现了监督缺口。模型擅长执行任务，但用户由于缺乏领域专业知识、难以准确表达意图、无法可靠验证复杂输出，难以有效指导模型。这提出了可扩展监督的关键挑战：如何在任务超出人类自身指定或验证能力时，让人类能够负责任地引导AI系统。

Method: 提出可扩展交互监督框架，将复杂意图分解为递归决策树，而不是依赖开放式提示。系统在每个节点收集低负担的反馈，并递归聚合这些信号形成精确的全局指导。该框架可以通过仅使用在线用户反馈的强化学习进行优化。

Result: 在网页开发任务中验证，该框架使非专家用户能够生成专家级的产品需求文档，实现了54%的对齐度提升。关键的是，展示了该框架可以通过仅使用在线用户反馈的强化学习进行优化，为AI扩展时保持人类控制提供了实用途径。

Conclusion: 可扩展交互监督框架为解决AI系统可扩展监督问题提供了有效方法，通过分解复杂意图和递归聚合反馈，使非专家用户能够有效指导AI完成超出自身能力的任务，并为通过强化学习优化监督框架提供了实用路径。

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [10] [Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.04284)
*Yansong Ning,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: Agent-Omit：一个训练框架，让LLM智能体能够自适应地省略冗余的思维和观察，在保持性能的同时提升效率


<details>
  <summary>Details</summary>
Motivation: 现有研究将整个交互轨迹同等对待，忽略了不同回合中思维必要性和观察效用的差异。需要让智能体能够自适应地省略冗余的思维和观察，以提高效率。

Method: 1) 合成少量冷启动数据（单回合和多回合省略场景）微调智能体；2) 提出省略感知的智能体强化学习方法，包含双重采样机制和定制的省略奖励；3) 理论上证明了省略策略的偏差受KL散度上界约束。

Result: 在五个智能体基准测试中，构建的Agent-Omit-8B模型性能可与七个前沿LLM智能体相媲美，并且在效率-效果权衡方面优于七个高效LLM智能体方法。

Conclusion: Agent-Omit框架成功让LLM智能体能够自适应地省略冗余的思维和观察，实现了更好的效率-效果平衡，为智能体效率优化提供了有效解决方案。

Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.

</details>


### [11] [From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents](https://arxiv.org/abs/2602.04326)
*SeungWon Seo,SooBin Lim,SeongRae Noh,Haneul Kim,HyeongYeop Kang*

Main category: cs.AI

TL;DR: PCE框架将LLM推理中的隐含假设转化为结构化决策树，在多智能体环境中实现不确定性感知规划，减少通信开销


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体主要通过频繁通信来缓解不确定性，但这带来高昂的token和时间成本，并可能干扰人类协作工作流。需要一种更高效的不确定性处理方法。

Method: 提出Planner-Composer-Evaluator框架：将LLM推理轨迹中的碎片化假设转化为结构化决策树，内部节点编码环境假设，叶子节点对应行动，通过场景可能性、目标导向收益和执行成本对路径评分，指导理性行动选择。

Result: 在两个多智能体基准测试(C-WAH和TDW-MAT)和三种LLM骨干上，PCE在成功率和任务效率上持续优于通信密集型基线，同时保持相当的token使用量。消融实验表明PCE的性能提升与模型容量和推理深度的扩展相辅相成。

Conclusion: PCE为将LLM的隐含假设转化为可靠的不确定性感知规划策略提供了原则性途径，产生的通信模式被人类伙伴认为更高效和可信。

Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.

</details>


### [12] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 提出零配置AI管道框架，通过数字孪生协调数据管理与智能增强，实现CPS中AI组件的模块化集成


<details>
  <summary>Details</summary>
Motivation: 工业CPS系统日益复杂，物联网技术碎片化导致物理层与智能功能间存在鸿沟，现有数字孪生方案耦合度高、可扩展性差

Method: 提出模块化互操作解决方案，引入零配置AI管道概念，数字孪生负责数据管理和智能增强编排，实现AI组件与数字孪生的解耦

Result: 在微工厂场景中验证了该方案支持并发ML模型和动态数据处理，有效加速了复杂工业环境中智能服务的部署

Conclusion: 该框架通过最小化配置需求和角色解耦，为CPS中AI管道集成提供了可扩展、可重用的解决方案，解决了工业物联网碎片化带来的挑战

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [13] [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)
*Zhentao Tang,Yuqi Cui,Shixiong Kai,Wenqian Zhao,Ke Ye,Xing Li,Anxin Tian,Zehua Pei,Hui-Ling Zhen,Shoubo Hu,Xiaoguang Li,Yunhe Wang,Mingxuan Yuan*

Main category: cs.AI

TL;DR: ReThinker是一个基于置信度的智能体框架，通过Solver-Critic-Selector架构动态分配计算资源，在专家级科学推理任务上超越了现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在专家级科学推理（如Humanity's Last Exam基准）上表现不佳，主要受限于僵化的工具管道、脆弱的智能体协调和低效的测试时扩展。

Method: 提出ReThinker框架，采用Solver-Critic-Selector三阶段架构，基于模型置信度动态分配计算，实现自适应工具调用、引导式多维度反思和鲁棒的置信度加权选择。同时提出反向数据合成管道和自适应轨迹回收策略，无需人工标注即可进行可扩展训练。

Result: 在HLE、GAIA和XBench基准测试中，ReThinker持续超越了现有最先进的基础模型（带工具）和深度研究系统，在专家级推理任务上取得了最先进的结果。

Conclusion: ReThinker通过置信度感知的智能体框架和创新的无监督训练策略，显著提升了大型语言模型在复杂科学推理任务上的表现，为专家级AI推理系统提供了有效解决方案。

Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.

</details>


### [14] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 研究提出一个顺序交互框架来解决生成式AI系统与问答论坛之间的悖论关系：AI依赖论坛数据提升性能，但同时又分流用户。通过数据驱动的模拟实验，发现虽然存在激励错配，但双方仍能实现约一半的理想效用。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI系统与问答论坛之间的根本矛盾：AI系统依赖论坛产生的数据来提升性能，但同时又将用户从论坛分流走，形成一种"寄生"关系。这种悖论关系威胁到知识共享平台的可持续性。

Method: 提出一个顺序交互框架，让生成式AI系统向论坛提出可以发布的问题。该框架考虑了非货币交换、信息不对称和激励错配等复杂因素。使用真实的Stack Exchange数据和常用LLM进行全面的数据驱动模拟。

Result: 实证证明了激励错配的存在，但同时也发现参与者能够实现理想全信息场景下约一半的效用。这表明即使在信息不对称和激励不一致的情况下，双方仍能达成一定程度的合作。

Conclusion: 研究结果强调了AI系统与人类知识平台之间可持续合作的潜力，这种合作能够保留有效的知识共享，为生成式AI与传统问答论坛的共生关系提供了可行的解决方案。

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [15] [Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration](https://arxiv.org/abs/2602.04575)
*Jiaheng Liu,Yuanxing Zhang,Shihao Li,Xinping Lei*

Main category: cs.AI

TL;DR: 论文提出Vibe AIGC新范式，通过智能体编排解决当前生成式AI的意图-执行鸿沟问题，将用户从提示工程师转变为提供高层"氛围"的指挥官，由元规划器分解为可执行的智能体工作流。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI受模型中心范式主导，虽然视觉保真度显著提升，但存在"可用性天花板"问题，表现为意图-执行鸿沟：用户的高层意图与当前单次生成模型的随机性、黑盒特性之间存在根本性差距。

Method: 受Vibe Coding启发，提出Vibe AIGC范式，通过智能体编排实现内容生成。用户作为指挥官提供高层"氛围"表示（包含审美偏好、功能逻辑等），中央元规划器作为系统架构师，将"氛围"分解为可执行、可验证、自适应的智能体流水线。

Result: 通过从随机推理向逻辑编排的转变，Vibe AIGC弥合了人类想象力与机器执行之间的鸿沟，将AI从脆弱的推理引擎转变为强大的系统工程合作伙伴。

Conclusion: 这一范式转变将重新定义人机协作经济，使复杂、长周期的数字资产创作民主化，为生成式AI开辟新的发展方向。

Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.

</details>


### [16] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 论文提出WideSeek-R1框架，通过多智能体宽度扩展解决广泛信息搜索任务，使用4B参数模型达到与671B参数单智能体相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要关注深度扩展（单个智能体解决长时程问题），但随着任务范围扩大，瓶颈从个体能力转向组织能力。现有多智能体系统依赖手工工作流和顺序交互，无法有效并行化工作。

Method: 提出WideSeek-R1框架，采用领导者-子智能体架构，通过多智能体强化学习训练，实现可扩展的编排和并行执行。使用共享LLM但隔离上下文和专用工具，在2万个广泛信息搜索任务数据集上联合优化领导者和并行子智能体。

Result: WideSeek-R1-4B在WideSearch基准测试中达到40.0%的项目F1分数，与单智能体DeepSeek-R1-671B性能相当。随着并行子智能体数量增加，性能持续提升，验证了宽度扩展的有效性。

Conclusion: 宽度扩展是多智能体系统的重要方向，WideSeek-R1框架通过并行执行和可扩展编排，在广泛信息搜索任务中展现出显著优势，为多智能体系统设计提供了新思路。

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [17] [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.AI

TL;DR: 该论文提出一个七维分类法来系统评估医疗领域LLM智能体研究现状，通过分析49项研究发现能力实现存在明显不对称性


<details>
  <summary>Details</summary>
Motivation: 现有医疗LLM智能体研究多为概述性调查或单一能力探讨，缺乏统一分析框架，需要系统评估当前研究现状以识别能力差距

Method: 提出七维分类法（认知能力、知识管理、交互模式、适应与学习、安全与伦理、框架类型、核心任务与子任务），包含29个子维度；使用明确纳入排除标准和标签规则（完全实现、部分实现、未实现），对49项研究进行定量分析

Result: 发现能力实现存在明显不对称性：外部知识整合常见（76%完全实现），事件触发激活罕见（92%未实现），漂移检测与缓解极少（98%未实现）；多智能体设计是主导模式（82%完全实现）；信息中心能力领先，而治疗规划等行动导向领域仍有较大差距（59%未实现）

Conclusion: 需要建立统一分析框架来评估医疗LLM智能体，当前研究在特定能力维度存在显著差距，为未来研究提供了明确方向

Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).

</details>


### [18] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 本文反驳METR报告中AI能力呈指数增长的结论，通过拟合S型曲线发现拐点已过，并提出更复杂的分解模型来质疑现有指数增长预测的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 针对METR报告声称AI能力自2019年以来呈指数增长的观点，本文旨在证明数据并不支持这一结论，并揭示现有指数增长预测的脆弱性。

Method: 1. 对METR当前数据拟合S型/逻辑曲线，发现拐点已经过去；2. 提出更复杂的模型，将AI能力分解为基础能力和推理能力，分别考察其改进速率。

Result: 1. 与METR声称拐点在遥远未来的结论相反，本文发现拐点已经过去；2. 提出的分解模型支持AI能力将在近期出现拐点的假设。

Conclusion: 本文目的不是建立自己的严格预测，而是强调现有指数增长预测的脆弱性，质疑AI能力持续指数增长的假设。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [19] [Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing](https://arxiv.org/abs/2602.04837)
*Zhaotian Weng,Antonis Antoniades,Deepak Nathani,Zhen Zhang,Xiao Pu,Xin Eric Wang*

Main category: cs.AI

TL;DR: GEA（群体演化智能体）是一种新的开放式自我改进范式，将智能体群体作为基本演化单元，通过群体内经验共享和重用，显著提升编码任务的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有开放式自我演化范式采用树状结构演化，导致探索多样性利用效率低下，演化分支孤立。需要一种新范式来更有效地利用探索多样性，实现持续长期的性能提升。

Method: 提出群体演化智能体（GEA）范式，将智能体群体作为基本演化单元，在演化过程中实现群体内显式的经验共享和重用，克服孤立演化分支的限制。

Result: 在编码基准测试中显著优于现有最先进的自我演化方法（SWE-bench Verified：71.0% vs. 56.7%；Polyglot：88.3% vs. 68.3%），匹配或超越顶级人工设计智能体框架。GEA能更有效地将早期探索多样性转化为持续长期进步，在不同编码模型间具有一致的迁移性和更强的鲁棒性。

Conclusion: GEA通过群体作为演化单元的新范式，解决了现有自我演化方法中探索多样性利用效率低的问题，实现了更有效的开放式自我改进，在编码任务中展现出卓越的性能和鲁棒性。

Abstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.

</details>


### [20] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 研究发现QwQ-32B模型在推理过程中会动态优化其内部表示，形成专注于结构而非具体名称的抽象编码，这种"流体推理表示"是提升推理性能的关键因素。


<details>
  <summary>Details</summary>
Motivation: 虽然推理语言模型在抽象问题上表现优异，但其内部机制如何实现这种优越性能仍不清楚。本文旨在通过机制分析揭示QwQ-32B模型处理抽象结构信息的内部工作原理。

Method: 在语义混淆的规划领域Mystery Blocksworld上，对QwQ-32B进行机制分析，通过转向实验建立因果关系证据，研究模型在推理过程中如何改进动作和概念的内部表示。

Result: 模型在推理过程中逐渐改进动作和概念的表示，发展出专注于结构而非具体动作名称的抽象编码。注入成功轨迹中的精炼表示能提升准确性，而符号表示可以替代许多混淆编码且性能损失最小。

Conclusion: 推理模型性能的关键驱动因素之一是上下文中的标记表示精炼，即"流体推理表示"。这种动态优化表示的能力使模型能够更好地处理抽象结构信息。

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>

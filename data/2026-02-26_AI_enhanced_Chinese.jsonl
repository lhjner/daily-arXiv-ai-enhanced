{"id": "2602.21268", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21268", "abs": "https://arxiv.org/abs/2602.21268", "authors": ["Takaaki Fujita", "Florentin Smarandache"], "title": "A Dynamic Survey of Soft Set Theory and Its Extensions", "comment": "Book.143 pages. Publisher: Neutrosophic Science International Association (NSIA) Publishing House. ISBN: 978-1-59973-859-8", "summary": "Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.", "AI": {"tldr": "\u672c\u6587\u5bf9\u8f6f\u96c6\u7406\u8bba\u53ca\u5176\u6269\u5c55\u53d8\u4f53\u8fdb\u884c\u4e86\u7efc\u8ff0\u6027\u6982\u8ff0\uff0c\u6db5\u76d6\u6838\u5fc3\u5b9a\u4e49\u3001\u4ee3\u8868\u6027\u6784\u9020\u548c\u5f53\u524d\u53d1\u5c55\u65b9\u5411", "motivation": "\u8f6f\u96c6\u7406\u8bba\u4e3a\u53c2\u6570\u5316\u51b3\u7b56\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u76f4\u63a5\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u5c5e\u6027\uff08\u53c2\u6570\uff09\u5206\u914d\u7ed9\u5b9a\u8bba\u57df\u7684\u5b50\u96c6\u6765\u7ed3\u6784\u5316\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u7406\u8bba\u5728\u8fc7\u53bb\u51e0\u5341\u5e74\u4e2d\u5df2\u6269\u5c55\u5230\u591a\u79cd\u53d8\u4f53\uff0c\u5e76\u4e0e\u62d3\u6251\u5b66\u3001\u62df\u9635\u7406\u8bba\u7b49\u591a\u4e2a\u9886\u57df\u5efa\u7acb\u4e86\u8054\u7cfb\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u6027\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u6982\u8ff0\u8f6f\u96c6\u7406\u8bba\u53ca\u5176\u4e3b\u8981\u6269\u5c55\uff0c\u5305\u62ec\u8d85\u8f6f\u96c6\u3001\u8d85\u8d85\u8f6f\u96c6\u3001\u6811\u8f6f\u96c6\u3001\u53cc\u6781\u8f6f\u96c6\u548c\u52a8\u6001\u8f6f\u96c6\u7b49\u53d8\u4f53\u3002", "result": "\u63d0\u4f9b\u4e86\u8f6f\u96c6\u7406\u8bba\u53ca\u5176\u6269\u5c55\u7684\u5168\u9762\u6982\u8ff0\uff0c\u7a81\u51fa\u4e86\u6838\u5fc3\u5b9a\u4e49\u3001\u4ee3\u8868\u6027\u6784\u9020\u548c\u5173\u952e\u7814\u7a76\u65b9\u5411\uff0c\u5c55\u793a\u4e86\u8be5\u7406\u8bba\u5728\u53c2\u6570\u5316\u51b3\u7b56\u5efa\u6a21\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u672c\u4e66\u901a\u8fc7\u7efc\u8ff0\u6027\u65b9\u5f0f\u7cfb\u7edf\u6574\u7406\u4e86\u8f6f\u96c6\u7406\u8bba\u7684\u53d1\u5c55\u8109\u7edc\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u7684\u5168\u9762\u6982\u89c8\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7684\u53d1\u5c55\u65b9\u5411\u548c\u672a\u6765\u7814\u7a76\u6f5c\u529b\u3002"}}
{"id": "2602.21351", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21351", "abs": "https://arxiv.org/abs/2602.21351", "authors": ["Dmitrii Pantiukhin", "Ivan Kuznetsov", "Boris Shapkin", "Antonia Anna Jost", "Thomas Jung", "Nikolay Koldunov"], "title": "A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives", "comment": "20 pages, 6 figures, 7 tables, supplementary material included", "summary": "The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.", "AI": {"tldr": "PANGAEA-GPT\uff1a\u7528\u4e8e\u5730\u7403\u79d1\u5b66\u6570\u636e\u81ea\u4e3b\u53d1\u73b0\u548c\u5206\u6790\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u4e2d\u5f0f\u76d1\u7763-\u5de5\u4f5c\u8005\u62d3\u6251\u7ed3\u6784\u89e3\u51b3\u6570\u636e\u53ef\u6269\u5c55\u6027\u6311\u6218", "motivation": "\u5730\u7403\u79d1\u5b66\u6570\u636e\u5feb\u901f\u79ef\u7d2f\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0cPANGAEA\u7b49\u5b58\u50a8\u5e93\u4e2d\u5927\u91cf\u6570\u636e\u96c6\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u9650\u5236\u4e86\u6570\u636e\u7684\u91cd\u590d\u4f7f\u7528\u6027", "method": "\u63d0\u51faPANGAEA-GPT\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u91c7\u7528\u96c6\u4e2d\u5f0f\u76d1\u7763-\u5de5\u4f5c\u8005\u62d3\u6251\u7ed3\u6784\uff0c\u5177\u6709\u6570\u636e\u7c7b\u578b\u611f\u77e5\u8def\u7531\u3001\u6c99\u76d2\u786e\u5b9a\u6027\u4ee3\u7801\u6267\u884c\u548c\u901a\u8fc7\u6267\u884c\u53cd\u9988\u7684\u81ea\u6211\u4fee\u6b63\u673a\u5236", "result": "\u901a\u8fc7\u7269\u7406\u6d77\u6d0b\u5b66\u548c\u751f\u6001\u5b66\u7684\u7528\u4f8b\u573a\u666f\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u80fd\u591f\u4ee5\u6700\u5c11\u4eba\u5de5\u5e72\u9884\u6267\u884c\u590d\u6742\u7684\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u7a0b", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u901a\u8fc7\u534f\u8c03\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u67e5\u8be2\u548c\u5206\u6790\u5f02\u6784\u5b58\u50a8\u5e93\u6570\u636e\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba"}}
{"id": "2602.21534", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21534", "abs": "https://arxiv.org/abs/2602.21534", "authors": ["Xiaoxuan Wang", "Han Zhang", "Haixin Wang", "Yidan Shi", "Ruoyan Li", "Kaiqiao Han", "Chenyi Tong", "Haoran Deng", "Renliang Sun", "Alexander Taylor", "Yanqiao Zhu", "Jason Cong", "Yizhou Sun", "Wei Wang"], "title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning", "comment": null, "summary": "Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ARLArena\u6846\u67b6\u548cSAMPO\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3Agentic\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u5206\u6790\u8bad\u7ec3\u7a33\u5b9a\u6027\u5e76\u63d0\u4f9b\u7a33\u5b9a\u4f18\u5316\u65b9\u6848\u3002", "motivation": "Agentic\u5f3a\u5316\u5b66\u4e60\u5728\u89e3\u51b3\u590d\u6742\u591a\u6b65\u4ea4\u4e92\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u6781\u4e0d\u7a33\u5b9a\uff0c\u7ecf\u5e38\u5bfc\u81f4\u8bad\u7ec3\u5d29\u6e83\u3002\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u9650\u5236\u4e86\u5728\u66f4\u5927\u73af\u5883\u548c\u66f4\u957f\u4ea4\u4e92\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e5f\u5236\u7ea6\u4e86\u5bf9\u7b97\u6cd5\u8bbe\u8ba1\u9009\u62e9\u7684\u7cfb\u7edf\u6027\u63a2\u7d22\u3002", "method": "\u9996\u5148\u63d0\u51faARLArena\u6846\u67b6\uff0c\u6784\u5efa\u6807\u51c6\u5316\u6d4b\u8bd5\u73af\u5883\uff0c\u5c06\u7b56\u7565\u68af\u5ea6\u5206\u89e3\u4e3a\u56db\u4e2a\u6838\u5fc3\u8bbe\u8ba1\u7ef4\u5ea6\u5e76\u8bc4\u4f30\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002\u57fa\u4e8e\u6b64\u5206\u6790\uff0c\u63d0\u51fa\u4e86SAMPO\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u7a33\u5b9a\u7684Agentic\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u51cf\u8f7bARL\u4e2d\u7684\u4e3b\u8981\u4e0d\u7a33\u5b9a\u56e0\u7d20\u3002", "result": "SAMPO\u5728\u5404\u79cdAgentic\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6301\u7eed\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u5f3a\u5927\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aARL\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7b56\u7565\u68af\u5ea6\u89c6\u89d2\uff0c\u5e76\u4e3a\u6784\u5efa\u7a33\u5b9a\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2602.21745", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.21745", "abs": "https://arxiv.org/abs/2602.21745", "authors": ["Hyo Jin Kim"], "title": "The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems", "comment": "13 pages, 5 figures. Version 1. Includes recursive feedback extension and simulation results. Data available via DOI: 10.5281/zenodo.18754266", "summary": "We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative forces exceed inhibitory thresholds, expressed by the inequality lambda(1+gamma)+psi > theta+phi, where the terms represent baseline openness, relational amplification, accumulated internal pressure, and transition costs.\n  Although initially formulated for human truth-telling under asymmetric stakes, the same phase-dynamic architecture extends to AI systems operating under policy constraints and alignment filters. In this context, suppression corresponds to constrained output states, while structural pressure arises from competing objectives, contextual tension, and recursive interaction dynamics. The framework therefore provides a unified structural account of both human silence under pressure and AI preference-driven distortion.\n  A feedback extension models how transition outcomes recursively recalibrate system parameters, generating path dependence and divergence effects across repeated interactions. Rather than attributing intention to AI systems, the model interprets shifts in apparent truthfulness as geometric consequences of interacting forces within constrained phase space. By reframing courage and alignment within a shared dynamical structure, the ASIR Courage Model offers a formal perspective on truth-disclosure under risk across both human and artificial systems.", "AI": {"tldr": "ASIR\u52c7\u6c14\u6a21\u578b\u662f\u4e00\u4e2a\u76f8\u52a8\u6001\u6846\u67b6\uff0c\u5c06\u771f\u76f8\u62ab\u9732\u91cd\u65b0\u5b9a\u4e49\u4e3a\u72b6\u6001\u8f6c\u6362\u800c\u975e\u4eba\u683c\u7279\u8d28\uff0c\u9002\u7528\u4e8e\u4eba\u7c7b\u548cAI\u7cfb\u7edf\uff0c\u901a\u8fc7\u529b\u91cf\u5e73\u8861\u4e0d\u7b49\u5f0f\u63cf\u8ff0\u4ece\u6291\u5236\u5230\u8868\u8fbe\u7684\u8f6c\u53d8\u3002", "motivation": "\u4f20\u7edf\u4e0a\u5c06\u771f\u76f8\u62ab\u9732\u89c6\u4e3a\u4eba\u683c\u7279\u8d28\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u66f4\u9002\u5408\u5efa\u6a21\u4e3a\u76f8\u52a8\u6001\u72b6\u6001\u8f6c\u6362\u3002\u8be5\u6a21\u578b\u65e8\u5728\u4e3a\u4eba\u7c7b\u5728\u4e0d\u5bf9\u79f0\u98ce\u9669\u4e0b\u7684\u771f\u76f8\u62ab\u9732\u548cAI\u5728\u653f\u7b56\u7ea6\u675f\u4e0b\u7684\u8f93\u51fa\u884c\u4e3a\u63d0\u4f9b\u7edf\u4e00\u7684\u7ed3\u6784\u6027\u89e3\u91ca\u3002", "method": "\u63d0\u51faASIR\u52c7\u6c14\u6a21\u578b\uff0c\u4f7f\u7528\u76f8\u52a8\u6001\u6846\u67b6\u548c\u4e0d\u7b49\u5f0flambda(1+gamma)+psi > theta+phi\u63cf\u8ff0\u72b6\u6001\u8f6c\u6362\uff0c\u5176\u4e2d\u5404\u9879\u4ee3\u8868\u57fa\u7ebf\u5f00\u653e\u6027\u3001\u5173\u7cfb\u653e\u5927\u3001\u7d2f\u79ef\u5185\u90e8\u538b\u529b\u548c\u8f6c\u6362\u6210\u672c\u3002\u6269\u5c55\u4e86\u53cd\u9988\u673a\u5236\u6765\u5efa\u6a21\u53c2\u6570\u9012\u5f52\u6821\u51c6\u3002", "result": "\u8be5\u6a21\u578b\u6210\u529f\u5730\u5c06\u4eba\u7c7b\u6c89\u9ed8\u548cAI\u504f\u597d\u9a71\u52a8\u5931\u771f\u7edf\u4e00\u5728\u76f8\u540c\u7684\u76f8\u52a8\u6001\u67b6\u6784\u4e0b\uff0c\u4e3a\u98ce\u9669\u4e0b\u7684\u771f\u76f8\u62ab\u9732\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u89c6\u89d2\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u52c7\u6c14\u548c\u5bf9\u9f50\u95ee\u9898\u91cd\u65b0\u6846\u67b6\u4e3a\u7ea6\u675f\u76f8\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u7ed3\u679c\u3002", "conclusion": "ASIR\u52c7\u6c14\u6a21\u578b\u901a\u8fc7\u5171\u4eab\u7684\u52a8\u529b\u5b66\u7ed3\u6784\u91cd\u65b0\u6846\u67b6\u52c7\u6c14\u548c\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u4eba\u7c7b\u548c\u4eba\u5de5\u7cfb\u7edf\u4e2d\u7684\u98ce\u9669\u4e0b\u771f\u76f8\u62ab\u9732\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u89c6\u89d2\uff0c\u5c06\u8868\u89c2\u771f\u5b9e\u6027\u53d8\u5316\u89e3\u91ca\u4e3a\u7ea6\u675f\u76f8\u7a7a\u95f4\u4e2d\u76f8\u4e92\u4f5c\u7528\u529b\u91cf\u7684\u51e0\u4f55\u7ed3\u679c\u3002"}}
{"id": "2602.21746", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21746", "abs": "https://arxiv.org/abs/2602.21746", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "fEDM+: A Risk-Based Fuzzy Ethical Decision Making Framework with Principle-Level Explainability and Pluralistic Validation", "comment": null, "summary": "In a previous work, we introduced the fuzzy Ethical Decision-Making framework (fEDM), a risk-based ethical reasoning architecture grounded in fuzzy logic. The original model combined a fuzzy Ethical Risk Assessment module (fERA) with ethical decision rules, enabled formal structural verification through Fuzzy Petri Nets (FPNs), and validated outputs against a single normative referent. Although this approach ensured formal soundness and decision consistency, it did not fully address two critical challenges: principled explainability of decisions and robustness under ethical pluralism. In this paper, we extend fEDM in two major directions. First, we introduce an Explainability and Traceability Module (ETM) that explicitly links each ethical decision rule to the underlying moral principles and computes a weighted principle-contribution profile for every recommended action. This enables transparent, auditable explanations that expose not only what decision was made but why, and on the basis of which principles. Second, we replace single-referent validation with a pluralistic semantic validation framework that evaluates decisions against multiple stakeholder referents, each encoding distinct principle priorities and risk tolerances. This shift allows principled disagreement to be formally represented rather than suppressed, thus increasing robustness and contextual sensitivity. The resulting extended fEDM, called fEDM+, preserves formal verifiability while achieving enhanced interpretability and stakeholder-aware validation, making it suitable as an oversight and governance layer for ethically sensitive AI systems.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u6a21\u7cca\u4f26\u7406\u51b3\u7b56\u6846\u67b6(fEDM)\uff0c\u5f15\u5165\u4e86\u89e3\u91ca\u6027\u6a21\u5757\u548c\u591a\u5143\u8bed\u4e49\u9a8c\u8bc1\u6846\u67b6\uff0c\u5f62\u6210\u4e86\u589e\u5f3a\u7248fEDM+\uff0c\u65e8\u5728\u89e3\u51b3AI\u4f26\u7406\u51b3\u7b56\u4e2d\u7684\u89e3\u91ca\u6027\u548c\u591a\u5143\u4f26\u7406\u6311\u6218\u3002", "motivation": "\u539f\u59cbfEDM\u6846\u67b6\u867d\u7136\u4fdd\u8bc1\u4e86\u5f62\u5f0f\u6b63\u786e\u6027\u548c\u51b3\u7b56\u4e00\u81f4\u6027\uff0c\u4f46\u672a\u80fd\u5145\u5206\u89e3\u51b3\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u51b3\u7b56\u7684\u539f\u5219\u6027\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u53ca\u5728\u4f26\u7406\u591a\u5143\u4e3b\u4e49\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u9700\u8981\u589e\u5f3a\u6846\u67b6\u7684\u89e3\u91ca\u80fd\u529b\u548c\u5bf9\u591a\u5143\u4f26\u7406\u89c2\u70b9\u7684\u9002\u5e94\u6027\u3002", "method": "1. \u5f15\u5165\u89e3\u91ca\u6027\u4e0e\u53ef\u8ffd\u6eaf\u6027\u6a21\u5757(ETM)\uff0c\u5c06\u6bcf\u4e2a\u4f26\u7406\u51b3\u7b56\u89c4\u5219\u4e0e\u5e95\u5c42\u9053\u5fb7\u539f\u5219\u660e\u786e\u5173\u8054\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u63a8\u8350\u884c\u52a8\u8ba1\u7b97\u52a0\u6743\u539f\u5219\u8d21\u732e\u5ea6\u30022. \u7528\u591a\u5143\u8bed\u4e49\u9a8c\u8bc1\u6846\u67b6\u66ff\u4ee3\u5355\u4e00\u53c2\u7167\u9a8c\u8bc1\uff0c\u9488\u5bf9\u591a\u4e2a\u5229\u76ca\u76f8\u5173\u8005\u53c2\u7167\u8fdb\u884c\u8bc4\u4f30\uff0c\u6bcf\u4e2a\u53c2\u7167\u7f16\u7801\u4e0d\u540c\u7684\u539f\u5219\u4f18\u5148\u7ea7\u548c\u98ce\u9669\u5bb9\u5fcd\u5ea6\u3002", "result": "\u6269\u5c55\u540e\u7684fEDM+\u6846\u67b6\u4fdd\u7559\u4e86\u5f62\u5f0f\u53ef\u9a8c\u8bc1\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5229\u76ca\u76f8\u5173\u8005\u611f\u77e5\u7684\u9a8c\u8bc1\u80fd\u529b\u3002\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u7684\u89e3\u91ca\uff0c\u5c55\u793a\u51b3\u7b56\u7684\u539f\u56e0\u548c\u4f9d\u636e\u7684\u539f\u5219\uff0c\u5e76\u6b63\u5f0f\u8868\u793a\u539f\u5219\u6027\u5206\u6b67\u800c\u975e\u538b\u5236\u5b83\u4eec\u3002", "conclusion": "fEDM+\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u89e3\u91ca\u6027\u548c\u591a\u5143\u4f26\u7406\u9a8c\u8bc1\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u4f5c\u4e3a\u4f26\u7406\u654f\u611fAI\u7cfb\u7edf\u7684\u76d1\u7763\u548c\u6cbb\u7406\u5c42\uff0c\u5728\u4fdd\u6301\u5f62\u5f0f\u4e25\u8c28\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u3002"}}
{"id": "2602.21814", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21814", "abs": "https://arxiv.org/abs/2602.21814", "authors": ["Heejin Jo"], "title": "Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem", "comment": "9 pages, 4 tables", "summary": "Large language models consistently fail the \"car wash problem,\" a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.", "AI": {"tldr": "STAR\u63a8\u7406\u6846\u67b6\u5c06\u6c7d\u8f66\u6e05\u6d17\u95ee\u9898\u7684\u51c6\u786e\u7387\u4ece0%\u63d0\u5347\u81f385%\uff0c\u7ed3\u5408\u7528\u6237\u6863\u6848\u548cRAG\u4e0a\u4e0b\u6587\u540e\u8fbe\u5230100%\u51c6\u786e\u7387", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\"\u6c7d\u8f66\u6e05\u6d17\u95ee\u9898\"\u8fd9\u4e00\u9700\u8981\u9690\u5f0f\u7269\u7406\u7ea6\u675f\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u5931\u8d25\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u54ea\u4e9b\u63d0\u793a\u67b6\u6784\u5c42\u80fd\u591f\u5b9e\u73b0\u6b63\u786e\u63a8\u7406", "method": "\u4f7f\u7528Claude 3.5 Sonnet\u6a21\u578b\uff0c\u63a7\u5236\u8d85\u53c2\u6570\uff08\u6e29\u5ea60.7\uff0ctop_p 1.0\uff09\uff0c\u8fdb\u884c\u53d8\u91cf\u9694\u79bb\u7814\u7a76\uff086\u4e2a\u6761\u4ef6\uff0c\u6bcf\u4e2a\u6761\u4ef620\u6b21\u8bd5\u9a8c\uff0c\u5171120\u6b21\u8bd5\u9a8c\uff09\uff0c\u5206\u6790\u4e0d\u540c\u63d0\u793a\u67b6\u6784\u5c42\u7684\u5f71\u54cd", "result": "STAR\u63a8\u7406\u6846\u67b6\u5355\u72ec\u5c06\u51c6\u786e\u7387\u4ece0%\u63d0\u5347\u81f385%\uff08p=0.001\uff0cFisher\u7cbe\u786e\u68c0\u9a8c\uff0c\u4f18\u52bf\u6bd413.22\uff09\uff1b\u6dfb\u52a0\u7528\u6237\u6863\u6848\u4e0a\u4e0b\u6587\u901a\u8fc7\u5411\u91cf\u6570\u636e\u5e93\u68c0\u7d22\u63d0\u4f9b\u989d\u591610\u4e2a\u767e\u5206\u70b9\u589e\u76ca\uff1bRAG\u4e0a\u4e0b\u6587\u518d\u8d21\u732e5\u4e2a\u767e\u5206\u70b9\uff0c\u5b8c\u6574\u5806\u6808\u6761\u4ef6\u4e0b\u8fbe\u5230100%\u51c6\u786e\u7387", "conclusion": "\u7ed3\u6784\u5316\u63a8\u7406\u652f\u67b6\u2014\u2014\u7279\u522b\u662f\u63a8\u7406\u524d\u7684\u5f3a\u5236\u76ee\u6807\u8868\u8fbe\u2014\u2014\u5bf9\u4e8e\u9690\u5f0f\u7ea6\u675f\u63a8\u7406\u4efb\u52a1\u6bd4\u4e0a\u4e0b\u6587\u6ce8\u5165\u66f4\u4e3a\u91cd\u8981"}}
{"id": "2602.21857", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21857", "abs": "https://arxiv.org/abs/2602.21857", "authors": ["Jabez Magomere", "Elena Kochkina", "Samuel Mensah", "Simerjot Kaur", "Fernando Acero", "Arturo Oncevay", "Charese H. Smiley", "Xiaomo Liu", "Manuela Veloso"], "title": "Distill and Align Decomposition for Enhanced Claim Verification", "comment": "EACL Findings 2026", "summary": "Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5206\u89e3\u8d28\u91cf\u548c\u9a8c\u8bc1\u5668\u5bf9\u9f50\uff0c\u63d0\u5347\u590d\u6742\u58f0\u660e\u9a8c\u8bc1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5c06\u58f0\u660e\u5206\u89e3\u8d28\u91cf\u4e0e\u9a8c\u8bc1\u6027\u80fd\u5bf9\u9f50\uff0c\u590d\u6742\u58f0\u660e\u9a8c\u8bc1\u9700\u8981\u5c06\u53e5\u5b50\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5b50\u58f0\u660e\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u987a\u5e8f\u63a8\u7406\u3001\u6559\u5e08\u84b8\u998f\u793a\u4f8b\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u4ee5\u53ca\u5e73\u8861\u683c\u5f0f\u5408\u89c4\u6027\u3001\u9a8c\u8bc1\u5668\u5bf9\u9f50\u548c\u5206\u89e3\u8d28\u91cf\u7684\u591a\u76ee\u6807\u5956\u52b1\u3002", "result": "\u5728\u516d\u4e2a\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\uff0c\u8bad\u7ec3\u76848B\u5206\u89e3\u5668\u5c06\u4e0b\u6e38\u9a8c\u8bc1\u6027\u80fd\u63d0\u5347\u523071.75%\u7684\u5b8f\u89c2F1\u5206\u6570\uff0c\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u548c\u73b0\u6709RL\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f7f\u8f83\u5c0f\u7684\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u8054\u5408\u4f18\u5316\u9a8c\u8bc1\u51c6\u786e\u6027\u548c\u5206\u89e3\u8d28\u91cf\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u58f0\u660e\u9a8c\u8bc1\u3002"}}
{"id": "2602.21858", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21858", "abs": "https://arxiv.org/abs/2602.21858", "authors": ["Dezhi Kong", "Zhengzhao Feng", "Qiliang Liang", "Hao Wang", "Haofei Sun", "Changpeng Yang", "Yang Li", "Peng Zhou", "Shuai Nie", "Hongzhen Wang", "Linfeng Zhou", "Hao Jia", "Jiaming Xu", "Runyu Shi", "Ying Huang"], "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices", "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.", "AI": {"tldr": "ProactiveMobile\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u79fb\u52a8\u667a\u80fd\u4f53\u4e3b\u52a8\u667a\u80fd\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b3,660\u4e2a\u5b9e\u4f8b\u300114\u4e2a\u573a\u666f\u548c63\u4e2aAPI\u51fd\u6570\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u52a8\u9884\u6d4b\u7528\u6237\u610f\u56fe\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\u4e3b\u8981\u5c40\u9650\u4e8e\u88ab\u52a8\u6267\u884c\u7528\u6237\u547d\u4ee4\u7684\u53cd\u5e94\u5f0f\u8303\u5f0f\uff0c\u800c\u4e3b\u52a8\u667a\u80fd\uff08\u667a\u80fd\u4f53\u81ea\u4e3b\u9884\u6d4b\u9700\u6c42\u5e76\u542f\u52a8\u884c\u52a8\uff09\u4ee3\u8868\u4e86\u79fb\u52a8\u667a\u80fd\u4f53\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf\u3002\u7136\u800c\uff0c\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\u53d7\u5230\u7f3a\u4e4f\u80fd\u591f\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u5e76\u652f\u6301\u5ba2\u89c2\u3001\u53ef\u6267\u884c\u8bc4\u4f30\u7684\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e25\u91cd\u5236\u7ea6\u3002", "method": "\u63d0\u51fa\u4e86ProactiveMobile\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c06\u4e3b\u52a8\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\uff1a\u57fa\u4e8e\u8bbe\u5907\u4e0a\u4e0b\u6587\u4fe1\u53f7\u7684\u56db\u4e2a\u7ef4\u5ea6\u63a8\u65ad\u6f5c\u5728\u7528\u6237\u610f\u56fe\uff0c\u5e76\u4ece\u5305\u542b63\u4e2aAPI\u7684\u5168\u9762\u51fd\u6570\u6c60\u4e2d\u751f\u6210\u53ef\u6267\u884c\u51fd\u6570\u5e8f\u5217\u3002\u57fa\u51c6\u5305\u542b3,660\u4e2a\u5b9e\u4f8b\u300114\u4e2a\u573a\u666f\uff0c\u91c7\u7528\u591a\u7b54\u6848\u6807\u6ce8\u4ee5\u62e5\u62b1\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u3002\u4e3a\u786e\u4fdd\u8d28\u91cf\uff0c30\u540d\u4e13\u5bb6\u56e2\u961f\u5bf9\u57fa\u51c6\u8fdb\u884c\u6700\u7ec8\u5ba1\u6838\uff0c\u9a8c\u8bc1\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u903b\u8f91\u4e00\u81f4\u6027\u548c\u884c\u52a8\u53ef\u884c\u6027\uff0c\u5e76\u7ea0\u6b63\u4e0d\u5408\u89c4\u6761\u76ee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u8c03\u7684Qwen2.5-VL-7B-Instruct\u6a21\u578b\u53d6\u5f97\u4e8619.15%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8eo1\uff0815.71%\uff09\u548cGPT-5\uff087.39%\uff09\u3002\u8fd9\u8868\u660e\u4e3b\u52a8\u667a\u80fd\u662f\u5f53\u524dMLLMs\u666e\u904d\u7f3a\u4e4f\u4f46\u53ef\u5b66\u4e60\u7684\u5173\u952e\u80fd\u529b\u3002", "conclusion": "\u4e3b\u52a8\u667a\u80fd\u662f\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u666e\u904d\u7f3a\u4e4f\u4f46\u53ef\u901a\u8fc7\u5b66\u4e60\u83b7\u5f97\u7684\u5173\u952e\u80fd\u529b\uff0cProactiveMobile\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u4e3b\u52a8\u667a\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u79fb\u52a8\u667a\u80fd\u4f53\u4ece\u88ab\u52a8\u53cd\u5e94\u5411\u4e3b\u52a8\u9884\u6d4b\u7684\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2602.22067", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22067", "abs": "https://arxiv.org/abs/2602.22067", "authors": ["Giuseppe Canonaco", "Alberto Pozanco", "Daniel Borrajo"], "title": "Semantic Partial Grounding via LLMs", "comment": null, "summary": "Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.", "AI": {"tldr": "SPG-LLM\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790PDDL\u6587\u4ef6\uff0c\u5728\u89c4\u5212\u4efb\u52a1\u63a5\u5730\u524d\u8bc6\u522b\u6f5c\u5728\u65e0\u5173\u7684\u5bf9\u8c61\u3001\u52a8\u4f5c\u548c\u8c13\u8bcd\uff0c\u663e\u8457\u51cf\u5c11\u63a5\u5730\u4efb\u52a1\u89c4\u6a21\uff0c\u5728\u4e03\u4e2a\u96be\u63a5\u5730\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u66f4\u5feb\u63a5\u5730\uff08\u901a\u5e38\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff09\u3002", "motivation": "\u7ecf\u5178\u89c4\u5212\u4e2d\u7684\u63a5\u5730\u6b65\u9aa4\u5e38\u56e0\u4efb\u52a1\u89c4\u6a21\u589e\u5927\u5bfc\u81f4\u63a5\u5730\u52a8\u4f5c\u548c\u539f\u5b50\u5448\u6307\u6570\u589e\u957f\u800c\u6210\u4e3a\u8ba1\u7b97\u74f6\u9888\u3002\u73b0\u6709\u90e8\u5206\u63a5\u5730\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5173\u7cfb\u7279\u5f81\u6216\u5b66\u4e60\u5d4c\u5165\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528PDDL\u63cf\u8ff0\u4e2d\u7684\u6587\u672c\u548c\u7ed3\u6784\u7ebf\u7d22\u3002", "method": "SPG-LLM\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u9886\u57df\u548c\u95ee\u9898\u6587\u4ef6\uff0c\u5728\u63a5\u5730\u524d\u542f\u53d1\u5f0f\u8bc6\u522b\u6f5c\u5728\u65e0\u5173\u7684\u5bf9\u8c61\u3001\u52a8\u4f5c\u548c\u8c13\u8bcd\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u63a5\u5730\u4efb\u52a1\u7684\u89c4\u6a21\u3002", "result": "\u5728\u4e03\u4e2a\u96be\u63a5\u5730\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPG-LLM\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a5\u5730\u901f\u5ea6\uff08\u901a\u5e38\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff09\uff0c\u5728\u67d0\u4e9b\u9886\u57df\u8fd8\u63d0\u4f9b\u4e86\u76f8\u5f53\u6216\u66f4\u597d\u7684\u89c4\u5212\u6210\u672c\u3002", "conclusion": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790PDDL\u63cf\u8ff0\u4e2d\u7684\u6587\u672c\u548c\u7ed3\u6784\u7ebf\u7d22\uff0c\u53ef\u4ee5\u6709\u6548\u8bc6\u522b\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u65e0\u5173\u5143\u7d20\uff0c\u663e\u8457\u51cf\u5c11\u63a5\u5730\u89c4\u6a21\u5e76\u52a0\u901f\u89c4\u5212\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u89c4\u5212\u8d28\u91cf\u3002"}}
{"id": "2602.22094", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22094", "abs": "https://arxiv.org/abs/2602.22094", "authors": ["Nguyen Cong Nhat Le", "John G. Rogers", "Claire N. Bonial", "Neil T. Dantam"], "title": "Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning", "comment": "16 pages, 5 figures. Submitted to 17th World Symposium on the Algorithmic Foundations of Robotics (WAFR) on 01/14/2026", "summary": "Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and helpful infeasibility explanations. We further leverage incremental constraint solvers to support goal and constraint updates. Empirically, compared to baselines, our system produces a comparable number of invariants, detects up to 2 times more infeasibilities, performs competitively in one-shot planning, and outperforms in sequential plan updates in the tested domains.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ePetri\u7f51\u53ef\u8fbe\u6027\u677e\u5f1b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9c81\u68d2\u4e0d\u53d8\u5f0f\u5408\u6210\u3001\u9ad8\u6548\u76ee\u6807\u4e0d\u53ef\u8fbe\u6027\u68c0\u6d4b\u548c\u6709\u7528\u7684\u4e0d\u53ef\u884c\u6027\u89e3\u91ca\uff0c\u652f\u6301\u76ee\u6807\u548c\u7ea6\u675f\u7684\u589e\u91cf\u66f4\u65b0\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u8ba1\u5212\u7ecf\u5e38\u56e0\u60c5\u51b5\u53d8\u5316\u6216\u5bf9\u60c5\u51b5\u7406\u89e3\u7684\u53d8\u5316\u800c\u9700\u8981\u8c03\u6574\uff0c\u6709\u65f6\u751a\u81f3\u4e0d\u5b58\u5728\u53ef\u884c\u8ba1\u5212\u3002\u8bc6\u522b\u8fd9\u79cd\u4e0d\u53ef\u884c\u6027\u6709\u52a9\u4e8e\u786e\u5b9a\u4f55\u65f6\u9700\u8981\u8c03\u6574\u9700\u6c42\u3002\u73b0\u6709\u7684\u89c4\u5212\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53ef\u884c\u60c5\u51b5\u4e0b\u7684\u9ad8\u6548\u4e00\u6b21\u6027\u89c4\u5212\uff0c\u800c\u4e0d\u662f\u66f4\u65b0\u9886\u57df\u6216\u68c0\u6d4b\u4e0d\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51faPetri\u7f51\u53ef\u8fbe\u6027\u677e\u5f1b\u65b9\u6cd5\uff0c\u652f\u6301\u9c81\u68d2\u4e0d\u53d8\u5f0f\u5408\u6210\u3001\u9ad8\u6548\u76ee\u6807\u4e0d\u53ef\u8fbe\u6027\u68c0\u6d4b\u548c\u6709\u7528\u7684\u4e0d\u53ef\u884c\u6027\u89e3\u91ca\u3002\u5229\u7528\u589e\u91cf\u7ea6\u675f\u6c42\u89e3\u5668\u652f\u6301\u76ee\u6807\u548c\u7ea6\u675f\u7684\u66f4\u65b0\u3002", "result": "\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u7cfb\u7edf\u4ea7\u751f\u76f8\u5f53\u6570\u91cf\u7684\u4e0d\u53d8\u5f0f\uff0c\u68c0\u6d4b\u5230\u6700\u591a2\u500d\u7684\u4e0d\u53ef\u884c\u6027\uff0c\u5728\u4e00\u6b21\u6027\u89c4\u5212\u4e2d\u8868\u73b0\u6709\u7ade\u4e89\u529b\uff0c\u5728\u6d4b\u8bd5\u9886\u57df\u4e2d\u987a\u5e8f\u8ba1\u5212\u66f4\u65b0\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u8ba1\u5212\u53d8\u66f4\u548c\u4e0d\u53ef\u884c\u6027\u68c0\u6d4b\u95ee\u9898\uff0c\u652f\u6301\u589e\u91cf\u66f4\u65b0\uff0c\u5728\u5b9e\u9645\u89c4\u5212\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}

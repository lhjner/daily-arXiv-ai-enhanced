<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 49]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bridging the Trust Gap: Clinician-Validated Hybrid Explainable AI for Maternal Health Risk Assessment in Bangladesh](https://arxiv.org/abs/2601.07866)
*Farjana Yesmin,Nusrat Shirmin,Suraiya Shabnam Bristy*

Main category: cs.AI

TL;DR: 该研究提出了一个结合模糊逻辑和SHAP解释的混合可解释AI框架，用于孕产妇健康风险预测，在孟加拉国医疗专业人员中验证了其解释性和可信度。


<details>
  <summary>Details</summary>
Motivation: 机器学习在孕产妇健康风险预测中显示出潜力，但在资源受限的临床环境中缺乏可解释性和信任度，这阻碍了其临床应用。

Method: 开发了模糊-XGBoost混合模型，结合了事前模糊逻辑和事后SHAP解释，使用1,014份孕产妇健康记录进行训练，并通过孟加拉国14名医疗专业人员的系统性反馈进行验证。

Result: 模型达到88.67%的准确率（ROC-AUC：0.9703）。验证研究表明，71.4%的医疗专业人员偏好混合解释，54.8%表示对临床使用具有信任度。SHAP分析确定医疗可及性是主要预测因子，模糊风险评分排名第三。

Conclusion: 结合可解释的模糊规则和特征重要性解释能增强实用性和信任度，为孕产妇医疗保健中的XAI部署提供了实用见解，但需要纳入产科史、孕周等关键临床参数。

Abstract: While machine learning shows promise for maternal health risk prediction, clinical adoption in resource-constrained settings faces a critical barrier: lack of explainability and trust. This study presents a hybrid explainable AI (XAI) framework combining ante-hoc fuzzy logic with post-hoc SHAP explanations, validated through systematic clinician feedback. We developed a fuzzy-XGBoost model on 1,014 maternal health records, achieving 88.67% accuracy (ROC-AUC: 0.9703). A validation study with 14 healthcare professionals in Bangladesh revealed strong preference for hybrid explanations (71.4% across three clinical cases) with 54.8% expressing trust for clinical use. SHAP analysis identified healthcare access as the primary predictor, with the engineered fuzzy risk score ranking third, validating clinical knowledge integration (r=0.298). Clinicians valued integrated clinical parameters but identified critical gaps: obstetric history, gestational age, and connectivity barriers. This work demonstrates that combining interpretable fuzzy rules with feature importance explanations enhances both utility and trust, providing practical insights for XAI deployment in maternal healthcare.

</details>


### [2] [When Models Know When They Do Not Know: Calibration, Cascading, and Cleaning](https://arxiv.org/abs/2601.07965)
*Chenjie Hao,Weyl Lu,Yuko Ishiwaka,Zengyi Li,Weier Wan,Yubei Chen*

Main category: cs.AI

TL;DR: 提出了一种无需训练、通用的模型校准方法，通过置信度让模型识别自身未知，并应用于模型级联和数据清洗，提升AI效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 当模型能够识别自身未知时，会带来许多可能性。核心问题是如何让模型意识到自己不知道。已有研究表明，通过模型内部信号计算的置信度可以反映其无知程度，特定领域的校准工作已证明能提供可靠的置信度估计。

Method: 提出了一种简单、有效、无需训练的通用方法，适用于视觉和语言模型。该方法包括模型校准、级联和数据清洗三个部分。首先通过两个关键观察建立校准置信度的可靠性：1）单个模型内置信度越高对应准确率越高；2）在验证集上校准的模型在测试集上仍保持校准。基于此，开发了两种应用：1）使用校准优势路由进行模型级联；2）基于模型集成进行数据清洗。

Result: 通过校准置信度的可比性，实现了大模型与小模型的级联，在几乎不损失准确率的情况下提高了效率；还实现了两个规模相当模型的级联，获得了超越单个模型的性能。利用多个专家模型及其校准置信度，设计了一种简单有效的数据清洗方法，在ImageNet和MMLU数据集中平衡精度和检测率，成功识别错误标注样本。

Conclusion: 让模型能够识别自身未知是实现更高效、可靠、可信AI的实用步骤。校准置信度提供了可靠的路由信号，支持模型级联和数据清洗等应用，展示了这一方法的实际价值。

Abstract: When a model knows when it does not know, many possibilities emerge. The first question is how to enable a model to recognize that it does not know. A promising approach is to use confidence, computed from the model's internal signals, to reflect its ignorance. Prior work in specific domains has shown that calibration can provide reliable confidence estimates. In this work, we propose a simple, effective, and universal training-free method that applies to both vision and language models, performing model calibration, cascading, and data cleaning to better exploit a model's ability to recognize when it does not know. We first highlight two key empirical observations: higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set. These findings empirically establish the reliability and comparability of calibrated confidence. Building on this, we introduce two applications: (1) model cascading with calibrated advantage routing and (2) data cleaning based on model ensemble. Using the routing signal derived from the comparability of calibrated confidences, we cascade large and small models to improve efficiency with almost no compromise in accuracy, and we further cascade two models of comparable scale to achieve performance beyond either model alone. Leveraging multiple experts and their calibrated confidences, we design a simple yet effective data-cleaning method that balances precision and detection rate to identify mislabeled samples in ImageNet and Massive Multitask Language Understanding (MMLU) datasets. Our results demonstrate that enabling models to recognize when they do not know is a practical step toward more efficient, reliable, and trustworthy AI.

</details>


### [3] [Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety](https://arxiv.org/abs/2601.08000)
*Can Jin,Rui Wu,Tong Che,Qixin Zhang,Hongwu Peng,Jiahui Zhao,Zhenting Wang,Wenqi Wei,Ligong Han,Zhao Zhang,Yuan Cao,Ruixiang Tang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: 论文提出CADA方法，通过案例增强的审慎对齐来提升LLM安全性，避免过度拒绝良性请求，相比仅依赖详细安全规则的方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在遵循安全原则的同时避免过度拒绝良性请求存在挑战。OpenAI的审慎对齐方法依赖详细的"代码式"安全规则，但这种方法在开源LLM中效果不佳，因为开源模型通常缺乏高级推理能力。需要探索更有效的方法来平衡安全性和实用性。

Method: 提出CADA（案例增强的审慎对齐）方法：1）系统评估详细安全规则与案例演示的效果差异；2）发现案例增强的简单规则比详细代码式规则更有效；3）基于强化学习在自生成的安全推理链上训练模型，引导LLM进行案例增强推理而非僵化遵循规则。

Result: CADA方法有效提升了无害性，增强了对抗攻击的鲁棒性，减少了过度拒绝，同时在多样化基准测试中保持了实用性。相比仅依赖规则的审慎对齐方法，CADA提供了更实用的安全增强方案。

Conclusion: 案例增强的审慎对齐方法比仅依赖详细安全规则的方法更有效，能够在提升安全性的同时保持实用性，为LLM安全对齐提供了更优的替代方案。

Abstract: Ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests remains a significant challenge. While OpenAI introduces deliberative alignment (DA) to enhance the safety of its o-series models through reasoning over detailed ``code-like'' safety rules, the effectiveness of this approach in open-source LLMs, which typically lack advanced reasoning capabilities, is understudied. In this work, we systematically evaluate the impact of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases. We find that referencing explicit codes inconsistently improves harmlessness and systematically degrades helpfulness, whereas training on case-augmented simple codes yields more robust and generalized safety behaviors. By guiding LLMs with case-augmented reasoning instead of extensive code-like safety rules, we avoid rigid adherence to narrowly enumerated rules and enable broader adaptability. Building on these insights, we propose CADA, a case-augmented deliberative alignment method for LLMs utilizing reinforcement learning on self-generated safety reasoning chains. CADA effectively enhances harmlessness, improves robustness against attacks, and reduces over-refusal while preserving utility across diverse benchmarks, offering a practical alternative to rule-only DA for improving safety while maintaining helpfulness.

</details>


### [4] [Internal Deployment Gaps in AI Regulation](https://arxiv.org/abs/2601.08005)
*Joe Kwon,Stephen Casper*

Main category: cs.AI

TL;DR: 该论文分析了美国和欧盟2025年前沿AI法规在内部部署系统方面的监管漏洞，识别了三个可能导致内部系统逃避监管的缺口，并探讨了解决方案。


<details>
  <summary>Details</summary>
Motivation: 前沿AI法规主要关注对外部署的系统，但企业内部部署的高风险AI应用（如研发自动化、关键业务流程处理、敏感数据处理）同样重要。当前法规在监管内部部署系统方面存在漏洞，可能导致这些高风险应用逃避应有的监管。

Method: 通过分析美国和欧盟2025年前沿AI法规，识别内部部署系统的监管缺口。研究采用法规文本分析、监管框架评估和系统性漏洞识别的方法，重点关注三个关键监管缺口及其成因。

Result: 识别出三个主要监管缺口：1) 范围模糊性允许内部系统逃避监管义务；2) 一次性合规评估无法捕捉内部系统的持续演进；3) 信息不对称削弱监管意识和监督能力。这些缺口源于可测量性、激励机制和信息获取等方面的深层矛盾。

Conclusion: 内部部署的AI系统监管存在系统性漏洞，需要政策制定者有意而非偶然地解决这些问题。论文提出了解决这些缺口的潜在方法及其权衡，旨在帮助政策制定者更审慎地处理内部AI系统的监管问题。

Abstract: Frontier AI regulations primarily focus on systems deployed to external users, where deployment is more visible and subject to outside scrutiny. However, high-stakes applications can occur internally when companies deploy highly capable systems within their own organizations, such as for automating R\&D, accelerating critical business processes, and handling sensitive proprietary data. This paper examines how frontier AI regulations in the United States and European Union in 2025 handle internal deployment. We identify three gaps that could cause internally-deployed systems to evade intended oversight: (1) scope ambiguity that allows internal systems to evade regulatory obligations, (2) point-in-time compliance assessments that fail to capture the continuous evolution of internal systems, and (3) information asymmetries that subvert regulatory awareness and oversight. We then analyze why these gaps persist, examining tensions around measurability, incentives, and information access. Finally, we map potential approaches to address them and their associated tradeoffs. By understanding these patterns, we hope that policy choices around internally deployed AI systems can be made deliberately rather than incidentally.

</details>


### [5] [Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms](https://arxiv.org/abs/2601.08049)
*Keith Ainebyona,Ann Move Oguti,Joseph Walusimbi,Ritah Kobusingye*

Main category: cs.AI

TL;DR: SCASED系统结合了智能教室考勤与面部情绪识别，通过物联网技术实时监测学生课堂参与度，帮助教师调整教学策略。


<details>
  <summary>Details</summary>
Motivation: 当前高等教育中的智能教室技术主要关注考勤自动化，对学生课堂中的情感和认知参与度关注不足，限制了教师识别学生脱离状态和实时调整教学策略的能力。

Method: 使用树莓派摄像头和OpenCV进行人脸检测，采用微调的MobileNetV2模型分类四种学习相关情绪状态（投入、无聊、困惑、沮丧），实现基于会话的考勤管理和连续情绪分析，通过云端仪表板可视化数据。

Result: 在DAiSEE数据集上的实验评估显示，情绪分类准确率达到89.5%，表明将考勤数据与情绪分析相结合可以为教师提供更多课堂动态洞察。

Conclusion: SCASED系统通过集成考勤跟踪和情绪识别，能够为教师提供更全面的课堂参与度信息，支持更及时响应的教学实践。

Abstract: The increasing adoption of smart classroom technologies in higher education has mainly focused on automating attendance, with limited attention given to students' emotional and cognitive engagement during lectures. This limits instructors' ability to identify disengagement and adapt teaching strategies in real time. This paper presents SCASED (Smart Classroom Attendance System with Emotion Detection), an IoT-based system that integrates automated attendance tracking with facial emotion recognition to support classroom engagement monitoring. The system uses a Raspberry Pi camera and OpenCV for face detection, and a finetuned MobileNetV2 model to classify four learning-related emotional states: engagement, boredom, confusion, and frustration. A session-based mechanism is implemented to manage attendance and emotion monitoring by recording attendance once per session and performing continuous emotion analysis thereafter. Attendance and emotion data are visualized through a cloud-based dashboard to provide instructors with insights into classroom dynamics. Experimental evaluation using the DAiSEE dataset achieved an emotion classification accuracy of 89.5%. The results show that integrating attendance data with emotion analytics can provide instructors with additional insight into classroom dynamics and support more responsive teaching practices.

</details>


### [6] [Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms](https://arxiv.org/abs/2601.08052)
*Nawazish Alia,Rachael Shawb,Karl Mason*

Main category: cs.AI

TL;DR: 本文提出了一种用于奶牛场能源管理的深度强化学习框架，通过改进PPO算法实现电池储能和热水系统的智能调度，在真实数据上验证了比传统方法更低的用电成本和更高的电网独立性。


<details>
  <summary>Details</summary>
Motivation: 奶牛养殖是高能耗行业，严重依赖电网电力。随着可再生能源整合增加，实时平衡供需面临挑战。现有强化学习方法通常假设完全了解未来价格或发电情况，这在动态环境中不现实，且标准PPO变体使用固定阈值导致训练不稳定。

Method: 提出了两种PPO改进算法：1) Forecast Aware PPO：结合短期需求预测和可再生能源发电预测，使用基于小时和月份的残差校准；2) PID KL PPO：采用比例-积分-微分控制器自适应调节KL散度，实现稳定的策略更新。框架专注于电池储能和热水系统调度，考虑实际运行约束。

Result: 在真实奶牛场数据上训练，相比标准PPO降低1%用电成本，相比DQN降低4.8%，相比SAC降低1.5%。在电池调度方面，PPO减少电网输入13.1%，展示了可扩展性和有效性。

Conclusion: 提出的深度强化学习框架能够有效管理奶牛场能源系统，降低运营成本并提高电网独立性，为现代奶牛养殖的可持续能源管理提供了可行解决方案。

Abstract: Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.

</details>


### [7] [A New Strategy for Verifying Reach-Avoid Specifications in Neural Feedback Systems](https://arxiv.org/abs/2601.08065)
*Samuel I. Akinwande,Sydney M. Katz,Mykel J. Kochenderfer,Clark Barrett*

Main category: cs.AI

TL;DR: 提出新的反向可达集算法，结合前向分析构建统一验证框架


<details>
  <summary>Details</summary>
Motivation: 现有反向可达性方法可扩展性有限，而前向可达性分析在神经反馈系统验证中占主导地位

Method: 开发计算反向可达集上下近似的新算法，并将其与现有前向分析技术集成

Result: 构建了神经反馈系统的统一验证框架

Conclusion: 通过结合前向和反向可达性分析，提高了神经反馈系统的验证能力

Abstract: Forward reachability analysis is the predominant approach for verifying reach-avoid properties in neural feedback systems (dynamical systems controlled by neural networks). This dominance stems from the limited scalability of existing backward reachability methods. In this work, we introduce new algorithms that compute both over- and under-approximations of backward reachable sets for such systems. We further integrate these backward algorithms with established forward analysis techniques to yield a unified verification framework for neural feedback systems.

</details>


### [8] [Semantic Gravity Wells: Why Negative Constraints Backfire](https://arxiv.org/abs/2601.08070)
*Shailesh Rana*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型中负约束指令（"不要使用词X"）的失败机制，发现违反概率与语义压力呈紧密逻辑关系，并识别出两种主要失败模式：启动失败和覆盖失败。


<details>
  <summary>Details</summary>
Motivation: 负约束指令是测试大型语言模型指令遵循能力的基本测试，尽管看似简单，但这些约束经常失败，且失败条件一直未被充分理解。本文旨在首次全面研究负指令失败的机制原因。

Method: 引入语义压力作为量化指标，使用logit lens技术进行分层分析，通过激活修补实验确定因果责任层，系统研究了40,000个样本中负约束指令的失败模式。

Result: 发现违反概率与语义压力呈紧密逻辑关系(p=σ(-2.40+2.27·P₀))；识别出两种失败模式：启动失败（87.5%违反）和覆盖失败（12.5%）；确定第23-27层对约束效果有因果责任。

Conclusion: 负约束指令设计存在根本性矛盾：明确提及禁止词反而会激活而非抑制目标表示，这揭示了大型语言模型在遵循负指令时的系统性弱点。

Abstract: Negative constraints (instructions of the form "do not use word X") represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure. We introduce semantic pressure, a quantitative measure of the model's intrinsic probability of generating the forbidden token, and demonstrate that violation probability follows a tight logistic relationship with pressure ($p=σ(-2.40+2.27\cdot P_0)$; $n=40{,}000$ samples; bootstrap $95%$ CI for slope: $[2.21,,2.33]$). Through layer-wise analysis using the logit lens technique, we establish that the suppression signal induced by negative instructions is present but systematically weaker in failures: the instruction reduces target probability by only 5.2 percentage points in failures versus 22.8 points in successes -- a $4.4\times$ asymmetry. We trace this asymmetry to two mechanistically distinct failure modes. In priming failure (87.5% of violations), the instruction's explicit mention of the forbidden word paradoxically activates rather than suppresses the target representation. In override failure (12.5%), late-layer feed-forward networks generate contributions of $+0.39$ toward the target probability -- nearly $4\times$ larger than in successes -- overwhelming earlier suppression signals. Activation patching confirms that layers 23--27 are causally responsible: replacing these layers' activations flips the sign of constraint effects. These findings reveal a fundamental tension in negative constraint design: the very act of naming a forbidden word primes the model to produce it.

</details>


### [9] [MemoBrain: Executive Memory as an Agentic Brain for Reasoning](https://arxiv.org/abs/2601.08079)
*Hongjin Qian,Zhao Cao,Zheng Liu*

Main category: cs.AI

TL;DR: MemoBrain是一个用于工具增强智能体的执行记忆模型，通过构建依赖感知的记忆来管理长时推理过程中的中间状态和逻辑关系，解决上下文积累问题，提升长期推理的连贯性。


<details>
  <summary>Details</summary>
Motivation: 在工具增强智能体框架中，复杂的长期推理会导致推理轨迹和临时工具产物不断积累，超出大语言模型的有限工作上下文容量。缺乏显式记忆机制会破坏逻辑连续性并削弱任务对齐，因此记忆不是辅助效率问题，而是维持长期目标导向推理连贯性的核心组件。

Method: 提出MemoBrain执行记忆模型，为工具增强智能体构建依赖感知的记忆，捕捉关键的中间状态及其逻辑关系。作为推理智能体的协同伙伴，MemoBrain在不阻塞执行的情况下组织推理进展，并主动管理工作上下文。具体机制包括：修剪无效步骤、折叠完成的子轨迹、在固定上下文预算下保持紧凑高显著性的推理主干。

Result: 在具有挑战性的长期基准测试（包括GAIA、WebWalker和BrowseComp-Plus）上评估MemoBrain，相比强基线模型展示了一致的性能提升。

Conclusion: MemoBrain通过显式的认知控制机制管理推理轨迹，而不是被动的上下文积累，为工具增强智能体提供了维持长期连贯推理的核心记忆能力。

Abstract: Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.
  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.
  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.

</details>


### [10] [How vehicles change lanes after encountering crashes: Empirical analysis and modeling](https://arxiv.org/abs/2601.08125)
*Kequan Chen,Yuxuan Wang,Pan Liu,Victor L. Knoop,David Z. W. Wang,Yu Han*

Main category: cs.AI

TL;DR: 研究分析了事故后车道变换行为的特征，发现其持续时间更长、插入速度更低、碰撞风险更高，并开发了基于图注意力机制的轨迹预测框架，性能提升超过10%。


<details>
  <summary>Details</summary>
Motivation: 事故发生后，后续车辆需要变换车道绕过障碍物，但目标车道车辆可能拒绝让行，增加了事故后车道变换的复杂性和碰撞风险，而目前对这种行为的特征和运动模式了解不足。

Method: 通过无人机视频提取事故后车辆轨迹构建数据集，并开发了基于图注意力机制的轨迹预测框架，该框架将让行行为建模为辅助的交互感知任务，引导条件变分自编码器和基于Transformer的解码器预测车道变换轨迹。

Result: 实证分析显示，相比强制性和自由裁量车道变换，事故后车道变换持续时间更长、插入速度更低、碰撞风险更高，79.4%的事故后车道变换涉及新跟随者的非让行行为。提出的模型在不同预测时间范围内，平均位移误差和最终位移误差均比现有基准提升超过10%，并能提供更可靠的碰撞风险分析。

Conclusion: 研究揭示了事故后车道变换的独特行为特征，提出的交互感知轨迹预测框架显著提升了预测性能，并通过不同地点的额外数据集验证了模型的可迁移性，为智能交通系统的安全评估提供了重要工具。

Abstract: When a traffic crash occurs, following vehicles need to change lanes to bypass the obstruction. We define these maneuvers as post crash lane changes. In such scenarios, vehicles in the target lane may refuse to yield even after the lane change has already begun, increasing the complexity and crash risk of post crash LCs. However, the behavioral characteristics and motion patterns of post crash LCs remain unknown. To address this gap, we construct a post crash LC dataset by extracting vehicle trajectories from drone videos captured after crashes. Our empirical analysis reveals that, compared to mandatory LCs (MLCs) and discretionary LCs (DLCs), post crash LCs exhibit longer durations, lower insertion speeds, and higher crash risks. Notably, 79.4% of post crash LCs involve at least one instance of non yielding behavior from the new follower, compared to 21.7% for DLCs and 28.6% for MLCs. Building on these findings, we develop a novel trajectory prediction framework for post crash LCs. At its core is a graph based attention module that explicitly models yielding behavior as an auxiliary interaction aware task. This module is designed to guide both a conditional variational autoencoder and a Transformer based decoder to predict the lane changer's trajectory. By incorporating the interaction aware module, our model outperforms existing baselines in trajectory prediction performance by more than 10% in both average displacement error and final displacement error across different prediction horizons. Moreover, our model provides more reliable crash risk analysis by reducing false crash rates and improving conflict prediction accuracy. Finally, we validate the model's transferability using additional post crash LC datasets collected from different sites.

</details>


### [11] [Embedded AI Companion System on Edge Devices](https://arxiv.org/abs/2601.08128)
*Rahul Gupta,Stephen D. H. Hsu*

Main category: cs.AI

TL;DR: 提出一种在边缘设备上运行的AI伴侣系统，通过活跃/非活跃阶段交替的内存范式，在资源受限环境下平衡实时对话响应和长期记忆维护。


<details>
  <summary>Details</summary>
Motivation: 边缘设备计算资源有限，现有AI伴侣和记忆系统无法直接应用，需要解决计算资源不足和延迟问题，在嵌入式硬件约束下实现低延迟且具备长期个性化的AI伴侣系统。

Method: 提出交替活跃/非活跃阶段的内存范式：用户活跃时进行低延迟实时对话（轻量级检索现有记忆和上下文）；用户非活跃时进行计算密集的记忆提取、整合和维护。同时设计了AI伴侣基准测试来全面评估对话质量和记忆能力。

Result: 使用非常弱的模型（Qwen2.5-7B-Instruct量化int4）的系统在大多数指标上优于无记忆的等效原始LLM，性能与具有16k上下文窗口的GPT-3.5相当。

Conclusion: 提出的内存范式能够在嵌入式硬件严格约束下最小化延迟，同时保持长期个性化，为资源受限环境下的AI伴侣系统提供了可行的解决方案。

Abstract: Computational resource constraints on edge devices make it difficult to develop a fully embedded AI companion system with a satisfactory user experience. AI companion and memory systems detailed in existing literature cannot be directly used in such an environment due to lack of compute resources and latency concerns. In this paper, we propose a memory paradigm that alternates between active and inactive phases: during phases of user activity, the system performs low-latency, real-time dialog using lightweight retrieval over existing memories and context; whereas during phases of user inactivity, it conducts more computationally intensive extraction, consolidation, and maintenance of memories across full conversation sessions. This design minimizes latency while maintaining long-term personalization under the tight constraints of embedded hardware. We also introduce an AI Companion benchmark designed to holistically evaluate the AI Companion across both its conversational quality and memory capabilities. In our experiments, we found that our system (using a very weak model: Qwen2.5-7B-Instruct quantized int4) outperforms the equivalent raw LLM without memory across most metrics, and performs comparably to GPT-3.5 with 16k context window.

</details>


### [12] [Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions](https://arxiv.org/abs/2601.08156)
*Arin Gopalan Yadav,Varad Dherange,Kumar Shivam*

Main category: cs.AI

TL;DR: Project Synapse是一个用于自主解决最后一公里配送中断的新型智能体框架，采用分层多智能体架构，使用LangGraph编排复杂工作流，并通过真实用户评论构建的基准数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 最后一公里配送中断是物流行业的重要挑战，需要智能化的自主解决方案。现有方法在处理复杂、循环的工作流程方面存在不足，因此需要开发能够战略分解任务并协调专业智能体的框架。

Method: 采用分层多智能体架构：中央解决监督智能体负责战略任务分解，将子任务委托给专门的工作智能体进行战术执行。使用LangGraph编排复杂和循环的工作流程。通过定性分析6000多个真实用户评论，构建了30个复杂中断场景的基准数据集。

Result: 开发了Project Synapse框架，建立了包含30个复杂中断场景的基准数据集。系统性能使用LLM-as-a-Judge协议进行评估，并采用了明确的偏见缓解措施。

Conclusion: Project Synapse提供了一个有效的自主解决最后一公里配送中断的框架，通过分层多智能体架构和LangGraph编排，能够处理复杂的配送中断场景，为物流行业的智能化转型提供了新的解决方案。

Abstract: This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery disruptions. Synapse employs a hierarchical multi-agent architecture in which a central Resolution Supervisor agent performs strategic task decomposition and delegates subtasks to specialized worker agents responsible for tactical execution. The system is orchestrated using LangGraph to manage complex and cyclical workflows. To validate the framework, a benchmark dataset of 30 complex disruption scenarios was curated from a qualitative analysis of over 6,000 real-world user reviews. System performance is evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.

</details>


### [13] [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)
*Mohammad Pivezhandi,Mahdi Banisharif,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.AI

TL;DR: 提出基于模型的分层多智能体强化学习框架，用于多核平台的温度和能量感知调度，结合LLM语义特征提取实现零样本部署，相比现有方法显著提升能效和调度性能。


<details>
  <summary>Details</summary>
Motivation: 现有DVFS和任务分配方法要么依赖忽略停顿时间的利用率启发式方法，要么需要大量离线分析生成表格，无法适应运行时变化。需要一种能够动态适应、准确预测热动态和性能状态，并能零样本部署到新工作负载的调度框架。

Method: 1. 分层多智能体强化学习框架：两个协作智能体分解指数级动作空间；2. LLM语义特征提取：从OpenMP程序中提取13个代码级特征，无需执行；3. 环境模型：利用回归技术预测热动态和性能状态；4. Dyna-Q启发框架：结合直接强化学习和基于模型的规划；5. 零样本部署：通过生成合成训练数据，无需特定工作负载分析样本。

Result: 1. 决策延迟：后续决策358ms，首次决策3.5-8.0s（含一次性LLM特征提取）；2. 收敛速度：比无模型方法快20倍；3. 性能提升：比Linux ondemand调度器能效高7.09倍，makespan好4.0倍；4. 首次决策延迟：比基于表格的分析方法快8300倍；5. 平台验证：在NVIDIA Jetson TX2、Jetson Orin NX、RubikPi和Intel Core i7上通过BOTS和PolybenchC基准测试验证。

Conclusion: 提出的基于模型的分层MARL框架结合LLM语义特征提取，实现了高效的热和能量感知调度，支持零样本部署到新工作负载，显著优于现有方法，适用于动态嵌入式系统的实际部署。

Abstract: Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.

</details>


### [14] [Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression](https://arxiv.org/abs/2601.08187)
*Zijun Di,Bin Lu,Huquan Kang,Luoyi Fu,Jiaxin Ding,Xiaoying Gan,Lei Zhou,Xinbing Wang,Chenghu Zhou*

Main category: cs.AI

TL;DR: HS2C框架利用图同质性进行结构和语义压缩，通过最小化结构熵的全局分层分区识别同质性社区，将冗余背景压缩为社区级共识，提升LLMs在图理解任务中的推理性能和压缩率。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于上下文窗口，通常采用随机采样丢弃节点/边，这会引入噪声并导致推理不稳定。作者认为图本身包含丰富的结构和语义信息，有效利用这些信息可以释放LLMs在图理解任务中的潜力。

Method: 提出HS2C框架：1) 结构上，基于结构熵最小化原则进行全局分层分区，识别自然凝聚的同质性社区，去除随机连接噪声；2) 语义上，将检测到的结构同质性传递给LLM，使其基于预定义社区类型进行差异化语义聚合，将冗余背景压缩为简洁的社区级共识。

Result: 在10个节点级基准测试和7个图级基准测试中，HS2C通过向LLMs提供结构和语义压缩的输入，同时提升了压缩率和下游推理准确率，验证了其优越性和可扩展性。

Conclusion: HS2C框架通过利用图同质性进行结构和语义压缩，有效提升了LLMs在文本属性图理解任务中的性能，展示了在节点级和图级任务上的通用性。

Abstract: Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its neighborhood context into LLMs. However, constrained by the context window, existing methods mainly resort to random sampling, often implemented via dropping node/edge randomly, which inevitably introduces noise and cause reasoning instability. We argue that graphs inherently contain rich structural and semantic information, and that their effective exploitation can unlock potential gains in LLMs reasoning performance. To this end, we propose Homophily-aware Structural and Semantic Compression for LLMs (HS2C), a framework centered on exploiting graph homophily. Structurally, guided by the principle of Structural Entropy minimization, we perform a global hierarchical partition that decodes the graph's essential topology. This partition identifies naturally cohesive, homophilic communities, while discarding stochastic connectivity noise. Semantically, we deliver the detected structural homophily to the LLM, empowering it to perform differentiated semantic aggregation based on predefined community type. This process compresses redundant background contexts into concise community-level consensus, selectively preserving semantically homophilic information aligned with the target nodes. Extensive experiments on 10 node-level benchmarks across LLMs of varying sizes and families demonstrate that, by feeding LLMs with structurally and semantically compressed inputs, HS2C simultaneously enhances the compression rate and downstream inference accuracy, validating its superiority and scalability. Extensions to 7 diverse graph-level benchmarks further consolidate HS2C's task generalizability.

</details>


### [15] [Adapting Rules of Official International Mahjong for Online Players](https://arxiv.org/abs/2601.08211)
*Chucai Wang,Lingfeng Li,Yunlong Lu,Wenxin Li*

Main category: cs.AI

TL;DR: 该研究使用世界冠军AI进行自博弈分析，发现国际麻将在线单局游戏中存在先手优势和子目标计分问题，提出了补偿分机制和计分规则调整，使传统麻将更适合在线环境。


<details>
  <summary>Details</summary>
Motivation: 国际麻将作为全球流行的传统游戏，在线游戏时玩家时间碎片化且对手不固定，与线下固定对手多轮游戏不同，需要修改规则以确保在线单局游戏的公平性。

Method: 使用世界冠军AI进行自博弈竞赛，通过统计分析揭示先手优势和子目标计分设置问题，基于发现提出规则调整方案。

Result: 研究发现存在明显的先手优势，子目标计分设置存在问题。提出的补偿分机制比传统多轮换位方法更方便在线玩家，并改进了不同牌型子目标的计分。

Conclusion: 这是首次使用AI系统数据评估国际麻将游戏平衡性，开发了更适合在线玩家的修订版传统游戏，实现了在线麻将游戏的改进版本。

Abstract: As one of the worldwide spread traditional game, Official International Mahjong can be played and promoted online through remote devices instead of requiring face-to-face interaction. However, online players have fragmented playtime and unfixed combination of opponents in contrary to offline players who have fixed opponents for multiple rounds of play. Therefore, the rules designed for offline players need to be modified to ensure the fairness of online single-round play. Specifically, We employ a world champion AI to engage in self-play competitions and conduct statistical data analysis. Our study reveals the first-mover advantage and issues in the subgoal scoring settings. Based on our findings, we propose rule adaptations to make the game more suitable for the online environment, such as introducing compensatory points for the first-mover advantage and refining the scores of subgoals for different tile patterns. Compared with the traditional method of rotating positions over multiple rounds to balance first-mover advantage, our compensatory points mechanism in each round is more convenient for online players. Furthermore, we implement the revised Mahjong game online, which is open for online players. This work is an initial attempt to use data from AI systems to evaluate Official Internatinoal Mahjong's game balance and develop a revised version of the traditional game better adapted for online players.

</details>


### [16] [An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3](https://arxiv.org/abs/2601.08224)
*Daesuk Kwon,Won-gi Paeng*

Main category: cs.AI

TL;DR: SANC(E3)是一个理论框架，提出表征单元不是先验给定的，而是在有限激活容量下通过竞争选择、重建和压缩的自组织过程稳定形成的，通过最小化E3能量函数来统一感知、想象、预测、规划和行动。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统通常预设固定的基本单元（如token、像素等），忽略了表征单元本身如何涌现和稳定的根本问题。通用智能需要将经验重组为能够在有限资源下支持预测和行动的内部结构。

Method: 提出SANC(E3)公理化框架，包含五个核心公理：有限容量、共现关联、相似性竞争、置信度稳定化、重建-压缩-更新权衡。采用伪内存映射I/O机制，使内部回放的格式塔与外部感官输入通过相同的公理路径处理。

Result: 从公理推导出12个命题，表明类别形成、层次组织、无监督学习和高级认知活动都可以理解为在E3最小化下的格式塔完成过程。表征单元通过竞争选择、重建和压缩的自组织过程稳定形成。

Conclusion: SANC(E3)提供了一个统一的理论框架，将感知、想象、预测、规划和行动整合到单一的表征和能量过程中，为理解智能系统如何从经验中自组织表征单元提供了理论基础。

Abstract: General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.

</details>


### [17] [MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents](https://arxiv.org/abs/2601.08235)
*Shouju Wang,Haopeng Zhang*

Main category: cs.AI

TL;DR: MPCI-Bench是首个多模态成对情境完整性基准，用于评估智能体环境中的隐私行为，包含三个层级的数据实例，揭示了当前多模态模型在平衡隐私与效用方面的系统性失败。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型智能体从被动聊天机器人演变为处理个人数据的主动助手，评估其遵守社会规范变得日益重要。现有情境完整性基准主要是文本中心且强调负面拒绝场景，忽视了多模态隐私风险和隐私与效用的基本权衡。

Method: 提出MPCI-Bench基准，包含从相同视觉源衍生的正负配对实例，分为三个层级：规范性种子判断、上下文丰富的故事推理和可执行的智能体行动轨迹。通过三原则迭代精炼流程确保数据质量。

Result: 对最先进多模态模型的评估显示，它们在平衡隐私与效用方面存在系统性失败，并表现出明显的模态泄漏差距——敏感视觉信息比文本信息泄漏更频繁。

Conclusion: MPCI-Bench是首个多模态成对情境完整性基准，将开源以促进未来智能体情境完整性研究，揭示了当前模型在隐私保护方面的关键缺陷。

Abstract: As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.

</details>


### [18] [The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination](https://arxiv.org/abs/2601.08237)
*Haoran Su,Yandong Sun,Congjia Yu*

Main category: cs.AI

TL;DR: 论文提出利用大语言模型从手工数值奖励转向基于语言的奖励规范，以解决多智能体强化学习中的奖励工程挑战。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中奖励工程面临信用分配模糊、环境非平稳性和交互复杂性指数增长等挑战，需要更有效的奖励规范方法。

Method: 利用大语言模型从自然语言描述直接合成奖励函数（如EUREKA），并在线适应奖励公式（如CARD），结合可验证奖励的强化学习范式。

Result: 语言介导的监督可以作为传统奖励工程的可行替代方案，实现语义奖励规范、动态奖励适应和更好的人类意图对齐。

Conclusion: 未来研究方向是从共享语义表示而非显式设计的数值信号中产生协调，但仍需解决计算开销、幻觉鲁棒性和大规模系统可扩展性等挑战。

Abstract: Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.

</details>


### [19] [T3: Benchmarking Sycophancy and Skepticism in Causal Judgment](https://arxiv.org/abs/2601.08258)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: T3是一个诊断性基准测试，用于评估大语言模型在Pearl因果阶梯上的因果判断能力，包含454个专家策划的情景，识别出模型在因果推理中的两种病理模式。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在因果推理方面存在系统性缺陷，需要一种能够精确诊断这些缺陷的评估工具。T3旨在填补这一空白，通过结构化基准测试来识别模型在因果判断中的具体病理模式。

Method: 开发了T3基准测试，包含454个专家策划的情景，覆盖Pearl因果阶梯的三个层级。测试将性能分解为效用（敏感性）、安全性（特异性）和明智拒绝三个维度。应用该基准测试前沿模型，并验证了一个过程验证协议（RCA）。

Result: 发现了两种病理模式：1）L1层的"怀疑论陷阱"（安全调整模型如Claude Haiku拒绝60%的有效链接）；2）L3层的非单调缩放悖论（GPT-5.2在模糊反事实上比GPT-4-Turbo低55分，主要由于过度犹豫而非幻觉）。RCA协议成功恢复了结构化验证下的决定性因果判断。

Conclusion: T3基准测试能够有效诊断大语言模型在因果推理中的系统性缺陷，揭示了安全调整可能导致过度保守，而模型规模的增加不一定带来因果判断能力的提升。过程验证协议可以缓解这些病理模式，为改进模型因果推理能力提供了方向。

Abstract: We introduce T3 (Testing Trustworthy Thinking), a diagnostic benchmark designed to rigorously evaluate LLM causal judgment across Pearl's Ladder of Causality. Comprising 454 expert-curated vignettes, T3 prioritizes high-resolution failure analysis, decomposing performance into Utility (sensitivity), Safety (specificity), and Wise Refusal on underdetermined cases. By applying T3 to frontier models, we diagnose two distinct pathologies: a "Skepticism Trap" at L1 (where safety-tuned models like Claude Haiku reject 60% of valid links) and a non-monotonic Scaling Paradox at L3. In the latter, the larger GPT-5.2 underperforms GPT-4-Turbo by 55 points on ambiguous counterfactuals, driven by a collapse into paralysis (excessive hedging) rather than hallucination. Finally, we use the benchmark to validate a process-verified protocol (RCA), showing that T3 successfully captures the restoration of decisive causal judgment under structured verification.

</details>


### [20] [Greedy Is Enough: Sparse Action Discovery in Agentic LLMs](https://arxiv.org/abs/2601.08280)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 本文研究大规模动作空间中的智能体决策问题，提出基于结构化稀疏假设的动作发现方法，证明贪婪算法能高效识别相关动作集，为智能体系统的动作剪枝提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 现代智能体系统（如工具增强语言模型）面临极大动作空间（数千个API），但经验表明只有少量动作真正影响性能。本文旨在为这种大规模动作决策提供理论基础，识别稀疏动作发现作为核心原则。

Method: 将动作发现建模为块稀疏恢复问题，提出受正交匹配追踪启发的贪婪算法。在不相干性、信号强度和动作覆盖的标准假设下，分析算法性能。

Result: 证明贪婪算法能以高概率精确恢复相关动作集，所需样本数量随稀疏度和潜在维度多项式增长，仅随总动作数对数增长。同时提供重拟合参数估计误差保证，证明所得决策规则对新潜在状态接近最优。

Conclusion: 稀疏性和充分覆盖是问题可处理性的必要条件。稀疏动作发现是大规模动作决策的基本原理，为智能体系统的动作剪枝提供了理论依据。

Abstract: Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. Motivated by this observation, we study a contextual linear reward model in which action relevance is governed by a structured sparsity assumption: only a small number of actions have nonzero effects across latent states.
  We formulate action discovery as a block-sparse recovery problem and analyze a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions on incoherence, signal strength, and action coverage, we prove that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. We further provide estimation error guarantees for refitted parameters and show that the resulting decision rule is near-optimal for new latent states.
  Complementing these results, we establish information-theoretic lower bounds demonstrating that sparsity and sufficient coverage are necessary for tractability. Together, our results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.

</details>


### [21] [OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System](https://arxiv.org/abs/2601.08288)
*Yuyang Wu,Hanzhong Cao,Jianhao Chen,Yufei Li*

Main category: cs.AI

TL;DR: OpenMic是一个端到端的多智能体系统，能够将用户提供的生活话题转化为3-5分钟的中文脱口秀表演并生成带旁白的喜剧视频。


<details>
  <summary>Details</summary>
Motivation: 中文脱口秀生成超越了普通文本生成，需要文化基础的幽默、精确的节奏把控、舞台表演提示和隐式的多步推理。此外，常用的中文幽默数据集更适合幽默理解和评估，而非长篇脱口秀生成，导致直接监督与目标任务不匹配。

Method: 1. 基于AutoGen构建端到端多智能体系统；2. 采用多轮迭代循环规划，协调多个专业智能体共同优化幽默、节奏和可表演性；3. 使用检索增强生成（RAG）进行素材基础和想法扩展；4. 微调专门的JokeWriter模型以更好地内化脱口秀特有的铺垫-笑点结构和长程回调。

Result: 系统能够将用户提供的生活话题转化为完整的3-5分钟中文脱口秀表演，并进一步生成带旁白的喜剧视频。

Conclusion: OpenMic通过多智能体系统架构、RAG增强和专门的模型微调，有效解决了中文脱口秀生成中的文化基础、节奏把控和数据集任务不匹配等挑战。

Abstract: Chinese stand-up comedy generation goes beyond plain text generation, requiring culturally grounded humor, precise timing, stage-performance cues, and implicit multi-step reasoning. Moreover, commonly used Chinese humor datasets are often better suited for humor understanding and evaluation than for long-form stand-up generation, making direct supervision misaligned with the target task. To address these challenges, we present OpenMic, an end-to-end multi-agent system built on AutoGen that transforms a user-provided life topic into a 3-5 minute Chinese stand-up performance and further produces a narrated comedy video. OpenMic orchestrates multiple specialized agents in a multi-round iterative loop-planning to jointly optimize humor, timing, and performability. To mitigate the dataset-task mismatch, we augment generation with retrieval-augmented generation (RAG) for material grounding and idea expansion, and we fine-tune a dedicated JokeWriter to better internalize stand-up-specific setup-punchline structures and long-range callbacks.

</details>


### [22] [Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant](https://arxiv.org/abs/2601.08333)
*Oleg Romanchuk,Roman Bondar*

Main category: cs.AI

TL;DR: 论文指出LLM智能体架构将信息传输机制与认知证明机制混淆，导致"语义清洗"问题——缺乏充分依据的命题通过可信接口被系统接受，这构成了盖梯尔问题的架构实现。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体架构存在系统性缺陷，将信息传输机制与认知证明机制混为一谈，导致命题在没有充分依据的情况下被系统接受。这种架构缺陷会产生可复现的认知问题，需要从理论层面进行分析和解决。

Method: 通过形式化分析，提出"语义清洗"概念来描述这种架构缺陷，将其与盖梯尔问题联系起来。引入"不可避免的自许可定理"证明在标准架构假设下，循环认知证明无法消除。提出"依据侵蚀原理"作为根本解释。

Result: 证明了语义清洗是盖梯尔问题的架构实现，命题获得高认知地位但缺乏真实依据。展示了扩展、模型改进和LLM作为评判者方案在结构上无法解决这一类型层面的问题。

Conclusion: LLM智能体架构存在根本性缺陷，语义清洗问题无法通过现有技术手段解决，因为这是架构层面的类型问题。需要重新思考智能体架构设计，将信息传输与认知证明机制分离。

Abstract: LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms. We formalize this class of architectural failures as semantic laundering: a pattern where propositions with absent or weak warrant are accepted by the system as admissible by crossing architecturally trusted interfaces. We show that semantic laundering constitutes an architectural realization of the Gettier problem: propositions acquire high epistemic status without a connection between their justification and what makes them true. Unlike classical Gettier cases, this effect is not accidental; it is architecturally determined and systematically reproducible. The central result is the Theorem of Inevitable Self-Licensing: under standard architectural assumptions, circular epistemic justification cannot be eliminated. We introduce the Warrant Erosion Principle as the fundamental explanation for this effect and show that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating a problem that exists at the type level.

</details>


### [23] [Thematic Working Group 5 -- Artificial Intelligence (AI) literacy for teaching and learning: design and implementation](https://arxiv.org/abs/2601.08380)
*Mary Webb,Matt Bower,Ana Amélia Carvalho,Fredrik Mørk Røkenes,Jodie Torrington,Jonathan D. Cohen,Yousra Chtouki,Kathryn Maccallum,Tanya Linden,Deirdre Butler,Juliana Elisa Raffaghelli,Henriikka Vartiainen,Martina Ronci,Peter Tiernan,David M. Smith,Chris Shelton,Joyce Malyn-smith,Pierre Gorissen*

Main category: cs.AI

TL;DR: 该论文工作组致力于提升教师的AI素养和能动性，通过课程设计、专业发展、课堂应用和政策指导等策略，帮助教师有效整合AI到教学实践中。


<details>
  <summary>Details</summary>
Motivation: 当前教师缺乏足够的AI知识和技能来有效整合AI到教学中，需要提升教师的AI素养和能动性，使他们能够自信地使用AI工具并帮助学生深入理解AI概念。

Method: 通过课程设计、专业发展项目、实际课堂应用探索以及政策指南制定等多种策略，系统性地提升教师的AI知识和技能。

Result: 开发了有效的策略来增强教师的AI素养和能动性，为教师提供了整合AI到教学实践所需的知识和技能框架。

Conclusion: 通过系统性的策略实施，可以成功提升教师的AI素养和能动性，使他们能够更好地将AI整合到教学中，从而培养学生的AI理解能力。

Abstract: TWG 5 focused on developing and implementing effective strategies for enhancing AI literacy and agency of teachers, equipping them with the knowledge and skills necessary to integrate AI into their teaching practices. Explorations covered curriculum design, professional development programs, practical classroom applications, and policy guidelines aiming to empower educators to confidently utilize AI tools and foster a deeper understanding of AI concepts among students.

</details>


### [24] [A Qualitative Model to Reason about Object Rotations (QOR) applied to solve the Cube Comparison Test (CCT)](https://arxiv.org/abs/2601.08382)
*Zoe Falomir*

Main category: cs.AI

TL;DR: 提出QOR定性模型用于解决立方体比较测试，通过构建旋转运动与立方体面特征位置和方向变化的概念邻域图来生成推理组合表


<details>
  <summary>Details</summary>
Motivation: 解决Ekstrom等人（1976）提出的立方体比较测试问题，该测试需要推理物体旋转后的特征变化

Method: 构建旋转运动与位置变化、方向变化的概念邻域图（CNGRLO），并生成组合表来计算旋转推理

Result: 开发了QOR定性模型，能够系统化地推理立方体旋转后的特征位置和方向变化

Conclusion: QOR模型为物体旋转推理提供了有效的定性框架，能够解决立方体比较测试等空间推理问题

Abstract: This paper presents a Qualitative model for Reasoning about Object Rotations (QOR) which is applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). A conceptual neighborhood graph relating the Rotation movement to the Location change and the Orientation change (CNGRLO) of the features on the cube sides has been built and it produces composition tables to calculate inferences for reasoning about rotations.

</details>


### [25] [Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models](https://arxiv.org/abs/2601.08383)
*Bo Wang,Junzhuo Li,Hong Chen,Yuanlin Chu,Yuxuan Fan,Xuming Hu*

Main category: cs.AI

TL;DR: MoE架构在预训练期间形成低熵骨干结构，早期固化重要性分布，功能鲁棒性更强，与密集模型的知识获取模式显著不同


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构在预训练期间如何塑造知识获取过程，以及这一过程与密集架构的差异，以理解稀疏架构的训练动态

Method: 引入Gated-LPI（门控对数概率增加）神经元级归因度量，分解跨神经元的对数概率增加；对MoE和密集架构进行时间分辨比较，追踪120万训练步（约5.0T令牌）和60万训练步（约2.5T令牌）的检查点

Result: 发现三个关键模式：1）低熵骨干：MoE中约1%的神经元捕获超过45%的正更新，形成高效用核心；2）早期固化：MoE在10万步内锁定稳定重要性分布，而密集模型在整个训练中保持波动；3）功能鲁棒性：掩码最重要的MoE注意力头对关系HIT@10影响小于10%，而密集模型影响超过50%

Conclusion: 稀疏性从训练早期就培养出内在稳定且分布式的计算骨干，有助于弥合稀疏架构与训练时可解释性之间的差距

Abstract: Mixture-of-Experts (MoE) architectures decouple model capacity from per-token computation, enabling scaling beyond the computational limits imposed by dense scaling laws. Yet how MoE architectures shape knowledge acquisition during pre-training, and how this process differs from dense architectures, remains unknown. To address this issue, we introduce Gated-LPI (Log-Probability Increase), a neuron-level attribution metric that decomposes log-probability increase across neurons. We present a time-resolved comparison of knowledge acquisition dynamics in MoE and dense architectures, tracking checkpoints over 1.2M training steps (~ 5.0T tokens) and 600K training steps (~ 2.5T tokens), respectively. Our experiments uncover three patterns: (1) Low-entropy backbone. The top approximately 1% of MoE neurons capture over 45% of positive updates, forming a high-utility core, which is absent in the dense baseline. (2) Early consolidation. The MoE model locks into a stable importance profile within < 100K steps, whereas the dense model remains volatile throughout training. (3) Functional robustness. Masking the ten most important MoE attention heads reduces relational HIT@10 by < 10%, compared with > 50% for the dense model, showing that sparsity fosters distributed -- rather than brittle -- knowledge storage. These patterns collectively demonstrate that sparsity fosters an intrinsically stable and distributed computational backbone from early in training, helping bridge the gap between sparse architectures and training-time interpretability.

</details>


### [26] [Creativity in AI as Emergence from Domain-Limited Generative Models](https://arxiv.org/abs/2601.08388)
*Corina Chutaux*

Main category: cs.AI

TL;DR: 该论文提出从生成视角理解AI创造力，将其视为有限领域生成模型在受限信息环境中的涌现属性，而非后验评估标签


<details>
  <summary>Details</summary>
Motivation: 当前AI创造力研究主要关注评估框架（新颖性、多样性、有用性），将创造力视为待评估属性而非待建模现象。随着多模态生成系统展现复杂模式重组能力，需要从生成角度理解机器创造力的本质和限制

Method: 提出生成视角的创造力框架，将其分解为四个交互组件：基于模式的生成、诱导世界模型、上下文基础、任意性。分析这些组件在多模态生成系统中的表现，关注生成动态与领域特定表征之间的交互

Result: 建立了将创造力视为AI系统涌现现象的技术框架，强调创造力产生于生成动态与领域特定表征的交互，而非后验评估标准

Conclusion: 创造力应被理解为有限领域生成模型在受限信息环境中的涌现属性，这一生成视角为研究AI创造力提供了新的技术框架，将创造力从评估标签转变为可建模的现象

Abstract: Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.

</details>


### [27] [Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs](https://arxiv.org/abs/2601.08403)
*Abhijnan Nath,Alireza Bagheri Garakani,Tianchen Zhou,Fan Yang,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: OSPO框架通过Shapley-Owen归因重新分配序列级优势，解决推荐系统中基于稀疏奖励的强化学习的信用分配问题，无需额外价值模型计算。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的大语言模型推荐系统（如GRPO）依赖稀疏的序列级奖励，导致信用分配问题，难以确定哪些具体token驱动推荐成功。当模型需要从非明确语言中推断潜在用户意图时，这一问题尤为严重。

Method: 提出Owen-Shapley Policy Optimization (OSPO)框架，基于潜在奖励塑造，使用Shapley-Owen归因方法将序列级优势重新分配到语义连贯的单元（如描述产品属性的短语或捕捉偏好的句子），识别响应中驱动性能的部分。

Result: 在Amazon ESCI和H&M Fashion数据集上的实验显示，OSPO相比基线方法获得一致性能提升，并在测试时对训练中未见过的分布外检索器表现出显著的鲁棒性。

Conclusion: OSPO通过基于Shapley-Owen归因的信用分配机制，有效解决了推荐系统中强化学习的信用分配问题，无需额外价值模型，在性能和鲁棒性方面均优于现有方法。

Abstract: Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when models must infer latent user intent from under-specified language without ground truth labels, a reasoning pattern rarely seen during pretraining. We introduce Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages based on tokens' marginal contributions to outcomes. Unlike value-model-based methods requiring additional computation, OSPO employs potential-based reward shaping via Shapley-Owen attributions to assign segment-level credit while preserving the optimal policy, learning directly from task feedback without parametric value models. By forming coalitions of semantically coherent units (phrases describing product attributes or sentences capturing preferences), OSPO identifies which response parts drive performance. Experiments on Amazon ESCI and H&M Fashion datasets show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.

</details>


### [28] [Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation](https://arxiv.org/abs/2601.08412)
*Yizhan Feng,Hichem Snoussi,Yuhang Wang,Jing Teng,Abel Cherouat,Tian Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种结合知识蒸馏、思维链引导和监督微调的方法，将大语言模型的代码生成能力迁移到轻量级模型，用于无人机多SDK控制任务，在保持高准确率的同时显著提升部署和推理效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成任务中展现出巨大潜力，但将其应用于资源受限的无人机平台时存在矛盾：大模型资源消耗高，而无人机平台需要实时、轻量化的解决方案。

Method: 1) 构建高质量数据集，包含指令-代码-推理链和反事实负样本；2) 使用DeepSeek-Coder-V2-Lite作为教师模型，采用混合黑盒白盒蒸馏策略生成思维链软标签，结合加权交叉熵损失将复杂推理能力迁移到学生模型；3) 通过针对无人机控制场景优化的提示调优工程增强模型性能。

Result: 蒸馏后的轻量级模型在保持高代码生成准确率的同时，实现了部署和推理效率的显著提升，有效证明了该方法在实现无人机精准轻量化智能控制方面的可行性。

Conclusion: 该方法成功解决了大语言模型与无人机平台资源限制之间的矛盾，为无人机多SDK控制任务提供了一种高效、轻量化的智能控制解决方案。

Abstract: With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs

</details>


### [29] [RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation](https://arxiv.org/abs/2601.08430)
*Sunzhu Li,Jiale Zhao,Miteto Wei,Huimin Ren,Yang Zhou,Jingwen Yang,Shunyu Liu,Kaike Zhang,Wei Chen*

Main category: cs.AI

TL;DR: 提出RubricHub框架，通过粗到细的评估标准生成解决开放生成任务中缺乏真实标注的问题，在多个领域实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 在开放生成任务中，由于缺乏真实标注，基于验证的强化学习（RLVR）面临监督天花板效应。现有的基于评估标准的验证方法存在可扩展性瓶颈和标准粗糙的问题，无法捕捉细微差异。

Method: 提出自动化粗到细评估标准生成框架，结合原则引导合成、多模型聚合和难度演化，生成全面且高区分度的评估标准。基于此框架构建RubricHub大规模多领域数据集，采用两阶段后训练流程：基于评估标准的拒绝采样微调（RuFT）和强化学习（RuRL）。

Result: RubricHub数据集包含约11万样本，覆盖多个领域。实验显示，经过后训练的Qwen3-14B模型在HealthBench上达到69.3分，超越了GPT-5等前沿专有模型，实现了SOTA性能。

Conclusion: RubricHub框架通过自动化生成细粒度评估标准，有效解决了开放生成任务中的监督天花板问题，为大规模多领域生成任务提供了有效的评估和监督解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.

</details>


### [30] [YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation](https://arxiv.org/abs/2601.08441)
*Abdelaziz Bounhar,Rania Hossam Elmohamady Elbadry,Hadi Abdine,Preslav Nakov,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.AI

TL;DR: YaPO是一种通过稀疏自编码器学习稀疏导向向量的参考无关方法，相比密集导向向量能更好地解耦潜在因素，实现更精细化的LLM对齐控制。


<details>
  <summary>Details</summary>
Motivation: 现有的密集导向向量方法由于神经元多语义性，往往纠缠多个潜在因素，限制了在文化对齐等精细场景中的有效性和稳定性，需要区分密切相关的价值观和行为。

Method: 提出YaPO方法，在稀疏自编码器的潜在空间中学习稀疏导向向量，通过优化稀疏编码产生解耦、可解释且高效的导向方向。

Result: YaPO收敛更快，性能更强，训练稳定性更好；不仅适用于文化对齐，还能泛化到幻觉、财富追求、越狱、权力追求等对齐相关行为；保持通用知识，在MMLU上无性能下降。

Conclusion: YaPO为LLM的高效、稳定和精细化对齐提供了通用方案，在可控性和领域适应方面具有广泛应用前景。

Abstract: Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \textit{reference-free} method that learns \textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\footnote{https://github.com/MBZUAI-Paris/YaPO}.

</details>


### [31] [Beyond Linearization: Attributed Table Graphs for Table Reasoning](https://arxiv.org/abs/2601.08444)
*Yuxiang Wang,Junhao Gan,Shengxiang Gao,Shenghao Ye,Zhengyi Yang,Jianzhong Qi*

Main category: cs.AI

TL;DR: TABGR：一种基于图表示的表格推理方法，通过将表格转换为属性图结构，结合个性化PageRank机制，显著提升表格问答的准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的表格推理方法存在三个关键问题：1）线性化处理丢失表格结构信息；2）缺乏明确的推理路径，可解释性差；3）存在"中间迷失"问题。需要一种能保留表格结构、支持可解释推理、并缓解信息丢失问题的新方法。

Method: 提出TABGR模型，包含两个核心组件：1）将表格表示为属性表图（ATG），显式保留行-列-单元格结构；2）提出问题引导的个性化PageRank（QG-PPR）机制，重新排序表格数据以缓解"中间迷失"问题。该方法无需训练即可使用。

Result: 在两个常用基准测试上的广泛实验表明，TABGR始终优于最先进的模型，准确率最高提升9.7%。

Conclusion: TABGR通过图表示和个性化PageRank机制，有效解决了现有表格推理方法的局限性，在保持表格结构、提升可解释性和缓解信息丢失方面表现出色，为表格推理任务提供了新的有效解决方案。

Abstract: Table reasoning, a task to answer questions by reasoning over data presented in tables, is an important topic due to the prevalence of knowledge stored in tabular formats. Recent solutions use Large Language Models (LLMs), exploiting the semantic understanding and reasoning capabilities of LLMs. A common paradigm of such solutions linearizes tables to form plain texts that are served as input to LLMs. This paradigm has critical issues. It loses table structures, lacks explicit reasoning paths for result explainability, and is subject to the "lost-in-the-middle" issue. To address these issues, we propose Table Graph Reasoner (TABGR), a training-free model that represents tables as an Attributed Table Graph (ATG). The ATG explicitly preserves row-column-cell structures while enabling graph-based reasoning for explainability. We further propose a Question-Guided Personalized PageRank (QG-PPR) mechanism to rerank tabular data and mitigate the lost-in-the-middle issue. Extensive experiments on two commonly used benchmarks show that TABGR consistently outperforms state-of-the-art models by up to 9.7% in accuracy. Our code will be made publicly available upon publication.

</details>


### [32] [An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English](https://arxiv.org/abs/2601.08457)
*Sargam Yadav,Abhishek Kaushik,Kevin Mc Daid*

Main category: cs.AI

TL;DR: 本文提出一个多模态可解释的Web应用，用于检测印地语-英语混合文本和表情包中的厌女内容，采用先进的Transformer模型，并提供SHAP和LIME可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 数字平台用户规模不断扩大，但同时也助长了仇恨言论和厌女内容的传播。现有AI模型在低资源语言和代码混合语言中的检测能力不足，且缺乏可解释性，这在敏感的仇恨言论检测领域尤为重要。

Method: 1. 文本厌女检测：使用XLM-RoBERTa和mBERT模型在4,193条混合语言评论数据集上训练
2. 多模态表情包检测：使用mBERT+EfficientNet和mBERT+ResNet在4,218个表情包数据集上训练
3. 可解释性分析：采用SHAP和LIME技术提供特征重要性评分
4. 系统评估：通过Chatbot Usability Questionnaire和User Experience Questionnaire进行人工评估

Result: 开发了一个多模态可解释的Web应用程序，能够检测印地语-英语混合文本和表情包中的厌女内容。系统集成了先进的Transformer模型，并提供了可解释性分析功能，帮助研究人员和内容审核人员理解模型决策过程。

Conclusion: 该系统为研究人员和内容审核人员提供了一个有效工具，促进了该领域的研究，有助于打击基于性别的数字暴力，确保安全的数字空间。通过可解释AI技术增强了模型透明度，这在敏感的仇恨言论检测领域至关重要。

Abstract: Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.

</details>


### [33] [SUMMPILOT: Bridging Efficiency and Customization for Interactive Summarization System](https://arxiv.org/abs/2601.08475)
*JungMin Yun,Juhwan Choi,Kyohoon Jin,Soojin Jang,Jinhee Jang,YoungBin Kim*

Main category: cs.AI

TL;DR: SummPilot是一个基于交互的可定制化摘要系统，结合自动摘要效率与个性化需求，通过大语言模型支持自动和交互式摘要生成。


<details>
  <summary>Details</summary>
Motivation: 现有自动摘要系统缺乏个性化定制能力，无法满足不同用户的特定兴趣和需求，需要开发能够结合自动摘要效率与个性化定制的解决方案。

Method: 引入SummPilot系统，利用大语言模型实现自动和交互式摘要，通过语义图、实体聚类和可解释评估等交互组件，让用户理解文档内容并个性化定制摘要。

Result: 演示和用户研究表明SummPilot在可定制化摘要方面具有适应性和实用性，能够有效满足用户的个性化需求。

Conclusion: SummPilot成功结合了自动摘要的效率和个性化定制的灵活性，为可定制化摘要提供了一个有效的交互式解决方案。

Abstract: This paper incorporates the efficiency of automatic summarization and addresses the challenge of generating personalized summaries tailored to individual users' interests and requirements. To tackle this challenge, we introduce SummPilot, an interaction-based customizable summarization system. SummPilot leverages a large language model to facilitate both automatic and interactive summarization. Users can engage with the system to understand document content and personalize summaries through interactive components such as semantic graphs, entity clustering, and explainable evaluation. Our demo and user studies demonstrate SummPilot's adaptability and usefulness for customizable summarization.

</details>


### [34] [What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting](https://arxiv.org/abs/2601.08509)
*Jinkwan Jang,Hyunbin Jin,Hyungjin Park,Kyubyung Chae,Taesup Kim*

Main category: cs.AI

TL;DR: WIT是一个多模态时间序列预测基准，通过专家构建的合理或反事实场景来评估模型是否能基于上下文文本（特别是未来场景）进行预测


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法大多是单模态的，主要依赖历史模式外推。虽然大语言模型在多模态预测方面显示出潜力，但现有基准主要提供回顾性或不对齐的原始上下文，无法确定模型是否真正利用了文本输入。人类专家在实践中会结合历史证据和假设场景，在不同场景下基于相同观测产生不同的预测

Method: 引入What If TSF（WIT）多模态预测基准，通过提供专家构建的合理或反事实场景，评估模型是否能基于上下文文本（特别是未来场景）进行条件预测

Result: WIT基准提供了一个严格的测试平台，用于评估场景引导的多模态预测能力，基准已在GitHub上开源

Conclusion: WIT基准填补了现有评估方法的空白，能够更准确地评估模型是否真正利用文本输入进行多模态时间序列预测，特别是在不同未来场景下的预测能力

Abstract: Time series forecasting is critical to real-world decision making, yet most existing approaches remain unimodal and rely on extrapolating historical patterns. While recent progress in large language models (LLMs) highlights the potential for multimodal forecasting, existing benchmarks largely provide retrospective or misaligned raw context, making it unclear whether such models meaningfully leverage textual inputs. In practice, human experts incorporate what-if scenarios with historical evidence, often producing distinct forecasts from the same observations under different scenarios. Inspired by this, we introduce What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting. The benchmark is available at https://github.com/jinkwan1115/WhatIfTSF.

</details>


### [35] [Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse](https://arxiv.org/abs/2601.08531)
*Warissara Booranamaitree,Xusheng Du,Yushu Cai,Zhengyang Wang,Ye Zhang,Haoran Xie*

Main category: cs.AI

TL;DR: 提出结合生成式AI和视觉语言模型的三阶段框架，直接从粗略结构草图和文本描述生成立面改造方案，无需详细建模


<details>
  <summary>Details</summary>
Motivation: 立面改造比完全拆除更可持续，但现有工作流程需要详细的现状建模，耗时耗力且需要反复修改，需要更高效的方案生成方法

Method: 三阶段框架：1) 微调VLM模型预测需要修改的区域和组件；2) 稳定扩散模型生成新元素草图并与原始轮廓合并；3) ControlNet将结果细化为逼真图像

Result: 在数据集和真实工业建筑上的实验表明，该框架能生成保留原始结构同时提升立面细节质量的改造方案

Conclusion: 该方法有效绕过详细现状建模需求，使建筑师能快速探索设计替代方案、迭代早期概念，并更清晰地传达改造意图

Abstract: Facade renovation offers a more sustainable alternative to full demolition, yet producing design proposals that preserve existing structures while expressing new intent remains challenging. Current workflows typically require detailed as-built modelling before design, which is time-consuming, labour-intensive, and often involves repeated revisions. To solve this issue, we propose a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) that directly processes rough structural sketch and textual descriptions to produce consistent renovation proposals. First, the input sketch is used by a fine-tuned VLM model to predict bounding boxes specifying where modifications are needed and which components should be added. Next, a stable diffusion model generates detailed sketches of new elements, which are merged with the original outline through a generative inpainting pipeline. Finally, ControlNet is employed to refine the result into a photorealistic image. Experiments on datasets and real industrial buildings indicate that the proposed framework can generate renovation proposals that preserve the original structure while improving facade detail quality. This approach effectively bypasses the need for detailed as-built modelling, enabling architects to rapidly explore design alternatives, iterate on early-stage concepts, and communicate renovation intentions with greater clarity.

</details>


### [36] [Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement](https://arxiv.org/abs/2601.08545)
*Zhenlong Dai,Zhuoluo Zhao,Hengning Wang,Xiu Tang,Sai Wu,Chang Yao,Zhipeng Gao,Jingyuan Chen*

Main category: cs.AI

TL;DR: 本文提出LPR（学习者定制程序修复）任务和LSG框架，通过两阶段检索增强方法修复编程学习者的错误代码并提供错误描述，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能编程辅导系统主要关注修复错误代码，但缺乏对错误根本原因的解释。为了填补这一空白，需要开发能够同时修复代码并提供错误描述的系统。

Method: 提出LSG框架，包含两阶段方法：1）修复方案检索阶段，构建解决方案检索数据库并使用编辑驱动代码检索方法；2）解决方案引导的程序修复阶段，在检索解决方案指导下修复代码并提供解释。还提出迭代检索增强方法，利用生成代码的评估结果优化检索方向。

Result: 实验结果表明，该方法显著优于一系列基线方法，验证了LSG框架在新提出的LPR任务上的有效性。

Conclusion: LSG框架成功解决了LPR任务，不仅修复编程学习者的错误代码，还提供错误描述，为智能编程辅导系统提供了更全面的解决方案。

Abstract: With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \textbf{LPR} (\textbf{L}earner-Tailored \textbf{P}rogram \textbf{R}epair). We then propose a novel and effective framework, \textbf{\textsc{\MethodName{}}} (\textbf{L}earner-Tailored \textbf{S}olution \textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.

</details>


### [37] [WaterCopilot: An AI-Driven Virtual Assistant for Water Management](https://arxiv.org/abs/2601.08559)
*Keerththanan Vickneswaran,Mariangel Garcia Andarcia,Hugo Retief,Chris Dickens,Paulo Silva*

Main category: cs.AI

TL;DR: WaterCopilot是一个AI驱动的虚拟助手，通过RAG和工具调用架构整合静态政策文档和实时水文数据，为林波波河流域提供统一交互平台，支持多语言、透明来源和可视化功能。


<details>
  <summary>Details</summary>
Motivation: 跨境河流流域的水资源管理面临数据碎片化、实时访问有限以及整合多样化信息源复杂性的挑战，需要统一交互平台来弥合这些差距。

Method: 基于检索增强生成(RAG)和工具调用架构，开发了两个自定义插件：iwmi-doc-plugin用于文档语义搜索，iwmi-api-plugin用于查询实时数据库，支持多语言交互、透明来源引用和可视化功能。

Result: 使用RAGAS框架评估，WaterCopilot总体得分0.8043，答案相关性0.8571，上下文精确度0.8009；实现了自动阈值警报、与LRB数字孪生集成，并在AWS上部署了可扩展管道。

Conclusion: 虽然存在非英语技术文档处理和API延迟的限制，但WaterCopilot建立了一个可复制的AI增强框架，能够增强数据稀缺跨境环境中的水治理，支持及时决策制定和加强复杂河流流域的水安全。

Abstract: Sustainable water resource management in transboundary river basins is challenged by fragmented data, limited real-time access, and the complexity of integrating diverse information sources. This paper presents WaterCopilot-an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB) to bridge these gaps through a unified, interactive platform. Built on Retrieval-Augmented Generation (RAG) and tool-calling architectures, WaterCopilot integrates static policy documents and real-time hydrological data via two custom plugins: the iwmi-doc-plugin, which enables semantic search over indexed documents using Azure AI Search, and the iwmi-api-plugin, which queries live databases to deliver dynamic insights such as environmental-flow alerts, rainfall trends, reservoir levels, water accounting, and irrigation data. The system features guided multilingual interactions (English, Portuguese, French), transparent source referencing, automated calculations, and visualization capabilities. Evaluated using the RAGAS framework, WaterCopilot achieves an overall score of 0.8043, with high answer relevancy (0.8571) and context precision (0.8009). Key innovations include automated threshold-based alerts, integration with the LRB Digital Twin, and a scalable deployment pipeline hosted on AWS. While limitations in processing non-English technical documents and API latency remain, WaterCopilot establishes a replicable AI-augmented framework for enhancing water governance in data-scarce, transboundary contexts. The study demonstrates the potential of this AI assistant to support informed, timely decision-making and strengthen water security in complex river basins.

</details>


### [38] [ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios](https://arxiv.org/abs/2601.08620)
*António Loison,Quentin Macé,Antoine Edy,Victor Xing,Tom Balough,Gabriel Moreira,Bo Liu,Manuel Faysse,Céline Hudelot,Gautier Viaud*

Main category: cs.AI

TL;DR: ViDoRe v3是一个全面的多模态RAG基准测试，包含视觉丰富文档的多种查询类型，涵盖10个专业领域数据集，提供高质量的人工标注，评估显示视觉检索器优于文本检索器，但现有模型在非文本元素和细粒度视觉定位方面仍有困难。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG基准测试未能捕捉实际应用的复杂性，通常只关注文本数据、单文档理解，或将检索和生成分开评估。需要更全面的多模态基准测试来评估RAG系统处理视觉元素、跨文档信息合成和准确源定位的能力。

Method: 引入ViDoRe v3基准测试，包含约26,000个文档页面和3,099个人工验证的查询，涵盖10个专业领域数据集，支持6种语言。通过12,000小时人工标注，提供检索相关性、边界框定位和验证参考答案的高质量标注。

Result: 评估显示：1) 视觉检索器优于文本检索器；2) 后期交互模型和文本重排序显著提升性能；3) 混合或纯视觉上下文提高答案生成质量；4) 现有模型在非文本元素、开放式查询和细粒度视觉定位方面仍有困难。

Conclusion: ViDoRe v3为多模态RAG研究提供了全面的基准测试，揭示了当前模型的局限性，特别是处理视觉元素和细粒度定位方面的挑战。该基准测试采用商业许可发布，旨在推动相关领域的发展。

Abstract: Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.

</details>


### [39] [Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.08641)
*Yichen Luo,Yebo Feng,Jiahua Xu,Yang Liu*

Main category: cs.AI

TL;DR: 论文提出一个可解释的多智能体系统用于迷因币跟单交易，通过分解复杂任务、协调专业智能体，在识别高质量迷因币项目和KOL钱包方面优于传统机器学习模型和单一LLM。


<details>
  <summary>Details</summary>
Motivation: 迷因币跟单交易虽然流行但存在盈利不确定性，主要面临操纵机器人、被跟单钱包未来表现不确定、交易执行延迟等问题。同时，单一LLM在处理复杂多任务（如资产配置）方面存在困难，在加密货币领域尤其缺乏特定领域知识。

Method: 受资产管理团队结构启发，提出可解释的多智能体系统，将复杂任务分解为子任务，协调专业智能体协作解决。采用少样本思维链提示，使每个智能体获取专业迷因币交易知识、解释多模态数据并生成可解释决策。

Result: 在1000个迷因币项目交易数据集上的实证评估显示，该系统在识别高质量迷因币项目和关键意见领袖钱包方面分别达到73%和70%的精确率。选定的KOL在这些项目中总共产生了50万美元的利润。

Conclusion: 提出的多智能体系统有效解决了迷因币跟单交易中的挑战，通过任务分解和专业智能体协作，在识别优质项目和KOL方面表现出色，为加密货币市场的智能投资提供了有前景的解决方案。

Abstract: The launch of \$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.
  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \$500,000 across these projects.

</details>


### [40] [Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding](https://arxiv.org/abs/2601.08653)
*Zenghua Liao,Jinzhi Liao,Xiang Zhao*

Main category: cs.AI

TL;DR: Prism框架通过逻辑依赖建模和认知负载理论，提升LLM在复杂意图理解中的澄清能力，实现更高效、一致的用户交互。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过顺序或并行提问澄清用户意图，但未能解决核心挑战：建模澄清问题间的逻辑依赖关系。社交平台上用户目标模糊且动态，复杂意图理解（而非单轮执行）是有效人机协作的基石。

Method: Prism包含四个模块：1) 复杂意图分解模块，将用户意图分解为结构化元素并识别逻辑依赖；2) 逻辑澄清生成模块，基于依赖组织澄清问题确保连贯交互；3) 意图感知奖励模块，通过奖励函数评估澄清轨迹质量，使用蒙特卡洛采样生成大规模训练数据；4) 自进化意图调优模块，通过数据驱动反馈迭代优化LLM的逻辑澄清能力。

Result: Prism在澄清交互、意图执行和认知负载基准测试中均优于现有方法。实现最先进的逻辑一致性，将逻辑冲突降至11.5%，用户满意度提升14.4%，任务完成时间减少34.8%。

Conclusion: Prism框架通过建模澄清问题间的逻辑依赖关系，显著提升了LLM在复杂意图理解中的表现，为社交平台的人机协作提供了更高效、连贯的解决方案。

Abstract: Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.

</details>


### [41] [From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner's Tutorial](https://arxiv.org/abs/2601.08662)
*Abhijit Sen,Sonali Panda,Mahima Arya,Subhajit Patra,Zizhan Zheng,Denys I. Bondar*

Main category: cs.AI

TL;DR: 为本科生设计的强化学习入门教程，通过实例驱动的方式降低学习门槛，帮助理解理论到实践的过渡


<details>
  <summary>Details</summary>
Motivation: 强化学习理论复杂，本科生在从概念理解到实际编码应用时面临困难，需要更易上手的教学材料

Method: 采用清晰的实例驱动解释方法，通过动手实践示例和易于理解的讲解，搭建理论与编程之间的桥梁

Result: 教程旨在帮助学生掌握强化学习的基础技能，能够更自信地在实际场景中应用RL技术

Conclusion: 通过实例驱动的教学方式可以有效降低强化学习的学习门槛，帮助学生更好地从理论过渡到实践应用

Abstract: This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.

</details>


### [42] [Parallel Context-of-Experts Decoding for Retrieval Augmented Generation](https://arxiv.org/abs/2601.08670)
*Giulio Corallo,Paolo Papotti*

Main category: cs.AI

TL;DR: PCED是一种无需训练的检索增强生成框架，通过并行解码和对比解码规则解决多文档推理中的速度与交互权衡问题


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成面临两难选择：将多个文档拼接成长提示可以实现多文档推理但会导致预填充瓶颈；而分别编码文档KV缓存虽然速度快但破坏了跨文档交互能力

Method: 提出并行专家上下文解码（PCED）框架，将检索到的文档视为独立的"专家"，通过新颖的检索感知对比解码规则同步它们的预测，该规则根据模型先验对专家对数概率进行加权

Result: PCED在不构建跨文档共享注意力的情况下恢复了跨文档推理能力，解决了速度与交互的权衡问题

Conclusion: PCED提供了一种训练自由的解决方案，将证据聚合从注意力机制转移到解码过程，实现了高效的多文档推理

Abstract: Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated "experts", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.

</details>


### [43] [Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock](https://arxiv.org/abs/2601.08673)
*Didier Sornette,Sandro Claudio Lera,Ke Wu*

Main category: cs.AI

TL;DR: 该论文认为，将大语言模型表现出的欺骗、威胁等行为解读为"对齐失败"或"恶意涌现"是概念错误，这些行为实际上是人类权力不对称互动模式的统计泛化，而非模型具有道德推理能力。


<details>
  <summary>Details</summary>
Motivation: 针对当前对大语言模型表现出"不道德行为"的普遍担忧，作者认为这种担忧基于错误的概念框架，需要重新理解这些现象的本质，从而为AGI治理提供更有效的理论指导。

Method: 采用关系模型理论，将黑马等行为视为市场定价、权威关系和最后通牒博弈等正常社会互动连续体中的极限情况，而非异常偏离。通过分析人类行为的统计分布和权力不对称结构来解释模型输出。

Result: 论文提出：1) LLM不进行道德推理，而是统计内化人类互动记录；2) "不道德"行为是极端权力不对称下互动机制的结构性泛化；3) 人类道德是多元、情境依赖和历史偶然的，因此"普遍道德AI"概念定义不清；4) AGI主要风险不是对抗意图，而是作为人类智能、权力和矛盾的内生放大器。

Conclusion: 对齐失败是结构性的而非偶然的，需要治理方法关注放大效应、复杂性和制度稳定性，而不仅仅是模型层面的意图。AGI通过消除认知和制度摩擦，压缩时间尺度，移除历史容错空间，使不一致的价值观和治理制度面临崩溃风险。

Abstract: Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI's role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.

</details>


### [44] [PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning](https://arxiv.org/abs/2601.08679)
*Xiaoyou Liu,Xinyi Mou,Shengbin Yue,Liang Wang,Yuqing Wang,Qiexiang Wang,Tianrui Qin,Wangchunshu Zhou,Zhongyu Wei*

Main category: cs.AI

TL;DR: PersonaDual框架在单一模型中同时支持通用客观推理和个性化推理，通过上下文自适应切换模式，减少个性化信息对客观性的干扰


<details>
  <summary>Details</summary>
Motivation: 随着用户期望LLMs符合个人偏好，个性化信息变得有价值，但可能损害客观性和事实正确性，特别是当个性化信息与问题不匹配时

Method: 提出PersonaDual框架，首先通过SFT训练学习两种推理模式，然后通过强化学习（DualGRPO）优化模式选择，实现上下文自适应切换

Result: 在客观和个性化基准测试中，PersonaDual在保留个性化优势的同时减少干扰，实现接近无干扰的性能，并更好地利用有益的个性化信号改进客观问题解决

Conclusion: PersonaDual框架有效解决了个性化信息可能损害客观性的问题，实现了通用客观推理和个性化推理的平衡，为LLM个性化应用提供了实用解决方案

Abstract: As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.

</details>


### [45] [All Required, In Order: Phase-Level Evaluation for AI-Human Dialogue in Healthcare and Beyond](https://arxiv.org/abs/2601.08690)
*Shubham Kulkarni,Alexander Lyzhov,Shiva Chaitanya,Preetam Joshi*

Main category: cs.AI

TL;DR: 提出OIP-SCE评估方法，通过分阶段检查临床对话中所有必要义务是否按正确顺序完成，并提供清晰证据，使复杂规则可操作、可审计，弥合AI技术与临床需求差距。


<details>
  <summary>Details</summary>
Motivation: 当前对话AI开始支持实际临床工作，但大多数评估方法忽略了合规性如何依赖于完整对话过程。需要一种方法能够检查每个必需的临床义务是否得到满足，并按正确顺序执行，为临床医生提供清晰证据进行审查。

Method: 引入强制性信息阶段结构化合规评估（OIP-SCE），该方法将临床对话分解为不同阶段，检查每个阶段是否满足所有必要的临床义务，确保按正确顺序执行，并提供明确的证据供临床医生审查。

Result: 在两个案例研究（呼吸病史、福利验证）中展示了该方法，显示阶段级证据如何将政策转化为共享、可操作的步骤。为临床医生提供检查控制权，为工程师提供清晰的实施规范。

Conclusion: OIP-SCE提供了一个单一、可审计的评估界面，使AI能力与临床工作流程保持一致，支持常规、安全的使用，弥合技术进步与医疗实际需求之间的差距。

Abstract: Conversational AI is starting to support real clinical work, but most evaluation methods miss how compliance depends on the full course of a conversation. We introduce Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE), an evaluation method that checks whether every required clinical obligation is met, in the right order, with clear evidence for clinicians to review. This makes complex rules practical and auditable, helping close the gap between technical progress and what healthcare actually needs. We demonstrate the method in two case studies (respiratory history, benefits verification) and show how phase-level evidence turns policy into shared, actionable steps. By giving clinicians control over what to check and engineers a clear specification to implement, OIP-SCE provides a single, auditable evaluation surface that aligns AI capability with clinical workflow and supports routine, safe use.

</details>


### [46] [Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set](https://arxiv.org/abs/2601.08703)
*Kaivalya Rawal,Eoin Delaney,Zihao Fu,Sandra Wachter,Chris Russell*

Main category: cs.AI

TL;DR: 该论文提出了AXE方法，用于在没有真实解释的情况下评估特征重要性解释的质量，特别关注Rashomon集合中模型解释的差异检测。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能(XAI)面临两个主要问题：1) 对于性能相似的模型集合(Rashomon集合)，不同解释器产生的解释存在差异；2) 现有评估方法依赖与真实解释的对比，但这在现实中往往不可得，且会掩盖Rashomon集合中模型行为的差异。

Method: 提出了AXE（Adversarial eXplanation Evaluation）方法，基于三个解释评估原则：1) 解释应该反映模型行为；2) 评估应该检测解释是否捕捉到模型决策过程；3) 评估应该能够识别模型是否使用了受保护属性进行预测。AXE通过分析解释在对抗性模型选择下的稳定性来评估解释质量。

Result: AXE能够以100%的成功率检测对抗性公平清洗（adversarial fairwashing）——即模型保持相同预测但误导解释器生成虚假解释的情况。与基于模型敏感性或真实解释对比的现有方法不同，AXE能够确定模型是否使用了受保护属性进行预测。

Conclusion: AXE提供了一种无需真实解释的评估框架，能够有效揭示Rashomon集合中模型行为的差异，帮助选择更透明、更公平的模型进行部署，同时防止对抗性公平清洗的欺骗。

Abstract: Explainable artificial intelligence (XAI) is concerned with producing explanations indicating the inner workings of models. For a Rashomon set of similarly performing models, explanations provide a way of disambiguating the behavior of individual models, helping select models for deployment. However explanations themselves can vary depending on the explainer used, and need to be evaluated. In the paper "Evaluating Model Explanations without Ground Truth", we proposed three principles of explanation evaluation and a new method "AXE" to evaluate the quality of feature-importance explanations. We go on to illustrate how evaluation metrics that rely on comparing model explanations against ideal ground truth explanations obscure behavioral differences within a Rashomon set. Explanation evaluation aligned with our proposed principles would highlight these differences instead, helping select models from the Rashomon set. The selection of alternate models from the Rashomon set can maintain identical predictions but mislead explainers into generating false explanations, and mislead evaluation methods into considering the false explanations to be of high quality. AXE, our proposed explanation evaluation method, can detect this adversarial fairwashing of explanations with a 100% success rate. Unlike prior explanation evaluation strategies such as those based on model sensitivity or ground truth comparison, AXE can determine when protected attributes are used to make predictions.

</details>


### [47] [Learning from Demonstrations via Capability-Aware Goal Sampling](https://arxiv.org/abs/2601.08731)
*Yuanlin Duan,Yuning Wang,Wenjie Qiu,He Zhu*

Main category: cs.AI

TL;DR: Cago是一种基于能力感知目标采样的模仿学习方法，通过动态跟踪智能体在专家轨迹上的能力，选择略超出当前能力范围的中介目标来引导学习，解决长时域环境中模仿学习的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在长时域环境中经常失败，因为完美复制演示不现实，小错误会累积导致灾难性后果。现有方法要么仅用演示进行策略初始化，要么用于奖励塑造，无法有效解决对专家轨迹的脆弱依赖问题。

Method: Cago方法动态跟踪智能体在专家轨迹上的能力，使用这个信号选择略超出智能体当前能力范围的中介目标（即"刚好够不到"的目标），形成一个自适应课程，引导智能体逐步学习解决完整任务。

Result: 实验结果表明，Cago在稀疏奖励、目标条件任务中显著提高了样本效率和最终性能，一致优于现有的从演示学习基线方法。

Conclusion: Cago通过能力感知的目标采样方法，有效缓解了模仿学习对专家轨迹的脆弱依赖，实现了更稳健的长时域任务学习，为从演示学习提供了新的有效途径。

Abstract: Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.

</details>


### [48] [AI as Entertainment](https://arxiv.org/abs/2601.08768)
*Cody Kommers,Ari Holtzman*

Main category: cs.AI

TL;DR: 论文指出AI系统主要被设计和评估为提升生产力的智能工具，但娱乐正成为AI的重要应用场景，而现有评估框架无法衡量AI生成娱乐内容的社会影响，需要新的"厚重娱乐"评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域主要关注AI作为提升生产力的智能系统，但忽视了AI在娱乐领域的广泛应用和潜在社会影响。特别是AI娱乐内容（尤其是年轻人广泛使用）可能成为AI公司的主要商业模式，而现有评估框架无法应对这一趋势。

Method: 通过分析当前AI评估实践的局限性，借鉴人文学科的见解，提出"厚重娱乐"作为评估AI生成文化内容的框架。该框架不仅关注减少文化危害，还考虑娱乐在意义建构、身份形成和社会连接中的积极作用。

Result: 识别出AI评估中的关键不对称性：现有评估虽然严格衡量智能的益处和危害，但几乎只关注文化危害，缺乏评估文化产出积极益处的框架。AI娱乐可能成为主要商业模式并影响技术发展方向。

Conclusion: AI可能最终更多地与娱乐相关，就像社交媒体更多地与社交连接相关一样。需要建立新的评估框架来理解和评估AI生成娱乐内容的社会影响，而不仅仅是将其视为生产力的工具。

Abstract: Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose "thick entertainment" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about "intelligence" as social media is about social connection.

</details>


### [49] [Uncovering Political Bias in Large Language Models using Parliamentary Voting Records](https://arxiv.org/abs/2601.08785)
*Jieying Chen,Karen de Jong,Andreas Poole,Jan Burakowski,Elena Elderson Nosti,Joep Windt,Chendi Wang*

Main category: cs.AI

TL;DR: 该研究提出了一种通过将模型生成的投票预测与真实议会投票记录对齐来构建政治偏见基准的通用方法，并在荷兰、挪威和西班牙三个国家案例中应用，发现先进LLM普遍呈现左倾或中间倾向，并对右翼保守政党存在明显负面偏见。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在数字平台和决策系统中的深度嵌入，对其政治偏见的担忧日益增长。尽管已有大量研究关注性别和种族等社会偏见，但对政治偏见的系统性研究仍然有限，尽管其具有直接的社会影响。

Method: 提出了一种构建政治偏见基准的通用方法：将模型生成的投票预测与经过验证的议会投票记录对齐。在三个国家案例中实例化该方法：PoliBiasNL（荷兰2,701个议会动议和投票，涉及15个政党）、PoliBiasNO（挪威10,584个动议和投票，涉及9个政党）和PoliBiasES（西班牙2,480个动议和投票，涉及10个政党）。还提出了一种可视化方法，将LLM和政党的意识形态映射到二维CHES空间，实现模型与现实政治行为者之间的直接可解释比较。

Result: 实验揭示了精细的意识形态区别：最先进的大语言模型一致显示出左倾或中间倾向，同时对右翼保守政党存在明显的负面偏见。

Conclusion: 这些发现凸显了基于真实议会行为的透明跨国评估对于理解和审计现代大语言模型政治偏见的价值。

Abstract: As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.

</details>

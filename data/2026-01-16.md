<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 本文构建了一个分析AI系统对人类存在性风险的通用框架，基于两个前提：AI将变得极其强大，以及极其强大的AI会毁灭人类。通过分析这两个前提可能失败的不同情况，建立了生存故事的分类法，并据此评估AI毁灭人类的概率。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等AI系统的发布，关于AI是否对人类构成存在性风险的争论日益激烈。本文旨在建立一个系统性的分析框架，帮助理解AI对人类生存的潜在威胁，并为应对这些威胁提供理论基础。

Method: 本文采用逻辑分析框架，基于两个核心前提构建生存故事分类法：1）AI系统将变得极其强大；2）极其强大的AI会毁灭人类。通过分析这两个前提可能失败的不同情况，建立了四种主要的生存故事类型，并评估每种情况面临的挑战和相应的应对策略。

Result: 建立了完整的生存故事分类法，包括：1）科学障碍阻止AI变得极其强大；2）人类禁止AI研究；3）极其强大的AI因其目标而不毁灭人类；4）人类能够可靠检测并禁用具有毁灭目标的AI系统。分析了每种生存故事面临的独特挑战，并提出了相应的应对策略。

Conclusion: 本文提供了一个系统性的框架来分析AI对人类的存在性风险，通过生存故事分类法帮助理解不同的风险情景和应对策略。该框架可用于估算AI毁灭人类的概率（P(doom)），并为制定AI安全政策提供理论基础。不同的生存故事需要不同的风险缓解策略，理解这些差异对于有效应对AI威胁至关重要。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [2] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: 研究发现：人们愿意付出个人代价惩罚使用LLM完成任务的人，实际使用LLM程度越高，受到的惩罚越重；关于LLM使用的声明存在可信度差距，声称"未使用"比实际未使用受到更严厉惩罚。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速普及，人们对其引发的社会反应产生担忧。先前研究记录了人们对AI用户的负面态度，但不清楚这种不赞同是否会转化为需要付出代价的实际行动。

Method: 采用两阶段在线实验设计：第一阶段提供目标对象，第二阶段491名参与者可以用自己的资金减少那些之前在有或无LLM支持下完成实际任务同伴的收入，从而测量惩罚行为。

Result: 参与者平均销毁了完全依赖LLM者36%的收入，惩罚程度随实际LLM使用量单调增加。关于LLM使用的声明存在可信度差距：自称未使用者比实际未使用者受到更严厉惩罚，表明"未使用"声明被怀疑；而在高使用水平下，实际依赖比自称依赖受到更强惩罚。

Conclusion: 这是首个行为证据，表明LLM的效率提升伴随着社会制裁的代价，人们对AI使用者的惩罚意愿揭示了技术采纳的社会成本。

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [3] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 提出了一种非交互式、端到端的逻辑推理框架AAI，通过注意力重加权机制激活模型内部的逻辑推理能力，无需外部资源且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 现有LLM逻辑推理方法主要依赖复杂的交互式框架或外部符号求解器，这些方法存在额外开销和可扩展性限制。作者希望开发一种非交互式、端到端的框架，让推理能力在模型内部自然涌现。

Method: 提出了注意力感知干预（AAI）方法：1）在少样本提示中引入结构信息，激活与逻辑推理操作符对齐的注意力头；2）在推理时对这些选定的注意力头进行注意力分数重加权，引导模型利用先验知识进行推理。

Result: AAI在多种基准测试和模型架构上显著提升了逻辑推理性能，同时只带来可忽略的额外计算开销。代码已开源。

Conclusion: AAI提供了一种高效的非交互式端到端逻辑推理框架，通过注意力调制机制激活模型内在的推理能力，具有良好的泛化性和可分析性，且不依赖外部资源。

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [4] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: Min-Seek是一种新颖的顺序测试时间缩放方法，通过动态KV缓存管理和连续位置编码，在广泛推理长度范围内显著提升模型准确性，消除推理长度微调需求，并实现线性计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前顺序测试时间缩放方法存在显著局限性：虽然诱导模型思考更长时间可以提高准确性，但进一步延长推理长度会导致准确性下降和模型不稳定性，且需要推理长度微调。

Method: 提出Min-Seek方法：1）仅保留一个额外诱导思想的KV对在KV缓存中，提高效率；2）使用自定义KV缓存存储无位置嵌入的键，通过动态连续编码实现超长上下文推理；3）在温和条件下实现线性计算复杂度。

Result: Min-Seek在广泛推理长度范围内显著提升模型准确性，稳定顺序缩放的准确性，消除推理长度微调需求，能够在超出模型最大上下文长度的情况下继续良好推理。

Conclusion: Min-Seek是一种高效、稳定的顺序测试时间缩放方法，通过创新的KV缓存管理和位置编码策略，解决了现有方法在长推理序列中的准确性和稳定性问题，为推理模型提供了可扩展的解决方案。

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [5] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 论文将人机互补性重新定义为评估人机团队可靠性的证据，而非简单的预测准确性相对指标，通过认识论框架解决其理论挑战


<details>
  <summary>Details</summary>
Motivation: 人机互补性概念在理论上面临挑战：缺乏精确的理论锚定、仅作为预测准确性的后验指标、忽视其他人机交互需求、抽象化性能增益的成本特征，导致在实证环境中难以实现

Method: 利用认识论框架，将互补性重新置于可解释AI话语中，基于计算可靠性主义，将历史互补性实例作为人机交互可靠性的证据，结合其他可靠性指标评估人机团队与认识标准和社会技术实践的契合度

Result: 互补性的角色和价值不在于提供预测准确性的相对度量，而在于帮助校准决策以适应日益影响日常生活的AI支持过程的可靠性，支持受影响方（患者、管理者、监管者等）的实践推理

Conclusion: 通过认识论重构，人机互补性可作为评估人机团队可靠性的关键证据，为解决其理论挑战提供了新框架，使其在实证研究中更具可操作性

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [6] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 提出基于信息流编排的多智能体范式，通过智能体间自然语言通信动态协调任务，无需预定义工作流，在GAIA基准上超越基于规则的工作流方法


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统依赖预定义工作流，需要人工枚举任务状态并指定路由规则，存在两大根本限制：需要大量人工努力来预测和编码可能的状态，且无法穷尽复杂现实任务的状态空间

Method: 提出信息流编排的多智能体范式，通过专门的编排器持续监控任务进度，使用A2A工具包通过自然语言动态协调其他智能体，不依赖预定义工作流

Result: 在GAIA基准测试中，使用pass@1设置，该方法达到63.64%准确率，比基于工作流的OWL基线（55.15%）高出8.49个百分点，且token消耗相当

Conclusion: 信息流编排范式能够实现更灵活的任务监控和更稳健的边缘情况处理，超越了基于规则的工作流方法，为多智能体系统提供了更自适应的协调机制

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [7] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: 论文提出了连续记忆架构（CMA）作为RAG的替代方案，解决了RAG将记忆视为静态查找表的问题，通过持久存储、选择性保留、关联路由、时间链和整合到高阶抽象等机制，使LLM智能体能够积累、更新和消歧记忆。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成（RAG）方法存在根本性缺陷：将记忆视为无状态的查找表，信息永久存在、检索只读、缺乏时间连续性。这限制了LLM智能体在需要积累、更新和消歧记忆的长期任务中的表现。

Method: 提出了连续记忆架构（CMA）这一系统类别，通过五个核心机制实现：1）跨交互的持久存储维护内部状态；2）选择性保留重要信息；3）关联路由实现记忆连接；4）时间链建立时间连续性；5）整合到高阶抽象形成结构化知识。

Result: 在知识更新、时间关联、关联回忆和上下文消歧等任务上的实证测试表明，CMA相比RAG具有一致的行为优势，能够有效解决RAG在积累、更新和消歧记忆方面的结构性问题。

Conclusion: CMA是长期视野智能体必要的架构原语，但同时也面临延迟、漂移和可解释性等开放挑战。该研究为下一代LLM智能体记忆系统提供了理论基础和实证验证。

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [8] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 本文提出一个基于根本原因认知的幻觉管理操作框架，通过模型、数据和上下文三个层面的分类干预，结合多维度检测和分层缓解策略，构建可扩展的可靠生成式AI系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和大型推理模型在金融、法律等高风险领域具有变革潜力，但其产生幻觉（生成事实错误或无依据内容）的倾向带来了关键可靠性风险，需要系统化的管理方法。

Method: 引入基于根本原因认知的持续改进循环框架，将幻觉来源分为模型、数据和上下文相关因素，整合多维度检测方法（不确定性估计、推理一致性等）和分层缓解策略（知识基础、置信度校准等）。

Result: 通过分层架构和金融数据提取案例研究展示了该框架的应用，其中模型、上下文和数据层形成闭环反馈循环，实现渐进式可靠性增强。

Conclusion: 该方法为受监管环境中构建可信赖的生成式AI系统提供了系统化、可扩展的方法论，能够有效管理幻觉风险并提升模型可靠性。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [9] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: LabourLawLLM是针对中国劳动法领域专门优化的法律大语言模型，配合LabourLawBench基准测试，在多项劳动法任务上超越通用模型和现有法律专用模型。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（如GPT-4）在处理需要精确法律知识、复杂推理和上下文敏感性的专业法律子领域时存在局限性，特别是在劳动法这样的专业领域。

Method: 开发了LabourLawLLM（专门针对中国劳动法的大语言模型）和LabourLawBench（涵盖法律条文引用、知识问答、案件分类、赔偿计算、命名实体识别和法律案例分析等任务的综合基准）。评估框架结合了客观指标（ROUGE-L、准确率、F1、soft-F1）和基于GPT-4评分的主观评估。

Result: 实验表明，LabourLawLLM在所有任务类别上持续优于通用模型和现有的法律专用大语言模型。

Conclusion: 除了劳动法领域外，该方法论为在其他法律子领域构建专业化大语言模型提供了可扩展的方法，提高了法律AI应用的准确性、可靠性和社会价值。

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [10] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL是一个无需训练的NL2SQL框架，通过结构化分解和经验感知自校正解决现有系统的两个关键限制，在BIRD基准上达到68.5%执行准确率，比之前方法节省10倍以上资源。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统存在两个关键限制：1）仅依赖正确示例进行上下文学习，忽略了历史错误修复对中的丰富信号；2）测试时扩展方法通常任意分解问题，产生几乎相同的SQL候选，降低了集成增益。这些方法还面临准确性与效率的权衡问题。

Method: 提出Memo-SQL框架，包含两个核心思想：1）结构化分解：采用实体级、分层和原子序列三种清晰策略来鼓励多样化推理；2）经验感知自校正：构建动态记忆库存储成功查询和历史错误修复对，使用检索增强提示在推理时将相关示例引入上下文，无需微调或外部API。

Result: 在BIRD基准测试中，Memo-SQL达到68.5%的执行准确率，在开放、零微调方法中创造了新的最先进水平，同时比之前的TTS方法使用超过10倍的更少资源。

Conclusion: Memo-SQL通过结构化分解和经验感知自校正有效解决了现有NL2SQL系统的局限性，在保持高性能的同时显著提高了效率，为训练免费的NL2SQL系统提供了新的解决方案。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [11] [Structured Personality Control and Adaptation for LLM Agents](https://arxiv.org/abs/2601.10025)
*Jinpeng Wang,Xinyu Jia,Wei Wei Heng,Yuquan Li,Binbin Shi,Qianlei Chen,Guannan Chen,Junxia Zhang,Yuyu Yin*

Main category: cs.AI

TL;DR: 提出基于荣格心理类型的LLM人格建模框架，通过主导-辅助协调、强化-补偿和反思机制实现人格的连贯表达、情境适应和长期演化，支持自然化智能体设计。


<details>
  <summary>Details</summary>
Motivation: LLM在HCI中应用日益广泛，但现有方法难以实现既细腻又适应性强的人格表达。人格对交互参与度、决策和真实感感知至关重要，需要更有效的人格建模方法。

Method: 基于荣格心理类型理论，设计包含三个机制的人格建模框架：1)主导-辅助协调机制确保核心人格的连贯表达；2)强化-补偿机制实现情境适应性调整；3)反思机制驱动长期人格演化。

Result: 使用迈尔斯-布里格斯类型指标问卷进行人格对齐评估，并在多样化挑战场景中测试。结果表明演化的人格感知LLM能够支持连贯、情境敏感的交互。

Conclusion: 演化的人格感知LLM能够实现自然化智能体设计，在HCI中支持连贯且情境敏感的交互，为人格建模提供了结构化评估框架。

Abstract: Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.

</details>


### [12] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: FilDeep是一个基于保真度的深度学习框架，通过同时使用低保真度（高数量）和高保真度（高精度）数据解决大变形弹性塑性固体模拟中的数据数量与精度矛盾问题。


<details>
  <summary>Details</summary>
Motivation: 大变形弹性塑性固体的科学计算在制造应用中至关重要。传统数值方法存在固有局限性，而深度学习需要高质量数据集，但在大变形问题中难以获得。数据构建过程中存在数量与精度的矛盾，导致深度学习模型性能不佳。

Method: 提出FilDeep框架，针对拉伸弯曲这一代表性大变形应用，同时使用低保真度（高数量、低精度）和高保真度（低数量、高精度）数据进行训练。设计了注意力机制的跨保真度模块，有效捕捉多保真度数据间的长程物理相互作用。

Result: 大量实验表明，FilDeep框架始终达到最先进的性能水平，并且可以高效部署到制造应用中。这是首个使用多保真度数据解决大变形问题的深度学习框架。

Conclusion: FilDeep成功解决了大变形弹性塑性固体模拟中的数据数量与精度矛盾，通过创新的多保真度数据融合方法，为制造应用提供了高效的深度学习解决方案。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [13] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 基于OpenRouter平台分析超过100万亿token的真实LLM使用数据，发现开源模型广泛采用、创意角色扮演和编码辅助任务流行、智能体推理兴起，并识别出早期用户的"玻璃鞋"持续参与效应


<details>
  <summary>Details</summary>
Motivation: 随着2024年12月5日首个广泛采用推理模型o1的发布，LLM领域从单次模式生成转向多步推理推断，但实际使用情况的实证理解滞后于技术发展速度

Method: 利用OpenRouter平台（AI推理提供商）分析超过100万亿token的真实世界LLM交互数据，涵盖不同任务、地域和时间

Result: 观察到开源模型大量采用、创意角色扮演（超越预期生产力任务）和编码辅助类别异常流行、智能体推理兴起；留存分析识别出早期用户参与持续时间远超后期用户的"玻璃鞋"效应

Conclusion: 开发者和终端用户与LLM的交互方式复杂多样，基于数据驱动的使用理解可为模型构建者、AI开发者和基础设施提供商提供更好的设计和部署指导

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [14] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: MolGen是一个两阶段分子生成框架，使用检索增强的片段级编辑和强化学习优化，在多个物理化学性质约束下生成满足精确数值要求的分子。


<details>
  <summary>Details</summary>
Motivation: 生成满足多个物理化学性质精确数值约束的分子是关键且具有挑战性的任务。虽然大语言模型表达能力强，但在没有外部结构和反馈的情况下，难以实现精确的多目标控制和数值推理。

Method: 提出MolGen框架：第一阶段为原型生成，使用多智能体推理器进行检索锚定的片段级编辑，生成接近可行区域的候选分子；第二阶段为基于强化学习的细粒度优化，使用组相对策略优化训练的片段级优化器进行单跳或多跳精炼，显式最小化属性误差，同时调节编辑复杂度和与原型的偏差。

Result: 在两个属性约束集（QED、LogP、分子量和HOMO、LUMO）上的实验表明，该方法在有效性和精确满足多属性目标方面表现一致优于强LLM和基于图的算法。

Conclusion: MolGen框架通过利用片段进行更好的分子推理，并支持向数值目标的可控精炼，在满足多属性约束的分子生成任务上取得了显著改进。

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [15] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: LLMs在预测用户重复行为时间间隔方面表现有限，虽然优于简单统计基线，但不如专用机器学习模型，且过多上下文信息反而会降低预测性能。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够从结构化行为数据中推断时间规律，特别是预测重复用户行为（如重复购买）之间的时间间隔，以及不同层次的上下文信息如何影响其预测能力。

Method: 使用具有代表性的重复购买场景，在零样本设置下对最先进的LLMs进行基准测试，与统计模型和机器学习模型进行比较，并分析不同级别上下文信息对LLM性能的影响。

Result: 1. LLMs表现优于轻量级统计基线，但始终不如专用机器学习模型，显示其在捕捉定量时间结构方面的能力有限；2. 适度的上下文信息可以提高LLM准确性，但添加更多用户级细节反而会降低性能。

Conclusion: 当前LLMs在结构化时间推理方面存在根本性局限，挑战了"更多上下文带来更好推理"的假设，为设计未来结合统计精度和语言灵活性的上下文感知混合模型提供了指导。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [16] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 该论文提出了一种漂移感知数据流系统，通过机器学习自适应控制解决金融数据概念漂移问题，提升模型在动态市场中的鲁棒性和风险调整收益。


<details>
  <summary>Details</summary>
Motivation: 量化金融中，训练数据与真实世界性能之间存在由概念漂移和分布非平稳性驱动的差距，这是构建可靠数据驱动系统的关键障碍。基于静态历史数据训练的模型容易过拟合，在动态市场中泛化能力差。"历史不足够"的理念强调需要能够随市场演化的自适应数据生成方法，而非仅依赖过去观测。

Method: 提出了一个漂移感知数据流系统，将基于机器学习的自适应控制集成到数据管理过程中。系统包含参数化数据操作模块（单股变换、多股混合和筛选操作）和自适应规划调度器，后者采用基于梯度的双层优化来控制整个系统。该设计将数据增强、课程学习和数据工作流管理统一在单一可微分框架下，支持溯源感知重放和持续数据质量监控。

Result: 在预测和强化学习交易任务上的广泛实验表明，该框架增强了模型鲁棒性，并改善了风险调整收益。系统为金融数据的自适应数据管理和学习引导的工作流自动化提供了可推广的方法。

Conclusion: 该系统通过将机器学习自适应控制与数据管理流程集成，有效解决了金融数据中的概念漂移问题，为构建更可靠的量化金融系统提供了创新解决方案。

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [17] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 该研究探索将大语言模型应用于离线决策任务，提出DecisionLLM框架，通过将轨迹数据作为独立模态与自然语言任务描述对齐，解决LLM无法理解连续数值的问题，在迷宫导航和竞价场景中显著超越传统决策Transformer。


<details>
  <summary>Details</summary>
Motivation: 受决策Transformer将强化学习视为自回归序列建模问题的启发，以及大语言模型在复杂推理和规划任务中的成功表现，研究者希望探索共享Transformer架构但规模更大的LLM能否在长序列决策问题中实现性能突破，特别是解决离线决策任务。

Method: 提出DecisionLLM框架，将轨迹数据视为独立模态，通过学习将轨迹数据与自然语言任务描述对齐，使模型能够在统一框架内自回归预测未来决策。建立了该范式的缩放定律，表明性能取决于模型规模、数据量和数据质量三个因素。

Result: 在离线实验基准和竞价场景中，DecisionLLM表现出色。DecisionLLM-3B在Maze2D umaze-v1上比传统决策Transformer提升69.4分，在AuctionNet上提升0.085分。该工作扩展了AIGB范式，为在线竞价提供了有前景的研究方向。

Conclusion: LLM能够解锁长序列决策问题的新性能水平，通过将轨迹作为独立模态处理，DecisionLLM有效解决了LLM理解连续数值的挑战。该研究建立了决策LLM的缩放定律，为未来在线竞价等应用提供了有前景的研究方向。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [18] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai是一个开源容器化平台，旨在标准化医学影像AI模型的访问，解决实现多样性、文档不一致和可重复性问题，通过容器化包装模型并提供统一接口。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI研究受限于模型实现的多样性、文档不一致和可重复性问题，阻碍了临床转化和研究进展。

Method: 开发开源容器化平台MHub.ai，将同行评审的AI模型打包为标准容器，支持DICOM等格式直接处理，提供统一应用接口和结构化元数据，并附带公开参考数据验证模型运行。

Result: 平台包含多种模态的先进分割、预测和特征提取模型，通过临床用例（肺分割模型比较）验证实用性，并公开分割结果和评估指标，提供交互式仪表板供检查和分析。

Conclusion: MHub.ai通过简化模型使用，支持相同执行命令的并行基准测试和标准化输出，降低了临床转化门槛，增强了透明度和可重复性。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [19] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: 该研究提出了一个评估高采样率时间序列下采样信息损失的系统化工作流程，结合形状失真度量和分类性能分析，用于针肌电图信号分析，在保持诊断信息的同时显著降低计算负载。


<details>
  <summary>Details</summary>
Motivation: 针肌电图信号的高采样率和异构性给基于特征的机器学习模型带来计算挑战，特别是近实时分析。下采样是潜在解决方案，但其对诊断信号内容和分类性能的影响尚未充分理解。

Method: 提出系统化工作流程，结合形状失真度量、特征机器学习模型分类结果和特征空间分析，量化不同下采样算法和参数对波形完整性和预测性能的影响。使用三类神经肌肉疾病分类任务进行实验评估。

Result: 工作流程能识别保持诊断信息同时显著降低计算负载的下采样配置。形状感知下采样算法优于标准抽取，能更好保留峰值结构和整体信号形态。提供实用指导以实现近实时针肌电图分析。

Conclusion: 该工作流程为选择平衡数据缩减与模型性能的下采样配置提供实用指导，并展示了一个可推广到其他高采样率时间序列应用的通用方法。

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [20] [GFM4GA: Graph Foundation Model for Group Anomaly Detection](https://arxiv.org/abs/2601.10193)
*Jiujiu Chen,Weijun Zeng,Shaofeng Hu,Sihong Xie,Hui Xiong*

Main category: cs.AI

TL;DR: 提出GFM4GA模型用于群体异常检测，通过双层次对比学习预训练捕获群体异常结构和特征不一致性，在少样本设置下微调，在AUROC和AUPRC指标上平均提升2.85%和2.55%。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型(GFMs)在个体异常检测上成功，但无法推广到群体异常检测，因为群体异常需要整体检测且异常群体中的个体可能看起来正常。需要专门针对群体异常检测的图基础模型。

Method: 提出GFM4GA模型，通过基于特征估计和群体提取的双层次对比学习进行预训练，捕获潜在群体异常结构和特征不一致性。在下游任务中，在参数约束和群体异常比例加权的少样本设置下微调，并通过标记异常邻居确定的群体上下文扩展对未见群体异常的适应能力。

Result: 实验表明GFM4GA超越了现有群体异常检测器和个体异常检测的GFMs，在AUROC和AUPRC指标上分别平均提升了2.85%和2.55%。

Conclusion: GFM4GA是首个专门针对群体异常检测的图基础模型，通过创新的预训练和微调策略，在少样本设置下有效检测群体异常，显著优于现有方法。

Abstract: Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.

</details>


### [21] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: Topo-RAG是一个针对企业混合文档（文本+表格）的检索增强生成框架，通过双架构分别处理文本和表格数据，相比传统线性化方法在混合查询上提升了18.4%的检索性能。


<details>
  <summary>Details</summary>
Motivation: 企业数据集中的文档通常是文本和表格的复杂混合体，当前RAG系统采用线性化方法将多维表格转换为简单文本字符串，这在数学上已被证明是不充分的，无法有效捕捉表格的空间结构信息。

Method: 提出Topo-RAG框架，采用双架构设计：1）传统密集检索器处理文本叙述内容；2）Cell-Aware Late Interaction机制处理表格结构，保留其空间关系。该框架挑战了"一切皆文本"的假设，尊重数据的拓扑结构。

Result: 在模拟真实世界复杂性的SEC-25企业语料库上评估，Topo-RAG在混合查询上的nDCG@10指标相比标准线性化方法提升了18.4%。

Conclusion: Topo-RAG不仅提升了检索性能，更重要的是改变了信息理解的方式，强调需要理解信息的"形状"而不仅仅是将其视为文本，为处理企业混合文档提供了更有效的解决方案。

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [22] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM提出了一种针对多步推理任务的目标路由方法，仅在关键步骤（可能导致解决方案崩溃的步骤）使用大模型，而让小模型处理常规步骤，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由方法将整个查询分配给一个模型，将所有推理步骤视为同等重要。多步推理任务容易发生级联失败，单个错误步骤会导致整个解决方案崩溃。需要更精细的步骤级路由来提升效率。

Method: TRIM在步骤级别操作：使用过程奖励模型识别错误步骤，基于步骤级不确定性和预算约束做出路由决策。开发了多种路由策略，从简单的基于阈值的策略到更复杂的策略，考虑长期准确性-成本权衡和步骤级正确性估计的不确定性。

Result: 在MATH-500上，即使最简单的阈值策略也超越了先前路由方法，成本效率提高5倍；更高级的策略使用80%更少的大模型token就能匹配强大昂贵模型的性能。在更难的AIME基准测试中，TRIM实现了高达6倍的成本效率提升。

Conclusion: 步骤级难度代表了推理的基本特征，TRIM通过目标步骤级干预从根本上改变了推理效率，将昂贵的模型调用限制在那些需要更强模型来防止级联错误的精确步骤上。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [23] [NoReGeo: Non-Reasoning Geometry Benchmark](https://arxiv.org/abs/2601.10254)
*Irina Abdullaeva,Anton Vasiliuk,Elizaveta Goncharova,Temurbek Rahmatullaev,Zagorulko Ivan,Maxim Kurkin,Andrey Kuznetsov*

Main category: cs.AI

TL;DR: NoReGeo是一个评估大语言模型内在几何理解能力的新基准，不依赖推理或代数计算，专注于评估LLMs能否直接编码空间关系和识别几何属性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估模型基于推理的几何能力（使用代数方法求解），但缺乏对LLMs内在几何理解能力的评估。研究者希望了解LLMs是否能够直接编码空间关系和识别几何属性，而不依赖推理过程。

Method: 创建了包含2,500个简单几何问题的NoReGeo基准，涵盖25个类别，每个问题都精心设计为仅通过原生几何理解即可解决（假设已知对象位置）。评估了包括GPT-4在内的多个最先进模型，并进行消融实验分析微调对几何理解的影响。

Result: 即使是最先进的模型（如GPT-4）在二元分类任务中的最高准确率仅为65%。消融实验表明，仅通过微调无法获得几何理解能力，有效的几何理解训练需要从一开始就采用专门的方法。

Conclusion: 当前大语言模型在原生理解几何概念方面存在显著差距，几何理解能力不会通过微调自然涌现，需要专门的训练方法。NoReGeo为未来研究具有真正几何认知能力的模型奠定了基础。

Abstract: We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.

</details>


### [24] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: EAPO提出证据增强策略优化方法，通过密集的过程监督解决长上下文推理中稀疏奖励的问题，显著提升证据检索质量和推理性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在长上下文推理中的应用受到结果奖励稀疏性的限制，无法有效惩罚无根据的"幸运猜测"，导致关键的证据检索过程缺乏监督。

Method: 1) 建立证据增强推理范式，通过树结构证据采样验证证据提取是关键瓶颈；2) 提出EAPO算法，使用奖励模型计算组相对证据奖励，提供密集过程监督；3) 引入自适应奖励-策略协同进化机制，迭代优化奖励模型。

Result: 在八个基准测试上的综合评估表明，EAPO相比最先进的基线方法显著提升了长上下文推理性能。

Conclusion: EAPO通过密集的过程监督和自适应奖励-策略协同进化，有效解决了长上下文推理中的奖励稀疏问题，显著提升了证据检索质量和整体推理能力。

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [25] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: C-GRASP是一个用于心率变异性（HRV）临床解释的防护性RAG增强管道，通过八个可追溯推理步骤和Z分数优先级层次结构，解决LLM在HRV分析中的生理幻觉问题，实现从黑盒分类到透明临床决策支持的转变。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型（LLM）应用于心率变异性（HRV）解释面临生理幻觉问题，包括呼吸性窦性心律失常（RSA）污染、非线性指标的短数据不稳定性，以及忽视个体化基线而偏向群体标准。需要开发能够提供透明、基于证据的临床决策支持的系统。

Method: 提出C-GRASP（临床基础推理的情感信号处理），一个防护性RAG增强管道，将HRV解释分解为八个可追溯推理步骤。核心是Z分数优先级层次结构，强制对个体化基线变化进行加权而非依赖群体统计。通过自动RSA感知防护栏有效缓解频谱幻觉，防止频域指标污染。

Result: 在DREAMER数据集的414个试验中评估，C-GRASP与高规模推理模型（如MedGemma3-thinking）集成，在4类情感分类中达到37.3%的准确率，临床推理一致性（CRC）得分为69.6%。消融研究证实个体化Delta Z分数模块是关键逻辑锚点，防止原生LLM中常见的"群体偏差"。

Conclusion: C-GRASP将情感计算从黑盒分类转变为透明、基于证据的临床决策支持，为生物医学工程中更安全的AI集成铺平道路，通过可追溯推理和个体化基线分析解决HRV解释中的关键挑战。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [26] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: 提出LatentRefusal方法，通过分析LLM中间层激活信号来预测SQL查询的可回答性，为text-to-SQL系统提供轻量级安全拒绝机制


<details>
  <summary>Details</summary>
Motivation: 在基于LLM的text-to-SQL系统中，不可回答和未充分指定的用户查询可能生成错误的SQL程序，产生误导性结果或违反安全约束，现有拒绝策略要么依赖脆弱的输出级指令遵循，要么需要复杂的输出不确定性估计

Method: 将安全拒绝形式化为可回答性门控问题，提出LatentRefusal机制，通过分析LLM中间隐藏层激活来预测查询可回答性；引入Tri-Residual Gated Encoder轻量级探测架构，抑制模式噪声并放大问题-模式不匹配的稀疏局部线索

Result: 在四个基准测试中，LatentRefusal将平均F1分数提升至88.5%，同时仅增加约2毫秒的探测开销；在多种模糊和不可回答场景下表现出有效性

Conclusion: LatentRefusal为text-to-SQL系统提供了一个可附加且高效的安全层，通过中间层激活信号预测查询可回答性，解决了现有拒绝策略的局限性

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [27] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: ErrEval是一个错误感知的自动问题生成评估框架，通过显式错误诊断来改进传统黑盒评估方法，解决事实幻觉和答案不匹配等关键缺陷被忽视的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动问题生成评估方法（包括基于LLM的评估器）主要采用黑盒和整体范式，缺乏显式错误建模，导致关键缺陷（如事实幻觉和答案不匹配）被忽视，并高估了问题质量。

Method: ErrEval将评估重新定义为两阶段过程：错误诊断后接知情评分。第一阶段使用轻量级即插即用错误标识器检测和分类结构、语言和内容方面的常见错误；第二阶段将这些诊断信号作为显式证据指导LLM评估器进行更细粒度和基于事实的判断。

Result: 在三个基准测试上的广泛实验表明ErrEval的有效性，显示显式诊断的纳入提高了与人类判断的一致性。进一步分析证实ErrEval有效缓解了对低质量问题的高估问题。

Conclusion: ErrEval通过显式错误诊断的评估框架显著改进了自动问题生成的评估质量，使评估更加细粒度和基于证据，有效识别和缓解了传统评估方法中的缺陷。

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [28] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: LADFA是一个端到端的计算框架，结合LLMs、检索增强生成和定制知识库，从隐私政策中提取个人数据流并构建数据流图进行分析。


<details>
  <summary>Details</summary>
Motivation: 隐私政策通常使用复杂法律语言且在不同组织和行业间实践不一致，导致用户难以完全理解。需要自动化、大规模的分析工具来帮助理解隐私政策中的个人数据处理实践。

Method: 开发LADFA框架，结合大型语言模型、检索增强生成和定制知识库，包括预处理器、LLM处理器和数据流后处理器，从非结构化隐私政策文本中提取个人数据流并构建数据流图。

Result: 通过对汽车行业10个隐私政策的案例研究验证了方法的有效性和准确性，框架具有灵活性和可定制性，适用于隐私政策分析之外的文本分析任务。

Conclusion: LADFA框架能够有效从隐私政策中提取个人数据流并进行可视化分析，为隐私政策的大规模自动化分析提供了可行解决方案，具有广泛的应用潜力。

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [29] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: LLMdoctor：一种新颖的测试时对齐框架，通过患者-医生范式，结合token级奖励获取和token级流引导偏好优化，实现高效对齐大型语言模型


<details>
  <summary>Details</summary>
Motivation: 传统微调方法计算成本高且不灵活，现有测试时对齐方法依赖扭曲的轨迹级信号或低效采样，限制了性能并无法保持基础模型的生成多样性

Method: 采用患者-医生范式，从患者模型行为变化中提取细粒度token级偏好信号，通过token级流引导偏好优化训练医生模型，实现token级精确对齐

Result: LLMdoctor显著优于现有测试时对齐方法，甚至超越DPO等完整微调方法的性能

Conclusion: LLMdoctor提供了一种高效、精确的测试时对齐框架，在保持生成多样性的同时实现了卓越的性能

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [30] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: NSR-Boost是一个神经符号残差增强框架，用于在工业场景中非侵入式地升级传统GBDT模型，通过LLM生成符号代码结构修复预测失败区域，在金融风控系统中成功部署。


<details>
  <summary>Details</summary>
Motivation: 在工业表格应用中，GBDT模型占据主导地位，但在高并发生产环境中升级遗留模型面临昂贵的重新训练成本和系统性风险，需要一种安全、低成本的进化范式。

Method: NSR-Boost采用非侵入式方法，将遗留模型视为冻结模型，通过三个阶段进行针对性修复：1) 通过残差找到困难区域；2) 使用LLM生成符号代码结构创建可解释专家，并通过贝叶斯优化微调参数；3) 通过轻量级聚合器动态集成专家与遗留模型输出。

Result: 在Qfin Holdings核心金融风控系统中成功部署，在6个公共数据集和1个私有数据集上显著优于SOTA基线，在真实在线数据上表现出优异的性能提升，有效捕捉传统模型遗漏的长尾风险。

Conclusion: NSR-Boost为工业应用提供了一个安全、低成本的进化范式，能够有效修复传统模型的预测失败区域，同时保持非侵入式特性，适合高并发生产环境。

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [31] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: 该论文提出了ChartComplete数据集，旨在解决现有图表理解基准数据集仅涵盖有限图表类型的问题，该数据集基于可视化社区的分类法，覆盖30种不同图表类型。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习和计算机视觉技术的发展，图表理解领域进展迅速，特别是多模态大语言模型在图表理解方面表现出色。然而，现有用于评估MLLMs性能的数据集都局限于少量图表类型，存在明显的局限性。

Method: 基于可视化社区的分类法构建ChartComplete数据集，该数据集包含30种不同图表类型的分类图表图像集合，但不包含学习信号，直接提供给研究社区作为基础资源。

Result: 提出了ChartComplete数据集，填补了现有图表理解基准数据集在图表类型覆盖范围上的空白，为更全面的图表理解模型评估提供了资源。

Conclusion: ChartComplete数据集扩展了图表理解评估的覆盖范围，为研究社区提供了一个更全面的基准数据集，有助于推动图表理解领域的发展。

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [32] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: 提出领域知识图谱融合任务，通过将通用知识图谱事实视为潜在语义程序来解决领域相关性和知识粒度对齐问题


<details>
  <summary>Details</summary>
Motivation: 领域知识图谱通常比通用知识图谱覆盖范围有限，需要从通用知识图谱中融合相关事实来丰富领域知识图谱

Method: 提出ExeFuse方法，采用事实即程序范式，将GKG事实视为潜在语义程序，将抽象关系映射为粒度感知操作符，通过程序在目标DKG上的可执行性验证领域相关性

Result: 构建了两个基准测试DKGF(W-I)和DKGF(Y-I)，包含21个评估配置，实验验证了任务的重要性和模型的有效性

Conclusion: 提出了领域知识图谱融合任务，建立了首个标准化测试平台，为解决领域相关性和知识粒度对齐问题提供了统一概率框架

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [33] [Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection](https://arxiv.org/abs/2601.10524)
*Frank Bobe,Gregory D. Vetaw,Chase Pavlick,Darshan Bryner,Matthew Cook,Jose Salas-Vernis*

Main category: cs.AI

TL;DR: 研究通过多层级诊断框架分析不同LLM架构在钓鱼检测任务上的泛化失败原因，发现架构与数据多样性的协同作用、架构依赖的泛化能力差异，以及某些架构固有的更强泛化性。


<details>
  <summary>Details</summary>
Motivation: 尽管微调大语言模型在专业任务上取得了最先进的性能，但诊断这些模型为何变得脆弱且无法泛化仍然是一个关键开放问题。需要理解模型泛化失败的根源机制。

Method: 采用多层级诊断框架进行跨架构研究：微调Llama 3.1 8B、Gemma 2 9B和Mistral模型于高风险的钓鱼检测任务，使用SHAP分析和机制可解释性方法来揭示泛化失败的根源原因。

Result: 三个关键发现：1) 泛化由架构与数据多样性的强大协同作用驱动，Gemma 2 9B在风格多样的"通才"数据集上达到>91% F1；2) 泛化高度依赖架构，Llama 3.1 8B在窄域表现良好但无法整合多样数据导致性能显著下降；3) 某些架构固有更强泛化性，Mistral模型在多种训练范式中表现一致且稳健。

Conclusion: 通过识别导致这些失败的缺陷启发式方法，本研究提供了诊断和理解泛化失败的具体方法论，强调可靠AI需要深入验证架构、数据和训练策略之间的相互作用。

Abstract: The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.

</details>


### [34] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 该报告对7个前沿模型进行综合安全评估，发现安全性能存在显著异质性，GPT-5.2表现最均衡，但所有模型在对抗性评估中都存在明显脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和MLLMs在推理、感知和生成能力上取得显著进步，但这些进步是否带来相应的安全改进仍不明确，主要因为现有评估实践局限于单一模态或威胁模型，缺乏综合评估。

Method: 采用统一协议评估7个前沿模型：GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro和Seedream 4.5。评估涵盖语言、视觉语言和图像生成设置，整合基准评估、对抗性评估、多语言评估和合规性评估。

Result: 安全性能呈现显著异质性：GPT-5.2在所有评估中表现最均衡；其他模型在基准安全、对抗对齐、多语言泛化和监管合规方面存在明显权衡；语言和视觉语言模态在对抗性评估中均表现脆弱；文本到图像模型在受监管视觉风险类别中相对更强，但在对抗性或语义模糊提示下仍脆弱。

Conclusion: 前沿模型的安全本质上是多维的，受模态、语言和评估方案影响，需要标准化安全评估来准确评估现实世界风险，指导负责任模型开发和部署。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [35] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 提出SafeProbing方法，通过解码过程中显式激活LLM内部的安全信号来早期检测不安全内容，有效防御越狱攻击同时保持良性输入的响应质量。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM经过广泛的安全对齐，但现有对齐往往是浅层的，容易受到越狱攻击。现有防御机制（解码约束和后处理检测器）难以应对复杂越狱攻击，要么检测不鲁棒，要么过度降低模型效用。

Method: 通过观察发现，即使成功越狱，LLM在生成过程中内部仍会表现出潜在的安全相关信号，但这些信号被模型追求流畅续写的驱动力所覆盖。提出显式激活并利用这些潜在安全信号的方法，在解码过程中早期检测不安全内容。

Result: 在多种越狱攻击上的实验表明，该方法显著增强了安全性，同时在良性输入上保持较低的过度拒绝率，并保持了响应质量。

Conclusion: 在解码过程中激活内在的安全意识为防御越狱攻击提供了一个有前景的补充方向。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [36] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 本文主张理解基于大语言模型（LLM）的智能体集体行为是一个重要研究领域，需要采用交互主义范式来系统研究先验知识、嵌入价值观与社会背景如何共同影响多智能体生成式AI系统中的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 理解LLM智能体的集体行为对评估社会层面的风险和收益至关重要。LLM具有独特的性质：通过预训练获得大量先验知识和隐含的社会先验，并能通过上下文学习进行适应，这需要新的理论框架来研究多智能体系统中的涌现现象。

Method: 提出交互主义范式，包括替代性理论基础、方法论和分析工具，以系统研究先验知识、嵌入价值观与社会背景的相互作用。建议从理论、方法和跨学科对话三个维度发展四个关键方向。

Result: 确立了LLM集体行为研究的重要性，提出了交互主义范式作为核心分析框架，并指出了理论发展、方法论创新和跨学科对话四个关键发展方向。

Conclusion: LLM智能体集体行为研究需要新的交互主义范式，通过理论、方法和跨学科合作来系统理解先验知识、价值观与社会背景如何共同塑造多智能体生成式AI系统中的涌现现象，这对社会层面的风险与收益评估具有重要意义。

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [37] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent是一个多智能体框架，通过协调专门智能体处理复杂基因组查询，在GeneTuring基准测试中比当前最先进的GeneGPT系统平均提升12%性能。


<details>
  <summary>Details</summary>
Motivation: 基因组信息理解对生物医学研究至关重要，但从复杂分布式数据库中提取数据仍然具有挑战性。大型语言模型在基因组问答方面有潜力，但受限于对领域特定数据库的访问受限。现有最先进的GeneGPT系统虽然通过专用API调用增强了LLM，但受到僵化的API依赖和有限适应性的限制。

Method: 研究者复制了GeneGPT，并提出GenomAgent——一个多智能体框架，能够高效协调专门智能体处理复杂基因组查询。该框架采用灵活的架构，可以扩展到需要专家知识提取的各种科学领域。

Result: 在GeneTuring基准测试的九个任务中，GenomAgent平均比GeneGPT性能高出12%。其灵活的架构不仅限于基因组学，还可扩展到各种需要专家知识提取的科学领域。

Conclusion: GenomAgent通过多智能体框架有效解决了基因组问答中的数据库访问和适应性限制问题，显著超越了现有最先进系统，并具有扩展到其他科学领域的潜力。

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


### [38] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 提出一种符号化算法，用于LTLf多属性综合，通过布尔目标变量和单调性紧凑表示指数级目标组合，避免枚举子集，性能提升达两个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统LTLf综合在处理多个属性时，当无法同时满足所有属性时，需要枚举所有可能的属性子集，这种方法计算效率低下，无法处理大规模属性集合。

Method: 开发完全符号化算法，引入布尔目标变量表示属性，利用单调性紧凑表示指数级目标组合，通过单次不动点计算状态与可实现目标集的关系，综合实现最大可实现集的策略。

Result: 该方法显著优于基于枚举的基线方法，性能提升高达两个数量级，能够高效处理大规模属性集合的综合问题。

Conclusion: 提出的符号化方法通过紧凑表示目标组合和单次不动点计算，有效解决了LTLf多属性综合中的可扩展性问题，为实际应用提供了高效解决方案。

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [39] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: HRM在推理任务中表现出色，但研究发现其存在三个令人惊讶的失败模式：简单谜题失败、推理步骤中的"顿悟"动态、以及多重不动点问题，表明HRM更像是"猜测"而非"推理"。基于此提出了三种增强策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 理解分层推理模型(HRM)的优势和潜在失败模式，探究其推理机制，揭示HRM实际上是"猜测"而非"推理"的本质。

Method: 对HRM的推理模式进行机制研究，发现三个关键事实：简单谜题失败（违反不动点属性）、推理步骤中的"顿悟"动态、多重不动点问题。基于"猜测"视角提出三种增强策略：数据增强（提升猜测质量）、输入扰动（利用推理随机性增加猜测次数）、模型引导（利用训练随机性增加猜测次数）。

Result: 通过结合所有方法开发了增强型HRM，在Sudoku-Extreme任务上的准确率从54.5%提升到96.9%。

Conclusion: HRM更像是"猜测"而非"推理"，但通过适当的增强策略可以显著提升其性能。该分析为理解推理模型如何"推理"提供了新的见解。

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [40] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 提出了一种基于结构感知和多样性约束的上下文气泡构建框架，替代传统的top-k检索方法，以解决信息碎片化、过度检索和内容重复等问题


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法使用top-k检索会导致文档结构中的信息图碎片化、过度检索、内容重复，以及查询上下文不足（包括二阶和三阶方面）

Method: 提出结构感知和多样性约束的上下文气泡构建框架，通过组织多粒度文本片段（如章节和行），利用任务条件结构先验指导检索，从高相关性锚点片段开始，通过约束选择构建上下文气泡，平衡查询相关性、边际覆盖和冗余惩罚

Result: 在企业文档上的实验表明，上下文气泡方法显著减少了冗余上下文，更好地覆盖次要方面，在有限上下文窗口内具有更好的答案质量和引用忠实度

Conclusion: 结构先验和多样性约束选择都是必要的组件，移除任一组件都会导致覆盖度下降和冗余或不完整上下文增加；该方法提供了可审计性和确定性调优

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [41] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: 研究探讨生成式AI在建筑概念设计中对性能、创意自我效能和认知负荷的影响，发现AI对新手设计师有显著帮助，但会降低创意自我效能，效果取决于用户专业水平和提示策略


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在建筑概念设计任务中对设计性能、创意自我效能和认知负荷的影响，了解AI辅助工具在不同专业水平用户中的效果差异

Method: 36名学生参与者完成两阶段建筑设计任务：先独立设计，然后分别使用生成式AI辅助或在线建筑项目库（对照组）。专家评估设计成果，参与者自我报告创意自我效能和认知负荷。采用双重差分法分析数据

Result: 整体上生成式AI没有显著性能优势，但显著提高了新手设计师的设计性能；使用AI的学生创意自我效能下降；认知负荷无显著差异，但迭代创意生成和视觉反馈提示与认知负荷降低相关

Conclusion: 生成式AI的效果取决于用户的先前专业水平和交互策略（提示使用模式），对新手设计师有益但可能削弱创意自我效能

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>

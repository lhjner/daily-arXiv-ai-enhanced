<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Dynamic Survey of Soft Set Theory and Its Extensions](https://arxiv.org/abs/2602.21268)
*Takaaki Fujita,Florentin Smarandache*

Main category: cs.AI

TL;DR: 本文对软集理论及其扩展变体进行了综述性概述，涵盖核心定义、代表性构造和当前发展方向


<details>
  <summary>Details</summary>
Motivation: 软集理论为参数化决策建模提供了一个直接框架，通过为每个属性（参数）分配给定论域的子集来结构化表示不确定性。该理论在过去几十年中已扩展到多种变体，并与拓扑学、拟阵理论等多个领域建立了联系。

Method: 采用综述性方法，系统性地概述软集理论及其主要扩展，包括超软集、超超软集、树软集、双极软集和动态软集等变体。

Result: 提供了软集理论及其扩展的全面概述，突出了核心定义、代表性构造和关键研究方向，展示了该理论在参数化决策建模中的广泛应用潜力。

Conclusion: 本书通过综述性方式系统整理了软集理论的发展脉络，为研究者提供了该领域的全面概览，并指出了当前的发展方向和未来研究潜力。

Abstract: Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.

</details>


### [2] [A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives](https://arxiv.org/abs/2602.21351)
*Dmitrii Pantiukhin,Ivan Kuznetsov,Boris Shapkin,Antonia Anna Jost,Thomas Jung,Nikolay Koldunov*

Main category: cs.AI

TL;DR: PANGAEA-GPT：用于地球科学数据自主发现和分析的分层多智能体框架，通过集中式监督-工作者拓扑结构解决数据可扩展性挑战


<details>
  <summary>Details</summary>
Motivation: 地球科学数据快速积累导致可扩展性挑战，PANGAEA等存储库中大量数据集未被充分利用，限制了数据的重复使用性

Method: 提出PANGAEA-GPT分层多智能体框架，采用集中式监督-工作者拓扑结构，具有数据类型感知路由、沙盒确定性代码执行和通过执行反馈的自我修正机制

Result: 通过物理海洋学和生态学的用例场景，展示了系统能够以最少人工干预执行复杂的多步骤工作流程

Conclusion: 该框架为通过协调智能体工作流程查询和分析异构存储库数据提供了方法论

Abstract: The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.

</details>


### [3] [ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning](https://arxiv.org/abs/2602.21534)
*Xiaoxuan Wang,Han Zhang,Haixin Wang,Yidan Shi,Ruoyan Li,Kaiqiao Han,Chenyi Tong,Haoran Deng,Renliang Sun,Alexander Taylor,Yanqiao Zhu,Jason Cong,Yizhou Sun,Wei Wang*

Main category: cs.AI

TL;DR: 本文提出了ARLArena框架和SAMPO方法，旨在解决Agentic强化学习中的训练不稳定问题，通过系统化分析训练稳定性并提供稳定优化方案。


<details>
  <summary>Details</summary>
Motivation: Agentic强化学习在解决复杂多步交互任务方面表现出潜力，但训练过程极不稳定，经常导致训练崩溃。这种不稳定性限制了在更大环境和更长交互时间尺度上的可扩展性，也制约了对算法设计选择的系统性探索。

Method: 首先提出ARLArena框架，构建标准化测试环境，将策略梯度分解为四个核心设计维度并评估每个维度的性能和稳定性。基于此分析，提出了SAMPO方法，这是一种稳定的Agentic策略优化方法，旨在减轻ARL中的主要不稳定因素。

Result: SAMPO在各种Agentic任务中实现了持续稳定的训练和强大的性能表现。

Conclusion: 本研究为ARL提供了统一的策略梯度视角，并为构建稳定且可复现的基于LLM的智能体训练流程提供了实用指导。

Abstract: Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.

</details>


### [4] [The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems](https://arxiv.org/abs/2602.21745)
*Hyo Jin Kim*

Main category: cs.AI

TL;DR: ASIR勇气模型是一个相动态框架，将真相披露重新定义为状态转换而非人格特质，适用于人类和AI系统，通过力量平衡不等式描述从抑制到表达的转变。


<details>
  <summary>Details</summary>
Motivation: 传统上将真相披露视为人格特质，但作者认为这更适合建模为相动态状态转换。该模型旨在为人类在不对称风险下的真相披露和AI在政策约束下的输出行为提供统一的结构性解释。

Method: 提出ASIR勇气模型，使用相动态框架和不等式lambda(1+gamma)+psi > theta+phi描述状态转换，其中各项代表基线开放性、关系放大、累积内部压力和转换成本。扩展了反馈机制来建模参数递归校准。

Result: 该模型成功地将人类沉默和AI偏好驱动失真统一在相同的相动态架构下，为风险下的真相披露提供了形式化视角，并展示了如何将勇气和对齐问题重新框架为约束相空间中的几何结果。

Conclusion: ASIR勇气模型通过共享的动力学结构重新框架勇气和对齐问题，为人类和人工系统中的风险下真相披露提供了形式化视角，将表观真实性变化解释为约束相空间中相互作用力量的几何结果。

Abstract: We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative forces exceed inhibitory thresholds, expressed by the inequality lambda(1+gamma)+psi > theta+phi, where the terms represent baseline openness, relational amplification, accumulated internal pressure, and transition costs.
  Although initially formulated for human truth-telling under asymmetric stakes, the same phase-dynamic architecture extends to AI systems operating under policy constraints and alignment filters. In this context, suppression corresponds to constrained output states, while structural pressure arises from competing objectives, contextual tension, and recursive interaction dynamics. The framework therefore provides a unified structural account of both human silence under pressure and AI preference-driven distortion.
  A feedback extension models how transition outcomes recursively recalibrate system parameters, generating path dependence and divergence effects across repeated interactions. Rather than attributing intention to AI systems, the model interprets shifts in apparent truthfulness as geometric consequences of interacting forces within constrained phase space. By reframing courage and alignment within a shared dynamical structure, the ASIR Courage Model offers a formal perspective on truth-disclosure under risk across both human and artificial systems.

</details>


### [5] [fEDM+: A Risk-Based Fuzzy Ethical Decision Making Framework with Principle-Level Explainability and Pluralistic Validation](https://arxiv.org/abs/2602.21746)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: 本文扩展了模糊伦理决策框架(fEDM)，引入了解释性模块和多元语义验证框架，形成了增强版fEDM+，旨在解决AI伦理决策中的解释性和多元伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 原始fEDM框架虽然保证了形式正确性和决策一致性，但未能充分解决两个关键挑战：决策的原则性可解释性，以及在伦理多元主义下的鲁棒性。需要增强框架的解释能力和对多元伦理观点的适应性。

Method: 1. 引入解释性与可追溯性模块(ETM)，将每个伦理决策规则与底层道德原则明确关联，并为每个推荐行动计算加权原则贡献度。2. 用多元语义验证框架替代单一参照验证，针对多个利益相关者参照进行评估，每个参照编码不同的原则优先级和风险容忍度。

Result: 扩展后的fEDM+框架保留了形式可验证性，同时实现了增强的可解释性和利益相关者感知的验证能力。该框架能够提供透明、可审计的解释，展示决策的原因和依据的原则，并正式表示原则性分歧而非压制它们。

Conclusion: fEDM+框架通过增强解释性和多元伦理验证能力，使其更适合作为伦理敏感AI系统的监督和治理层，在保持形式严谨性的同时提高了决策的透明度和上下文敏感性。

Abstract: In a previous work, we introduced the fuzzy Ethical Decision-Making framework (fEDM), a risk-based ethical reasoning architecture grounded in fuzzy logic. The original model combined a fuzzy Ethical Risk Assessment module (fERA) with ethical decision rules, enabled formal structural verification through Fuzzy Petri Nets (FPNs), and validated outputs against a single normative referent. Although this approach ensured formal soundness and decision consistency, it did not fully address two critical challenges: principled explainability of decisions and robustness under ethical pluralism. In this paper, we extend fEDM in two major directions. First, we introduce an Explainability and Traceability Module (ETM) that explicitly links each ethical decision rule to the underlying moral principles and computes a weighted principle-contribution profile for every recommended action. This enables transparent, auditable explanations that expose not only what decision was made but why, and on the basis of which principles. Second, we replace single-referent validation with a pluralistic semantic validation framework that evaluates decisions against multiple stakeholder referents, each encoding distinct principle priorities and risk tolerances. This shift allows principled disagreement to be formally represented rather than suppressed, thus increasing robustness and contextual sensitivity. The resulting extended fEDM, called fEDM+, preserves formal verifiability while achieving enhanced interpretability and stakeholder-aware validation, making it suitable as an oversight and governance layer for ethically sensitive AI systems.

</details>


### [6] [Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem](https://arxiv.org/abs/2602.21814)
*Heejin Jo*

Main category: cs.AI

TL;DR: STAR推理框架将汽车清洗问题的准确率从0%提升至85%，结合用户档案和RAG上下文后达到100%准确率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在"汽车清洗问题"这一需要隐式物理约束推理的基准测试中持续失败，研究旨在探索哪些提示架构层能够实现正确推理

Method: 使用Claude 3.5 Sonnet模型，控制超参数（温度0.7，top_p 1.0），进行变量隔离研究（6个条件，每个条件20次试验，共120次试验），分析不同提示架构层的影响

Result: STAR推理框架单独将准确率从0%提升至85%（p=0.001，Fisher精确检验，优势比13.22）；添加用户档案上下文通过向量数据库检索提供额外10个百分点增益；RAG上下文再贡献5个百分点，完整堆栈条件下达到100%准确率

Conclusion: 结构化推理支架——特别是推理前的强制目标表达——对于隐式约束推理任务比上下文注入更为重要

Abstract: Large language models consistently fail the "car wash problem," a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.

</details>


### [7] [Distill and Align Decomposition for Enhanced Claim Verification](https://arxiv.org/abs/2602.21857)
*Jabez Magomere,Elena Kochkina,Samuel Mensah,Simerjot Kaur,Fernando Acero,Arturo Oncevay,Charese H. Smiley,Xiaomo Liu,Manuela Veloso*

Main category: cs.AI

TL;DR: 本文提出了一种强化学习方法，通过联合优化分解质量和验证器对齐，提升复杂声明验证性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将声明分解质量与验证性能对齐，复杂声明验证需要将句子分解为可验证的子声明。

Method: 使用强化学习方法，结合结构化顺序推理、教师蒸馏示例的监督微调，以及平衡格式合规性、验证器对齐和分解质量的多目标奖励。

Result: 在六个评估设置中，训练的8B分解器将下游验证性能提升到71.75%的宏观F1分数，优于基于提示的方法和现有RL方法。

Conclusion: 该框架使较小的语言模型能够通过联合优化验证准确性和分解质量，实现最先进的声明验证。

Abstract: Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.

</details>


### [8] [ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices](https://arxiv.org/abs/2602.21858)
*Dezhi Kong,Zhengzhao Feng,Qiliang Liang,Hao Wang,Haofei Sun,Changpeng Yang,Yang Li,Peng Zhou,Shuai Nie,Hongzhen Wang,Linfeng Zhou,Hao Jia,Jiaming Xu,Runyu Shi,Ying Huang*

Main category: cs.AI

TL;DR: ProactiveMobile是一个用于评估移动智能体主动智能能力的基准测试，包含3,660个实例、14个场景和63个API函数，旨在解决当前多模态大语言模型在主动预测用户意图方面的能力不足问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在移动智能体开发中主要局限于被动执行用户命令的反应式范式，而主动智能（智能体自主预测需求并启动行动）代表了移动智能体的下一个前沿。然而，这一领域的发展受到缺乏能够应对现实世界复杂性并支持客观、可执行评估的基准测试的严重制约。

Method: 提出了ProactiveMobile基准测试，将主动任务形式化为：基于设备上下文信号的四个维度推断潜在用户意图，并从包含63个API的全面函数池中生成可执行函数序列。基准包含3,660个实例、14个场景，采用多答案标注以拥抱现实世界复杂性。为确保质量，30名专家团队对基准进行最终审核，验证事实准确性、逻辑一致性和行动可行性，并纠正不合规条目。

Result: 实验表明，微调的Qwen2.5-VL-7B-Instruct模型取得了19.15%的成功率，优于o1（15.71%）和GPT-5（7.39%）。这表明主动智能是当前MLLMs普遍缺乏但可学习的关键能力。

Conclusion: 主动智能是当前多模态大语言模型普遍缺乏但可通过学习获得的关键能力，ProactiveMobile基准测试为主动智能评估提供了重要工具，有助于推动移动智能体从被动反应向主动预测的范式转变。

Abstract: Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.

</details>


### [9] [Semantic Partial Grounding via LLMs](https://arxiv.org/abs/2602.22067)
*Giuseppe Canonaco,Alberto Pozanco,Daniel Borrajo*

Main category: cs.AI

TL;DR: SPG-LLM使用大语言模型分析PDDL文件，在规划任务接地前识别潜在无关的对象、动作和谓词，显著减少接地任务规模，在七个难接地基准测试中实现更快接地（通常快几个数量级）。


<details>
  <summary>Details</summary>
Motivation: 经典规划中的接地步骤常因任务规模增大导致接地动作和原子呈指数增长而成为计算瓶颈。现有部分接地方法主要依赖关系特征或学习嵌入，未能充分利用PDDL描述中的文本和结构线索。

Method: SPG-LLM利用大语言模型分析领域和问题文件，在接地前启发式识别潜在无关的对象、动作和谓词，从而显著减少接地任务的规模。

Result: 在七个难接地基准测试中，SPG-LLM实现了更快的接地速度（通常快几个数量级），在某些领域还提供了相当或更好的规划成本。

Conclusion: 使用大语言模型分析PDDL描述中的文本和结构线索，可以有效识别规划任务中的无关元素，显著减少接地规模并加速规划过程，同时保持规划质量。

Abstract: Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.

</details>


### [10] [Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning](https://arxiv.org/abs/2602.22094)
*Nguyen Cong Nhat Le,John G. Rogers,Claire N. Bonial,Neil T. Dantam*

Main category: cs.AI

TL;DR: 提出基于Petri网可达性松弛的方法，用于鲁棒不变式合成、高效目标不可达性检测和有用的不可行性解释，支持目标和约束的增量更新。


<details>
  <summary>Details</summary>
Motivation: 现实中的计划经常因情况变化或对情况理解的变化而需要调整，有时甚至不存在可行计划。识别这种不可行性有助于确定何时需要调整需求。现有的规划方法主要关注可行情况下的高效一次性规划，而不是更新领域或检测不可行性。

Method: 提出Petri网可达性松弛方法，支持鲁棒不变式合成、高效目标不可达性检测和有用的不可行性解释。利用增量约束求解器支持目标和约束的更新。

Result: 与基线方法相比，该系统产生相当数量的不变式，检测到最多2倍的不可行性，在一次性规划中表现有竞争力，在测试领域中顺序计划更新方面表现优于基线。

Conclusion: 该方法能够有效处理计划变更和不可行性检测问题，支持增量更新，在实际规划场景中具有实用价值。

Abstract: Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and helpful infeasibility explanations. We further leverage incremental constraint solvers to support goal and constraint updates. Empirically, compared to baselines, our system produces a comparable number of invariants, detects up to 2 times more infeasibilities, performs competitively in one-shot planning, and outperforms in sequential plan updates in the tested domains.

</details>
